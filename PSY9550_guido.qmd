---
title: "quarto_notes_hacks"
author: "Winnie Ma"
format: html
#format: 
 # html: default # ville generert HTML med mulighet for å laste ned PDF fra nettsiden.
  #pdf: default
editor: visual
bibliography: references.bib
html:
    toc: true
    toc-location: left
    embed-resources: true
    page-layout: full
    html-math-method: katex
    number-sections: true
    #self-contained: true
    #code-fold: true
---

```{r, echo=F, warning=F, eval=FALSE} Sys.setlocale(locale='no_NB.utf8')}
```

```{css, echo = F} body{   font-family: Helvetica;   font-size: 16pt;   max-width: 1000px;   margin: auto;   margin-left:310px; } pre{   font-size: 14px; } /* Headers */ h1{     font-size: 24pt;   } h1,h2{     font-size: 20pt;   } h3,h4,h5,h6{   font-size: 18pt; } #TOC {   position: fixed;   left: 0;   top: 0;   width: 300px;   height: 100%;   overflow:auto; }
```

```{r setup,} #| include: false #| message: false #| warning: false #| results: hide knitr::opts_chunk$set(echo = TRUE, dpi = 300)  library(rethinking) library(magrittr) library(rstan) # add other packages you use here}
```

# Exercises chapter 1&2

-   Chapter 1: 2E1-2E4, 2M1-2M3

```{r}

#define grid 
p_grid <- seq( from=0 , to=1 , length.out=20 ) 
# define prior, p for water
prior <- rep( 1 , 20 ) 

# compute likelihood at each value in grid 
likelihood <- dbinom( 6 , size=9 , prob=p_grid ) 

# compute product of likelihood and prior 

unstd.posterior <- likelihood * prior 
# standardize the posterior, so it sums to 1 

posterior <- unstd.posterior / sum(unstd.posterior)

```

## Plotting

If necessary, you can use a header 2 style (`# [your header text]`) to indicate sub-section of a solution. This will typically not be necessary.

Plotting is very easy. Just write the code for the plot into a code block:

`{r} hist(rnorm(1000, mean = 0, sd = 1),      main = "Histogram of 1000 random draws from the standard normal distribution")}`

`{r} curve(dnorm(x, mean = 0, sd = 1),      from = -4, to = 4, n = 500,      main = "Probability density for the standard normal distribution")}`

`{r} curve(pnorm(x, mean = 0, sd = 1),      from = -4, to = 4, n = 500,      main = "Cumulative density for the standard normal distribution")}`

`{r} curve(qnorm(x, mean = 0, sd = 1),      from = 0, to = 1, n = 500,      main = "Quantile function for the standard normal distribution")}`

`{r} curve(dbinom(6, size = 10, prob = 0.4),      from = 0, to = 1, n = 500,      main = "Probability density for the binomial distribution")}`

## Hide standard output

If you estimate models with `ulam` it is a good idea to do estimation and plotting of model results in separate code blocks.

Use a code block like a following to estimate the model. Here `#| results: hide` makes that the standard output will not be included in the rendered html document.

`{r results='hide'}`

`# #| results: hide`

`# n.cores = ifelse(Sys.info()["sysname"] == "Darwin",4,1)`

`#  # my.fit = ulam( #   alist( #     y ~ dnorm(mu, sigma), #     mu ~ dnorm(0,3), #     sigma ~ dexp(2) #   ), #   data = list(y = rnorm(25)), #   log_lik = TRUE, #   chains = 4) #`

# 

# And do additional steps with the resulting fit object in the following code blocks:

`{r} precis(my.fit)`

`{r} divergent(my.fit)`

## Equations

If you want to show equations, it is sufficient to write for example y = a + b\*x.

If you want show prettier equations, have to use LATEX. For instance `$y = a + b*x$`, where the `$`s indicate start and end of an inline equation, becomes $y = a + b*x$. [Here](https://tilburgsciencehub.com/building-blocks/collaborate-and-share-your-work/write-your-paper/amsmath-latex-cheatsheet/) is a cheat sheet that explains latex commands.

For instance, we can write

-   Greek letter like this `$y = \alpha + \beta*x$`: $y = \alpha + \beta*x$.

-   subscripts by using ``` _``  and superscripts by using ```\^`;`$y_i = \alpha + \beta*x_i + \gamma^2$\`: $y_i = \alpha + \beta*x_i + \gamma^2$.

-   fractions like with `\frac{ ... }{ ... }`: `$P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}$.` becomes $P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}$. this is Bayes rule. The probability of a hypothesis or parameter value $\theta$ given the data is equal to the probability of the data given $\theta$ times the prior probability of the $\theta$, divided by the probability of the data.

If we have multi line expressions, we start and end with two `$` and ude `\\` to indicate line breaks:

```         
$$  y \sim normal(\bar \mu, \sigma)   \\  \bar \mu \sim normal(0,3)  \\  \sigma \sim exponential(1) \\ $$
```

becomes

$$  y \sim normal(\bar \mu, \sigma)   \\  \bar \mu \sim normal(0,3)  \\  \sigma \sim exponential(1) \\ $$

# Discussion in class

From class: how to get your prior? It has to be based in reality, and cannot be a crazy estimate/guess, because we know that our posterior is a result of the prior. Reviewers might ask you to do a prior sensitivity analysis of changing priors.

WOW, I suddenly understood the difference between p_grid and prior. Yes, prior is indeed the distribution/probability of the different p's that we have in p_grid. Lets say prior is the probability of people with curly hair, while p_grid gives us the whole range in possible p \[0,1\], and we wanted to know the posterior P(women\|curly). This makes sense as our focus on women with curly hair is very much dependent on the probability of people with curly hair!

The height and details of the posterior is

# Chapter 1&2. guido

<https://htmlpreview.github.io/?https://raw.githubusercontent.com/gbiele/StatRethink/master/Chapter1/ExplainBayes.html#we-know-things-before-seeing-the-data>

```{r, warning=F}

library(tidyverse)
library(plotrix) #histstack

prob.success.prior = .2 # guessed expected proportion 
n.prior.obs = 3 # Here, sample size just refers to the number of fellow students you know well enough to guess how they would do.

a.prior = prob.success.prior*n.prior.obs # number of successes
b.prior = (1-prob.success.prior)*n.prior.obs # number of fails
```

```{r}
# using the alfa og beta from the super small friend sample. Simulate a beta distribution by drawing 5000 random points from the beta distribution.

N.sim = 5000
info_prior = rbeta(N.sim, a.prior, b.prior)
info_prior[1:30] # a beta distribution. Prior. P(x), as in P(x|data) = posterior = P(data|x)*P(x)/P(data)

hist(info_prior, 
     breaks = seq(0,1,.05),
     col = "blue",
     main = "Information before seeing data",
     xlab = "Success probability")



```

```{r}

# our sample size of 13

n.obs = 13 # number of responses
successes = 4 # number of participants with a perfect score
prob.success = successes / n.obs
```

```{r}
# we make a random draw of #successes from the binomial distribution made from the 13 sample, and see if it matches 4 (our observed number of successes)

simulated.successes = 
        rbinom(1, n.obs, info_prior[1])
simulated.successes
# well, 10 is quite far off.

```

```{r}

# we try again, but make 250 draws with shifting priors (from prior beta distribution)
simulated.successes = 
        rbinom(250,n.obs,info_prior[1:250])

# we keep the prior probabilities that produced simulated-successes that match with our observed successes (4). Ex: from 250 simulations and diff info_prior, only 19 matched
good.thetas = 
        info_prior[which(simulated.successes == successes)] %>% 
        round(digits = 2)
round(good.thetas, digits = 3)
```

```{r}
filtered.samples = 
        data.frame(prior.value = info_prior) %>% # info_prior is 5000 long
        rowwise() %>% 
        mutate(simulated.successes = rbinom(1,n.obs,prior.value),
               keep = ifelse(simulated.successes == successes,"keep","reject"))


# rowwise and mutate are dplyr functions
filtered.samples$keep = 
  factor(filtered.samples$keep,
         levels = c("reject","keep"))
```

```{r}
filtered.samples %>% 
  histStack(prior.value~keep,.,
            breaks = seq(0,1,.05),
            col = c("blue","purple"),
            xlab = "Success probability",
            main = "Filter prior information",
            legend.pos = "topright")
```

```{r}
# i think this code makes a video

fn = 
  paste(a.prior,b.prior,n.obs,prob.success,"mp4", sep = ".")

my_histStack = function(dt, ylim = NULL) {
  dt %>%
    histStack(prior.value~keep,.,
              ylim = ylim, xlim = c(0,1),
              breaks = seq(0,1,.05),
              col = c("blue","purple"),
              xlab = "Success probability",
              main = "Filter prior information with data",
              legend.pos = "topright", border = NA,
              cex = 1.75, cex.axis = 1.75, cex.lab = 1.75, cex.main = 2)
}
ylim = c(0,
           hist(filtered.samples$prior.value,
                breaks = seq(0,1,.05), plot = FALSE)$count %>% max())

if(!file.exists(fn)) {
  library(plotrix)
  kk = round(seq(1,nrow(filtered.samples), length.out = 500))
  for (i in 1:length(kk)) {
    png(paste0("anim/",sprintf("%03.f", i),".png"),
        width = 700, height = 700)
    par(mar = c(5.1, 6.1, 4.1, 1.1))
    my_histStack(filtered.samples[1:kk[i],], ylim)
    dev.off()
  }
  imgs = list.files("anim", full.names = T)
  library(av)
  av::av_encode_video(imgs, framerate = 30,
                      output = fn)
}
tmp = file.remove("animation.mp4")
tmp = file.copy(fn,"animation.mp4")
```

Standardizing posterior: This distribution is only proportional to the posterior distribution, because the product of posterior and likelihood does not sum up to 1. We can calculate the posterior probability distribution by dividing by the sum.

```{r}
prior_x_likelihood = prior * likelihood
s = sum(prior_x_likelihood)
posterior = prior_x_likelihood/s
c(1/s, sum(posterior))
```

The following figure illustrates how we get from the the un-normalized posterior to the normalized posterior, which sums to 1, by multiplying with a constant, which is just `1/prior_x_likelihood`.

## **Grid search**

What is the posterior probability of land, given 10 W and 3 L tosses?

First we defined a grid and plot a the prior probability values:

```{r}
p_grid = seq(0,1,by = .05)
prior = dbeta(p_grid,2,1)
plot(p_grid, prior, type = "h", col = "blue",
     ylab = "density", main = "Prior")


```

Vertical lines indicate the prior plausibility for parameter values in the grid.

Next we calculate the likelihood, i.e. the probability of the data given the model (a binomial distribution), the data (10 W, 3 L) and the parameter p (in p_grid):

```{r}
likelihood = dbinom(10,13,p_grid) 
plot(p_grid, likelihood, type = "h", col = "red",      
     ylab = "density", main = "Likelihood")
```

And now we can calculate the un-normalized posterior as a product of prior and likelihood:

```{r}

posterior = prior * likelihood
plot(p_grid, posterior, type = "h", col = "violet", 
     ylab = "density", main = "Posterior")
```

Vertical lines indicate the posterior plausibility of the parameter values in the grid, given the data and the prior.

Here is a plot with all three together:

```{r}
par(mar = c(5.1,4.1,1,2.1))
ylim = c(0,max(c(likelihood,posterior,prior)))
plot(p_grid, prior, type = "h",col = "blue", 
     ylab = "density", ylim = ylim)
lines(p_grid+.005, likelihood, type = "h", col = "red")
lines(p_grid-.005, posterior, type = "h", col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

We can make the plot easier to view by normalizing all values so that they sum up to 1 for prior, likelihood and posterior. In each distribution, only the relative values at the different points of the grid are relevant!

```{r}
n_prior = prior/sum(prior)
n_likelihood = likelihood/sum(likelihood)
n_posterior = posterior/sum(posterior)
n_ylim = c(0,max(c(n_likelihood,n_posterior,n_prior)))
par(mar = c(5.1,4.1,1,2.1))
plot(p_grid, n_prior, type = "h", col = "blue", 
     ylab = "normalized density", ylim = n_ylim)
lines(p_grid+.005, n_likelihood, type = "h", col = "red")
lines(p_grid-.005, n_posterior, type = "h", col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

This plot helps to understand that for a grid search we take each point of the grid and:

-   determine the prior probability (we used a beta distribution)

-   calculate the likelihood, i.e. the conditional probability of the data (we used the binomial distribution)

-   calculate the posterior by multiplying the prior and the likelihood.

However, usually one would not show distributions with vertical lines. Instead, one just shows their outlines:

```{r}
par(mar = c(5.1,4.1,1,2.1))
plot(p_grid, n_prior, col = "blue", type = "l", 
     ylab = "normalized density", ylim = n_ylim)
lines(p_grid, n_likelihood, col = "red")
lines(p_grid, n_posterior, col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

This distribution is not very smooth. We can simply make it smoother by increasing the gridsize:

```{r}
fp_grid = seq(0,1,by = .001)
f_prior = dbeta(fp_grid,2,1)
f_likelihood = dbinom(10,13,fp_grid)
f_posterior = f_prior * f_likelihood

f_prior = f_prior/sum(f_prior)
f_posterior = f_posterior/sum(f_posterior)
f_likelihood = f_likelihood/sum(f_likelihood)
f_ylim = c(0,max(c(f_likelihood,f_posterior,f_prior)))

par(mar = c(5.1,4.1,1,2.1))
plot(fp_grid, f_prior, type = "l", col = "blue",
     ylab = "normalized density", ylim = f_ylim)
lines(fp_grid, f_likelihood, col = "red")
lines(fp_grid, f_posterior, col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

# Chapter 3. guido

<https://htmlpreview.github.io/?https://raw.githubusercontent.com/gbiele/StatRethink/master/Chapter2/Chapter2BG.html>

```{r, warning=T}
library(cursr)
library(cape)
library(coda)
library(mvtnorm)
library(devtools)
library(loo)
library(dagitty)
library(magrittr)
# install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))

```

```{r}


set.seed(123)

# draw the background
draw_pie()
draw_ellipse()

N = 365
is.winter = vector(length = N) # vector to count winter days
is.snow = vector(length = N) # vector to count snow days

for (k in 1:N) {
  # generate random point with custom function
  xy = rpoint_in_circle()
  
  # check if it is a snow day, i.e. in ellipse, with custom function
  is.snow[k] = in_ellipse(xy,h.e,k.e,a.e,b.e,e.rot)
  # check if it is a winter day
  is.winter[k] = xy[1] > 0 & xy[2] < 0
  
  # plot points
  points(xy[1],xy[2],
         pch = ifelse(is.snow[k] == T,8,21), cex = .75,
         bg = ifelse(is.winter[k] == T,"blue","red"),
         col = ifelse(is.winter[k] == T,"blue","red"))
}
legend(.75,.8,
       pch = c(8,21,15,15), bty = "n",
       col = c("black","black","blue","red"),
       legend = c("snow","no snow", "winter", "no winter"))
```

## showing the results of cancer screening

He also show the different ways of calculating probability using two trees: conditional probability, and natural frequency.

```{r}
set.seed(123)
par(mar = c(0,0,0,0))
cols = c("violet","red","orange","green4")
pie(c(1,9,89,901), labels = "", border = NA, col = cols )

legend("topleft",
       bty = "n",
       fill = cols,
       legend = c(
         "False negatitve",
         "True positive",
         "False positive",
         "True Negative"
       ))
```
