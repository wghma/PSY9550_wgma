---
title: "quarto_notes_hacks"
author: "Winnie Ma"
format: html
#format: 
 # html: default # ville generert HTML med mulighet for å laste ned PDF fra nettsiden.
  #pdf: default
editor: visual
bibliography: references.bib
html:
    toc: true
    toc-location: left
    embed-resources: true
    page-layout: full
    html-math-method: katex
    number-sections: true
    self-contained: true
    code-fold: true
---

```{r, echo=F, warning=F, eval=FALSE} Sys.setlocale(locale='no_NB.utf8')}
```

```{css, echo = F} body{   font-family: Helvetica;   font-size: 16pt;   max-width: 1000px;   margin: auto;   margin-left:310px; } pre{   font-size: 14px; } /* Headers */ h1{     font-size: 24pt;   } h1,h2{     font-size: 20pt;   } h3,h4,h5,h6{   font-size: 18pt; } #TOC {   position: fixed;   left: 0;   top: 0;   width: 300px;   height: 100%;   overflow:auto; }
```

```{r setup,} #| include: false #| message: false #| warning: false #| results: hide knitr::opts_chunk$set(echo = TRUE, dpi = 300)  library(rethinking) library(magrittr) library(rstan) # add other packages you use here}
```

# Exercises chapter 1&2

-   Chapter 1: 2E1-2E4, 2M1-2M3

```{r}

#define grid 
p_grid <- seq( from=0 , to=1 , length.out=20 ) 
# define prior, p for water
prior <- rep( 1 , 20 ) 

# compute likelihood at each value in grid 
likelihood <- dbinom( 6 , size=9 , prob=p_grid ) 

# compute product of likelihood and prior 

unstd.posterior <- likelihood * prior 
# standardize the posterior, so it sums to 1 

posterior <- unstd.posterior / sum(unstd.posterior)

```

## Plotting

If necessary, you can use a header 2 style (`# [your header text]`) to indicate sub-section of a solution. This will typically not be necessary.

Plotting is very easy. Just write the code for the plot into a code block:

`{r} hist(rnorm(1000, mean = 0, sd = 1),      main = "Histogram of 1000 random draws from the standard normal distribution")}`

`{r} curve(dnorm(x, mean = 0, sd = 1),      from = -4, to = 4, n = 500,      main = "Probability density for the standard normal distribution")}`

`{r} curve(pnorm(x, mean = 0, sd = 1),      from = -4, to = 4, n = 500,      main = "Cumulative density for the standard normal distribution")}`

`{r} curve(qnorm(x, mean = 0, sd = 1),      from = 0, to = 1, n = 500,      main = "Quantile function for the standard normal distribution")}`

`{r} curve(dbinom(6, size = 10, prob = 0.4),      from = 0, to = 1, n = 500,      main = "Probability density for the binomial distribution")}`

## Hide standard output

If you estimate models with `ulam` it is a good idea to do estimation and plotting of model results in separate code blocks.

Use a code block like a following to estimate the model. Here `#| results: hide` makes that the standard output will not be included in the rendered html document.

`{r results='hide'}`

`# #| results: hide`

`# n.cores = ifelse(Sys.info()["sysname"] == "Darwin",4,1)`

`#  # my.fit = ulam( #   alist( #     y ~ dnorm(mu, sigma), #     mu ~ dnorm(0,3), #     sigma ~ dexp(2) #   ), #   data = list(y = rnorm(25)), #   log_lik = TRUE, #   chains = 4) #`

# 

# And do additional steps with the resulting fit object in the following code blocks:

`{r} precis(my.fit)`

`{r} divergent(my.fit)`

## Equations

If you want to show equations, it is sufficient to write for example y = a + b\*x.

If you want show prettier equations, have to use LATEX. For instance `$y = a + b*x$`, where the `$`s indicate start and end of an inline equation, becomes $y = a + b*x$. [Here](https://tilburgsciencehub.com/building-blocks/collaborate-and-share-your-work/write-your-paper/amsmath-latex-cheatsheet/) is a cheat sheet that explains latex commands.

For instance, we can write

-   Greek letter like this `$y = \alpha + \beta*x$`: $y = \alpha + \beta*x$.

-   subscripts by using ``` _``  and superscripts by using ```\^`;`$y_i = \alpha + \beta*x_i + \gamma^2$\`: $y_i = \alpha + \beta*x_i + \gamma^2$.

-   fractions like with `\frac{ ... }{ ... }`: `$P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}$.` becomes $P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}$. this is Bayes rule. The probability of a hypothesis or parameter value $\theta$ given the data is equal to the probability of the data given $\theta$ times the prior probability of the $\theta$, divided by the probability of the data.

If we have multi line expressions, we start and end with two `$` and ude `\\` to indicate line breaks:

```         
$$  y \sim normal(\bar \mu, \sigma)   \\  \bar \mu \sim normal(0,3)  \\  \sigma \sim exponential(1) \\ $$
```

becomes

$$  y \sim normal(\bar \mu, \sigma)   \\  \bar \mu \sim normal(0,3)  \\  \sigma \sim exponential(1) \\ $$

# Discussion in class

## In class chapter 1&2

From class: how to get your prior? It has to be based in reality, and cannot be a crazy estimate/guess, because we know that our posterior is a result of the prior. Reviewers might ask you to do a prior sensitivity analysis of changing priors.

WOW, I suddenly understood the difference between p_grid and prior. Yes, prior is indeed the distribution/probability of the different p's that we have in p_grid. Lets say prior is the probability of people with curly hair, while p_grid gives us the whole range in possible p \[0,1\], and we wanted to know the posterior P(women\|curly). This makes sense as our focus on women with curly hair is very much dependent on the probability of people with curly hair!

The height and details of the posterior is defined by the grid size, the more grid points between 0 and 1, the higher the resolution.

## in class Chapter 3

THe reason why some dont like bayesian statistics, they do not bother to define or find a prior. Fisher tried finding his posterior without the prior, but he didnt find a solution.

would discourage use of linear regression for binary data, because then the model would allow predictions below 0 and values above 1.

# Chapter 1&2 and 3 guido

<https://htmlpreview.github.io/?https://raw.githubusercontent.com/gbiele/StatRethink/master/Chapter1/ExplainBayes.html#we-know-things-before-seeing-the-data>

```{r, warning=F}

library(tidyverse)
library(plotrix) #histstack

prob.success.prior = .2 # guessed expected proportion 
n.prior.obs = 3 # Here, sample size just refers to the number of fellow students you know well enough to guess how they would do.

a.prior = prob.success.prior*n.prior.obs # number of successes
b.prior = (1-prob.success.prior)*n.prior.obs # number of fails
```

```{r}
# using the alfa og beta from the super small friend sample. Simulate a beta distribution by drawing 5000 random points from the beta distribution.

N.sim = 5000
info_prior = rbeta(N.sim, a.prior, b.prior)
# a beta distribution. Prior. P(x), as in P(x|data) = posterior = P(data|x)*P(x)/P(data)

hist(info_prior, 
     breaks = seq(0,1,.05),
     col = "blue",
     main = "Information before seeing data",
     xlab = "Success probability")



```

```{r}

# our sample size of 13

n.obs = 13 # number of responses
successes = 4 # number of participants with a perfect score
prob.success = successes / n.obs
```

```{r}
# we make a random draw of #successes from the binomial distribution made from the 13 sample, and see if it matches 4 (our observed number of successes)

simulated.successes = 
        rbinom(1, n.obs, info_prior[1])
simulated.successes
# well, 10 is quite far off.

```

```{r}

# we try again, but make 250 draws with shifting priors (from prior beta distribution)
simulated.successes = 
        rbinom(250,n.obs,info_prior[1:250])

# we keep the prior probabilities that produced simulated-successes that match with our observed successes (4). Ex: from 250 simulations and diff info_prior, only 19 matched
good.thetas = 
        info_prior[which(simulated.successes == successes)] %>% 
        round(digits = 2)
round(good.thetas, digits = 3)
```

```{r}
filtered.samples = 
        data.frame(prior.value = info_prior) %>% # info_prior is 5000 long
        rowwise() %>% 
        mutate(simulated.successes = rbinom(1,n.obs,prior.value),
               keep = ifelse(simulated.successes == successes,"keep","reject"))


# rowwise and mutate are dplyr functions
filtered.samples$keep = 
  factor(filtered.samples$keep,
         levels = c("reject","keep"))
```

```{r}
filtered.samples %>% 
  histStack(prior.value~keep,.,
            breaks = seq(0,1,.05),
            col = c("blue","purple"),
            xlab = "Success probability",
            main = "Filter prior information",
            legend.pos = "topright")
```

```{r}
# i think this code makes a video

fn = 
  paste(a.prior,b.prior,n.obs,prob.success,"mp4", sep = ".")

my_histStack = function(dt, ylim = NULL) {
  dt %>%
    histStack(prior.value~keep,.,
              ylim = ylim, xlim = c(0,1),
              breaks = seq(0,1,.05),
              col = c("blue","purple"),
              xlab = "Success probability",
              main = "Filter prior information with data",
              legend.pos = "topright", border = NA,
              cex = 1.75, cex.axis = 1.75, cex.lab = 1.75, cex.main = 2)
}
ylim = c(0,
           hist(filtered.samples$prior.value,
                breaks = seq(0,1,.05), plot = FALSE)$count %>% max())

if(!file.exists(fn)) {
  library(plotrix)
  kk = round(seq(1,nrow(filtered.samples), length.out = 500))
  for (i in 1:length(kk)) {
    png(paste0("anim/",sprintf("%03.f", i),".png"),
        width = 700, height = 700)
    par(mar = c(5.1, 6.1, 4.1, 1.1))
    my_histStack(filtered.samples[1:kk[i],], ylim)
    dev.off()
  }
  imgs = list.files("anim", full.names = T)
  library(av)
  av::av_encode_video(imgs, framerate = 30,
                      output = fn)
}
tmp = file.remove("animation.mp4")
tmp = file.copy(fn,"animation.mp4")
```

Standardizing posterior: This distribution is only proportional to the posterior distribution, because the product of posterior and likelihood does not sum up to 1. We can calculate the posterior probability distribution by dividing by the sum.

```{r}
prior_x_likelihood = prior * likelihood
s = sum(prior_x_likelihood)
posterior = prior_x_likelihood/s
c(1/s, sum(posterior))
```

The following figure illustrates how we get from the the un-normalized posterior to the normalized posterior, which sums to 1, by multiplying with a constant, which is just `1/prior_x_likelihood`.

## Grid search

What is the posterior probability of land, given 10 W and 3 L tosses?

First we defined a grid and plot a the prior probability values: Is it true then that our prior tells us it is 2 times the chance for 100% water on Earth than it is actually 50% water.

```{r}
p_grid = seq(0,1,by = .05)
prior = dbeta(p_grid,2,1)
plot(p_grid, prior, type = "h", col = "blue",
     ylab = "density", main = "Prior")


```

### Why beta and not binomial prior?

So, why cant we have a binomial prior? Let's say we have a feeling that W=2, Land = 1. simulate 20 points

```{r}
p_grid = seq(0,1,0.05)
p = 2/3
plot(p_grid, dbinom(2,3, p_grid))

```

So, how does a beta looks like when W =2, Land = 1

```{r}
# why use beta distribution as prior???
# Beta (a,b), a = successes, b = fails
plot(p_grid, dbeta(p_grid, 2,1))

```

...And what about dbeta(p_grid, 20, 20)

```{r}
plot(p_grid, dbeta(p_grid, 20,20))
```

...and what about a=2, b=2...

```{r}

plot(p_grid, dbeta(p_grid, 2,2))
```

Vertical lines indicate the prior plausibility for parameter values in the grid.

Next we calculate the likelihood, i.e. the probability of the data given the model (a binomial distribution), the data (10 W, 3 L) and the parameter p (in p_grid):

```{r}
likelihood = dbinom(10,13,p_grid) 
plot(p_grid, likelihood, type = "h", col = "red",      
     ylab = "density", main = "Likelihood")
```

And now we can calculate the un-normalized posterior as a product of prior and likelihood:

```{r}

posterior = prior * likelihood
plot(p_grid, posterior, type = "h", col = "violet", 
     ylab = "density", main = "Posterior")
```

Vertical lines indicate the posterior plausibility of the parameter values in the grid, given the data and the prior.

Here is a plot with all three together:

```{r}
par(mar = c(5.1,4.1,1,2.1))
ylim = c(0,max(c(likelihood,posterior,prior)))
plot(p_grid, prior, type = "h",col = "blue", 
     ylab = "density", ylim = ylim)
lines(p_grid+.005, likelihood, type = "h", col = "red")
lines(p_grid-.005, posterior, type = "h", col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

We can make the plot easier to view by normalizing all values so that they sum up to 1 for prior, likelihood and posterior. In each distribution, only the relative values at the different points of the grid are relevant!

```{r}
n_prior = prior/sum(prior)
n_likelihood = likelihood/sum(likelihood)
n_posterior = posterior/sum(posterior)
n_ylim = c(0,max(c(n_likelihood,n_posterior,n_prior)))
par(mar = c(5.1,4.1,1,2.1))
plot(p_grid, n_prior, type = "h", col = "blue", 
     ylab = "normalized density", ylim = n_ylim)
lines(p_grid+.005, n_likelihood, type = "h", col = "red")
lines(p_grid-.005, n_posterior, type = "h", col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

This plot helps to understand that for a grid search we take each point of the grid and:

-   determine the prior probability (we used a beta distribution)

-   calculate the likelihood, i.e. the conditional probability of the data (we used the binomial distribution)

-   calculate the posterior by multiplying the prior and the likelihood.

However, usually one would not show distributions with vertical lines. Instead, one just shows their outlines:

```{r}
par(mar = c(5.1,4.1,1,2.1))
plot(p_grid, n_prior, col = "blue", type = "l", 
     ylab = "normalized density", ylim = n_ylim)
lines(p_grid, n_likelihood, col = "red")
lines(p_grid, n_posterior, col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

This distribution is not very smooth. We can simply make it smoother by increasing the gridsize:

```{r}
fp_grid = seq(0,1,by = .001)
f_prior = dbeta(fp_grid,2,1)
f_likelihood = dbinom(10,13,fp_grid)
f_posterior = f_prior * f_likelihood

f_prior = f_prior/sum(f_prior)
f_posterior = f_posterior/sum(f_posterior)
f_likelihood = f_likelihood/sum(f_likelihood)
f_ylim = c(0,max(c(f_likelihood,f_posterior,f_prior)))

par(mar = c(5.1,4.1,1,2.1))
plot(fp_grid, f_prior, type = "l", col = "blue",
     ylab = "normalized density", ylim = f_ylim)
lines(fp_grid, f_likelihood, col = "red")
lines(fp_grid, f_posterior, col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

# Chapter 3. guido

<https://htmlpreview.github.io/?https://raw.githubusercontent.com/gbiele/StatRethink/master/Chapter2/Chapter2BG.html>

```{r, warning=T}
library(cursr)
library(cape)
library(coda)
library(mvtnorm)
library(devtools)
library(loo)
library(dagitty)
library(magrittr)
# install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))

```

```{r}


set.seed(123)

# draw the background
draw_pie()
draw_ellipse()

N = 365
is.winter = vector(length = N) # vector to count winter days
is.snow = vector(length = N) # vector to count snow days

for (k in 1:N) {
  # generate random point with custom function
  xy = rpoint_in_circle()
  
  # check if it is a snow day, i.e. in ellipse, with custom function
  is.snow[k] = in_ellipse(xy,h.e,k.e,a.e,b.e,e.rot)
  # check if it is a winter day
  is.winter[k] = xy[1] > 0 & xy[2] < 0
  
  # plot points
  points(xy[1],xy[2],
         pch = ifelse(is.snow[k] == T,8,21), cex = .75,
         bg = ifelse(is.winter[k] == T,"blue","red"),
         col = ifelse(is.winter[k] == T,"blue","red"))
}
legend(.75,.8,
       pch = c(8,21,15,15), bty = "n",
       col = c("black","black","blue","red"),
       legend = c("snow","no snow", "winter", "no winter"))
```

## showing the results of cancer screening

He also show the different ways of calculating probability using two trees: conditional probability, and natural frequency.

```{r}
set.seed(123)
par(mar = c(0,0,0,0))
cols = c("violet","red","orange","green4")
pie(c(1,9,89,901), labels = "", border = NA, col = cols )

legend("topleft",
       bty = "n",
       fill = cols,
       legend = c(
         "False negatitve",
         "True positive",
         "False positive",
         "True Negative"
       ))
```

Beta distribution, how it works.Dbeta(x,1,1) gives a uniform distribution.

```{r}

plot(dbeta(seq(0,1,0.05), 1,1))
```

Beta distribution, how it works.Dbeta(x,1,2) gives a linear downwards.

```{r}

plot(seq(0,1,0.05), 
     dbeta(seq(0,1,0.05), 1,2))
```

## Learning from posterior

```{r}
n_water = 13
n_total = 20
p_grid = seq(0,1,.01)
prior = rep(1,length(p_grid))
likelihood = dbinom(n_water, n_total, prob = p_grid)
posterior = prior*likelihood
posterior = posterior / sum(posterior)
plot(p_grid,posterior,'l',
     main = "Posterior distribution for the proportion of water")
```

```{r}
# generate samples from the posterior distribution
posterior_samples = 
  sample(p_grid,5000, prob = posterior, replace = TRUE)
# simulate data given the median of the 
# posterior distribution for proportion of water
predictions.point = 
  rbinom(5000,n_total, prob = median(posterior_samples))  # 5000 people with globes in their hands with median p water ratio. THey toss it 20 times each.
simplehist(
  predictions.point, 
  xlab = "# predicted water tosses",
  main = "Posterior predictive distribution from median")
points(n_water,0,pch = 17, col = "green3", cex = 3)
points(round(mean(predictions.point)),0, col = "red", pch = 16, cex = 2)
```

However, **in a Bayesian approach, we do not use a point estimates of parameters to generate posterior predictions, but the full posterior distribution.**

```{r}
predictions.dist = rbinom(5000,n_total, prob = posterior_samples)
```

```{r}
simplehist(
  predictions.dist,
  ylim = c(0, max(table(predictions.point))),
  col = "blue",
  xlab = "# predicted water tosses",
  main = "Posterior predictive distribution")
lines(as.numeric(names(table(predictions.point)))+.2,
      table(predictions.point), type ="h", lwd = 3)
points(n_water,0,pch = 17, col = "green3", cex = 2.5)
points(round(mean(predictions.dist)),0, col = "red", pch = 16, cex = 2)
legend("topleft",
       bty = "n",
       col = c("blue","black"), lty = 1, lwd = 3,
       title = "Predictions from posterior",
       legend = c("5000 globes across distribution",
                  "5000 median globes"))

```

We try the wrong likelihood distribution for our case here: poisson. It gives us chances of water ABOVE 20, where 20 is our number of observations.

Now lets redo the analysis, but instead of using the binomial distribution we use another distribution for integer data, the poisson distribution. The sole parameter of the poisson distribution is the mean number of expected counts.

```{r}
m_grid = seq(0,40,.1)
prior = rep(1,length(m_grid))
likelihood = dpois(n_water, lambda = m_grid)
posterior = prior*likelihood
posterior = posterior / sum(posterior)
plot(m_grid,posterior,'l',
     main = "Posterior distribution for expected water tosses.")
```

```{r}
# generate samples from the posterior distribution
posterior_samples = sample(m_grid,5000, prob = posterior, replace = TRUE)
# simulate data given the median of the 
# posterior distribution for proportion of water
predictions.dist = rpois(5000,posterior_samples)
simplehist(
  predictions.dist, xlab = "# predicted water tosses",
  main = "Posterior predictive distribution from 'wrong' model.")
points(round(mean(predictions.dist)),0,pch = 17, col = "green3", cex = 2.5)
points(n_water,0, col = "red", pch = 16, cex = 2)
abline(v = 20.5, lty = 3, col = "magenta")

```

Here is a posterior predictive distribution from another bad model. What aspect of model is problematic?

```{r}
n_water = 13
n_total = 20
p_grid = seq(0,1,.01)
prior = dbeta(p_grid,1,10)
likelihood = dbinom(n_water, n_total, prob = p_grid)
posterior = prior*likelihood
```

Because now it has a MAP at 9 water, when our data observed 13 out of 20. This happens because of our prior that strongly skews it to the left.

```{r}
posterior = posterior / sum(posterior)
posterior_samples = 
  sample(p_grid,5000, prob = posterior, replace = TRUE)
predictions.dist = rbinom(5000,n_total, prob = posterior_samples)
simplehist(
  predictions.dist,
  xlab = "# predicted water tosses",
  main = "Bad posterior predictive distribution (I)")
points(n_water,0,pch = 17, col = "green3", cex = 2.5)
points(round(mean(predictions.dist)),0, col = "red", pch = 16, cex = 2)
```

# Chapter 4 (book)

```{r}
library(rethinking) 
data(Howell1) 
d <- Howell1

d2 <- d[ d$age >= 18 , ]

```

have a look at data d

```{r}
precis( d )
```

```{r}

#post1 gives dataframe with only 100 rows
n = 100
rep = 100


post <- tibble(
  # mu = seq(150,160,length.out=n ),
  # sigma = seq(7,9,length.out=n ),
  mu = rep(seq(150,160,length.out=n ), times = rep),
  sigma = rep(seq(7,9,length.out=n ), each = rep),
  LL1 = sapply( 1:nrow(.) , 
                function(i) sum( dnorm( d2$height , mu[i] , sigma[i] , log=TRUE ) ) ) , #10000 calc likelihood of observed height-vector given each 10000 combinations of mu and sigma.
  # LL2 = sum( dnorm( d2$height , mu, sigma , log=TRUE ) ) ,
  prod = LL1 + dnorm( mu , 178 , 20 , TRUE ) + dunif( sigma , 0 , 50 , TRUE ),
  prob = exp( prod - max(prod) )
)


# post2 gives dataframe with 10000 rows.

mu.list <- seq( from=150, to=160 , length.out=100 )
sigma.list <- seq( from=7 , to=9 , length.out=100 )
post2 <- expand.grid( mu=mu.list , sigma=sigma.list ) # expand.grid gir alle kombinasjoner av mu og sigma.
post2$LL <- sapply( 1:100 , function(i) sum( dnorm( d2$height , post2$mu[i] , post2$sigma[i] , log=TRUE ) ) ) 
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) + dunif( post2$sigma , 0 , 50 , TRUE ) 
# 
post2$prob <- exp( post2$prod - max(post2$prod) )
```

**`sapply`** is used to apply a function to each element of the specified vector.

-   The function being applied calculates the log-likelihood for each combination of **`mu`** and **`sigma`** using the **`dnorm`** function.

-   **`dnorm`** computes the density of the normal distribution with parameters **`post2$mu[i]`** (mean) and **`post2$sigma[i]`** (standard deviation) for the values in the **`d2$height`** vector.

-   **`sum`** is used to add up the log-likelihood values for each element in the **`d2$height`** vector.

-   The resulting log-likelihood values are stored in the **`LL`** column of the **`post2`** data frame.

In summary, this code generates a grid of combinations of **`mu`** and **`sigma`**, and for each combination, it calculates the log-likelihood of a set of observed heights (**`d2$height`**) assuming a normal distribution with the corresponding **`mu`** and **`sigma`** values. The results are stored in a data frame (**`post2`**) with columns for **`mu`**, **`sigma`**, and the log-likelihood (**`LL`**).

```{r}
contour_xyz( post2$mu , post2$sigma , post2$prob )
```

```{r}
#?image_xyz
image_xyz( post2$mu , post2$sigma , post2$prob )
```

Get samples from our normal posterior distribution

```{r}
library(rethinking)
sample.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob ) 
sample.mu <- post2$mu[ sample.rows ] 
sample.sigma <- post2$sigma[ sample.rows ]

plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )

# Adjust the plot to your tastes by playing around with cex (character expansion, the size of the points), pch (plot character), and the 0.1 transparency value.
```

```{r}
sample.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob ) 
sample.mu <- post2$mu[ sample.rows ] 
sample.sigma <- post2$sigma[ sample.rows ]

plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
```

For example, to characterize the shapes of the marginal posterior densities of \sigma\ and \\mu\\ , all we need to do is:

```{r}

dens( sample.mu ) 

```

```{r}
dens( sample.sigma )
```

To summarize the widths of these densities with posterior compatibility intervals, just like in Chapter 3:

```{r}
PI( sample.mu ) 
PI( sample.sigma )

```

Overthinking: Sample size and the normality of \\sigma ’s posterior. Before moving on to using quadratic approximation (quap) as shortcut to all of this inference, it is worth repeating the analysis of the height data above, but now with only a fraction of the original data. The reason to do this is to demonstrate that, in principle, the posterior is not always so Gaussian in shape. There’s no trouble with the mean, mu. For a Gaussian likelihood and a Gaussian prior on mu, the posterior distribution is always Gaussian as well, regardless of sample size. It is the standard deviation sigma that causes problems. So if you care about sigma—often people do not—you do need to be careful of abusing the quadratic approximation.

The deep reasons for the posterior of signa tending to have a long right-hand tail are complex. But a useful way to conceive of the problem is that variances must be positive. As a result, there must be more uncertainty about how big the variance (or standard deviation) is than about how small it is. For example, if the variance is estimated to be near zero, then you know for sure that it can’t be much smaller. But it could be a lot bigger.

```{r}
d3 <- sample( d2$height , size=20 )
```

```{r}
n = 200

post3 <- tibble(
  mu = seq(150,170,length.out=n ),
  sigma = seq(4,20,length.out=n ),
  LL1 = sapply( 1:n , 
                function(i) sum( dnorm( d3 , mu[i] , sigma[i] , log=TRUE ) ) ) ,
  LL2 = sum( dnorm( d3 , mu, sigma , log=TRUE ) ) ,
  prod = LL1 + dnorm( mu , 178 , 20 , TRUE ) + dunif( sigma , 0 , 50 , TRUE ),
  prob = exp( prod - max(prod, na.rm = T) )
)



sample3.rows <- sample( 1:n , size=1e4 , replace=TRUE , prob=post3$prob ) 
sample3.mu <- post3$mu[ sample3.rows ] 
sample3.sigma <- post3$sigma[ sample3.rows ] 

plot( sample3.mu , sample3.sigma , cex=0.5 , col=col.alpha(rangi2,0.1) , xlab="mu" , ylab="sigma" , pch=16 )
```

## Quap()

```{r}
d <- Howell1 
d2 <- d[ d$age >= 18 , ]
```

Now place the R code equivalents into an alist. Here’s an alist of the formulas above:Now place the R code equivalents into an alist. Here’s an alist of the formulas above:

```{r}
flist <- alist( height ~ dnorm( mu , sigma ) , 
                mu ~ dnorm( 178 , 20 ) , 
                sigma ~ dunif( 0 , 50 ) )
```

Fit the model to the data in the data frame d2 with:

```{r}
m4.1 <- quap( flist , data=d2 )
precis( m4.1 )
```

These numbers provide Gaussian approximations for each parameter’s marginal distribution. This means the plausibility of each value of mu, after averaging over the plausibilities of each value of sigma, is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4. The 5.5% and 94.5% quantiles are percentile interval boundaries, corresponding to an 89% compatibility interval. Why 89%? It’s just the default. It displays a quite wide interval, so it shows a high-probability range of parameter values. If you want another interval, such as the conventional and mindless 95%, you can use precis(m4.1,prob=0.95). But I don’t recommend 95% intervals, because readers will have a hard time not viewing them as significance tests.

89 is also a prime number, so if someone asks you to justify it, you can stare at them meaningfully and incant, “Because it is prime.” That’s no worse justification than the conventional justification for 95%.

I encourage you to compare these 89% boundaries to the compatibility intervals from the grid approximation earlier. You’ll find that they are almost identical. When the posterior is approximately Gaussian, then this is what you should expect.

### start values quap()

Start values for quap. quap estimates the posterior by climbing it like a hill. To do this, it has to start climbing someplace, at some combination of parameter values. Unless you tell it otherwise, quap starts at random values sampled from the prior. But it’s also possible to specify a starting value for any parameter in the model.

These start values are good guesses of the rough location of the MAP values.

```{r}
start <- list( mu=mean(d2$height), 
               sigma=sd(d2$height) ) 

m4.1 <- quap( flist , data=d2 , start=start )
```

Alist() vs list(): Note that the list of start values is a regular list, not an alist like the formula list is. The two functions alist and list do the same basic thing: allow you to make a collection of arbitrary R objects. They differ in one important respect: list evaluates the code you embed inside it, while alist does not. So when you define a list of formulas, you should use alist, so the code isn’t executed. But when you define a list of start values for parameters, you should use list, so that code like mean(d2\$height) will be evaluated to a numeric value.

The priors we used before are very weak, both because they are nearly flat and because there is so much data. So I’ll splice in a more informative prior for mu, so you can see the effect. All I’m going to do is change the standard deviation of the prior to 0.1, so it’s a very narrow prior.

```{r}
m4.2 <- quap( alist( 
  height ~ dnorm( mu , sigma ) , 
  mu ~ dnorm( 178 , 0.1 ) , # 0.1 instead of 20
  sigma ~ dunif( 0 , 50 ) ) , data=d2 ) 

precis( m4.2 )
```

```{r}
precis(m4.1)
```

Notice that the estimate for mu has hardly moved off the prior dnorm(178, 0.1) . The prior was very concentrated around 178. So this is not surprising. But also notice that the estimate for sigma has changed quite a lot, even though we didn’t change its prior at all. Once the golem is certain that the mean is near 178—as the prior insists—then the golem has to estimate sigma conditional on that fact. This results in a different posterior for sigma, even though all we changed is prior information about the other parameter (mu).

### sampling from quap()

The above explains how to get a quadratic approximation of the posterior, using quap. But how do you then get samples from the quadratic approximate posterior distribution? The answer is rather simple, but non-obvious, and it requires recognizing that a quadratic approximation to a posterior distribution with more than one parameter dimension—mu and sigma each contribute one dimension—is just a multi-dimensional Gaussian distribution.

Just like a mean and standard deviation (or its square, a variance) are sufficient to describe a one-dimensional Gaussian distribution, a list of means and a matrix of variances and covariances are sufficient to describe a multi-dimensional Gaussian distribution. To see this matrix of variances and covariances, for model m4.1, use:

```{r}
vcov(m4.1)
```

(why is the self-cov not 1?

The above is a variance-covariance matrix. It is the multi-dimensional glue of a quadratic approximation, because it tells us how each parameter relates to every other parameter in the posterior distribution. A variance-covariance matrix can be factored into two elements: (1) a vector of variances for the parameters and (2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others.

Can also decompose it:

```{r}
diag( vcov( m4.1 ) ) 
cov2cor( vcov( m4.1 ) )
```

Each entry shows the correlation, bounded between 1and +1, for each pair of parameters. The 1’s indicate a parameter’s correlation with itself. If these values were anything except 1, we would be worried. The other entries are typically closer to zero, and they are very close to zero in this example. This indicates that learning mu tells us nothing about sigma and likewise that learning sigma tells us nothing about mu. This is typical of simple Gaussian models of this kind. But it is quite rare more generally, as you’ll see in later chapters.

```{r}
post <- extract.samples( m4.1 , n=1e4 ) 
head(post)
```

You end up with a data frame, post, with 10,000 (1e4) rows and two columns, one column for mu and one for sigma. Each value is a sample from the posterior, so the mean and standard deviation of each column will be very close to the MAP values from before. You can confirm this by summarizing the samples:

```{r}
precis(post)
```

About the function extract.samples(), it is based upon mvrnorm. The function rnorm simulates random Gaussian values, while mvrnorm simulates random vectors of multivariate Gaussian values. Here’s how to use it directly to do what extract.samples does:

```{r}
library(MASS) 
post <- mvrnorm( n=1e4 , 
                 mu=coef(m4.1) , 
                 Sigma=vcov(m4.1) )
```

## 4.4 Linear prediction

earlier in this chapter we have only sampled one variable: height. Usually we would like to study relations between variables, and maybe predict a variable based upon the other.

So now let’s look at how height in these Kalahari foragers (the outcome variable) covaries with weight (the predictor variable).

Go ahead and plot height and weight against one another to get an idea of how strongly they covary:

```{r}
plot( d2$height ~ d2$weight )
```

Regression, what does it mean? Regression = models. The term has come to mean using one or more predictor variables to model the distribution of one or more outcome variables. The original use of term, however, arose from anthropologist Francis Galton’s (1822–1911) observation that the sons of tall and short men tended to be more similar to the population mean, hence regression to the mean.73 The causal reasons for regression to the mean are diverse. In the case of height, the causal explanation is a key piece of the foundation of population genetics. But this phenomenon arises statistically whenever individual measurements are assigned a common distribution, leading to shrinkage as each measurement informs the others. In the context of Galton’s height data, attempting to predict each son’s height on the basis of only his father’s height is folly. Better to use the population of fathers. This leads to a prediction for each son which is similar to each father but “shrunk” towards the overall mean.

Log-normal: rlnorm (like rnorm) and dlnorm (dnorm).

```{r}
library(graphics)
set.seed(2971) 
N <- 100 # 100 lines 

# lin_norm <- tibble(
#   a = rnorm( N , 178 , 20 ),
#   b = rnorm( N , 0 , 10 ),
#   x_mean = mean(d2$weight),
#   height_i = a + b*(d2$weight-x_mean)
# )


# plotter alle linjene for d2$weight som x. 
a <- rnorm( N , 178 , 20 )
b <- rnorm( N , 0 , 10 )
xbar <- mean(d2$weight) 

plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , xlab="weight" , ylab="height" ) +
abline( h=0 , lty=2 ) +
abline( h=272 , lty=1 , lwd=0.5 ) +
mtext( "b ~ dnorm(0,10)" )
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) , 
                        from=min(d2$weight) , 
                        to=max(d2$weight) , add=TRUE , col=col.alpha("black",0.2) )

 

```

The pattern doesn’t look like any human population at all. It essentially says that the relationship between weight and height could be absurdly positive or negative. Before we’ve even seen the data, this is a bad model. Can we do better? We can do better immediately. We know that average height increases with average weight, at least up to a point. Let’s try restricting it to positive values. The easiest way to do this is to define the prior as Log-Normal instead.

If the logarithm of beta  is normal, then beta itself is strictly positive. The reason is that exp(x) is greater than zero for any real number x. This is the reason that Log-Normal priors are commonplace.

What does it mean that the density function of b can exceed 1?

```{r}
b <- rlnorm( 1e4 , 0 , 1 ) 
dens( b , xlim=c(0,5) , adj=0.1 )
```

```{r}
# just curious...
dens(exp(b), xlim=c(0,100) , adj=0.1 )
```

```{r}
set.seed(2971) 
N <- 100 # 100 lines 
a <- rnorm( N , 178 , 20 ) 
b <- rlnorm( N , 0 , 1 )
xbar <- mean(d2$weight) 

plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , xlab="weight" , ylab="height" ) +
abline( h=0 , lty=2 ) +
abline( h=272 , lty=1 , lwd=0.5 ) +
mtext( "b ~ dnorm(0,10)" )
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) , 
                        from=min(d2$weight) , 
                        to=max(d2$weight) , add=TRUE , col=col.alpha("black",0.2) )
```

```{r}
library(rethinking) 
data(Howell1) 
d <- Howell1 
d2 <- d[ d$age >= 18 , ] 
# define the average weight, x-bar 
xbar <- mean(d2$weight) # fit model 

m4.3 <- quap(
  alist(
height ~ dnorm( mu , sigma ) , 
mu <- a + b*( weight - xbar ) ,
a ~ dnorm( 178 , 20 ) , 
b ~ dlnorm( 0 , 1 ) ,  # remeber b here is log b
sigma ~ dunif( 0 , 50 ) ), 
data=d2 )

m4.3b <- quap(
  alist(
height ~ dnorm( mu , sigma ) , 
mu <- a + exp(log_b)*( weight - xbar ) ,
a ~ dnorm( 178 , 20 ) , 
log_b ~ dlnorm( 0 , 1 ) ,  # remeber b here is log b
sigma ~ dunif( 0 , 50 ) ), 
data=d2 )
```

The first row gives the quadratic approximation for alpha, the second the approximation for beta, and the third approximation for sigma.

Since  beta is a slope, the value 0.90 can be read as a person 1 kg heavier is expected to be 0.90 cm taller. 89% of the posterior probability lies between 0.84 and 0.97. 89% because just because, just as arbiterary as 95%.

That suggests that  beta values close to zero or greatly above one are highly incompatible with these data and this model. It is most certainly not evidence that the relationship between weight and height is linear, because the model only considered lines. It just says that, if you are committed to a line, then lines with a slope around 0.9 are plausible ones.

```{r}
precis( m4.3 )
```

Remember, the numbers in the default precis output aren’t sufficient to describe the quadratic posterior completely. For that, we also require the variance-covariance matrix. You can see the covariances among the parameters with vcov:

```{r}
round( vcov( m4.3 ) , 3 )
```

Very little covariation among the parameters in this case. Using pairs(m4.3) shows both the marginal posteriors and the covariance. In the problems at the end of the chapter, you’ll see that the lack of covariance among the parameters results from the something called centering.

```{r}
pairs(m4.3)
```

### plotting inference against the data

It’s almost always much more useful to plot the posterior inference against the data. Not only does plotting help in interpreting the posterior, but it also provides an informal check on model assumptions. When the model’s predictions don’t come close to key observations or patterns in the plotted data, then you might suspect the model either did not fit correctly or is rather badly specified. But even if you only treat plots as a way to help in interpreting the posterior, they are invaluable.

We’re going to start with a simple version of that task, superimposing just the posterior mean values over the height and weight data. Then we’ll slowly add more and more information to the prediction plots, until we’ve used the entire posterior distribution. We’ll start with just the raw data and a single line. The code below plots the raw data, computes the posterior mean values for a and b, then draws the implied line:

```{r}
plot( height ~ weight , data=d2 , col=rangi2 ) 
post <- extract.samples( m4.3 ) 
a_map <- mean(post$a) 
b_map <- mean(post$b) 
curve( a_map + b_map*(x - xbar) , add=TRUE )
```

Plots of the average line, like Figure 4.6, are useful for getting an impression of the magnitude of the estimated influence of a variable. But they do a poor job of communicating uncertainty. Remember, the posterior distribution considers every possible regression line connecting height to weight. It assigns a relative plausibility to each. This means that each combination of alpha and beta has a posterior probability. It could be that there are many lines with nearly the same posterior probability as the average line. Or it could be instead that the posterior distribution is rather narrow near the average line.

what are the parameters for each line? Lets look at 10 of them:

```{r}
post <- extract.samples( m4.3 ) 
post[1:10,]
```

Now we want to add uncertainty around the mean, by showing the scatter of lines around the average line (that we plotted).

```{r}
N <- 10 
dN <- d2[ 1:N , ] 

mN <- quap( 
  alist( height ~ dnorm( mu , sigma ) ,
         mu <- a + b*( weight - mean(weight) ) , 
         a ~ dnorm( 178 , 20 ) , 
         b ~ dlnorm( 0 , 1 ) , 
         sigma ~ dunif( 0 , 50 ) ) , data=dN )

# extract 20 samples from the posterior 
post <- extract.samples( mN , n=20 ) 
a_avg <- mean(extract.samples( mN)$a) 
b_avg <- mean(extract.samples( mN)$b) 

# display raw data and sample size 
plot( dN$weight , dN$height , xlim=range(d2$weight) , ylim=range(d2$height) , col=rangi2 , xlab="weight" , ylab="height" )+
  mtext(concat("N = ",N)) 
  curve( a_avg + b_avg*(x -  mean(dN$weight)) , add=TRUE ,  col=col.alpha("red",1))
# plot the lines, with transparency 
for ( i in 1:20 ) curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) , col=col.alpha("black",0.3) , add=TRUE )
```

Lets try again with 150 N. Yes, this looks better with respect to less uncertainty with more drawn samples from the posterior distribution. Look how th lines are more dispersed at the more extreme weights (low or high).

```{r}

N <- 150
dN <- d2[ 1:N , ] 

mN <- quap( 
  alist( height ~ dnorm( mu , sigma ) ,
         mu <- a + b*( weight - mean(weight) ) , 
         a ~ dnorm( 178 , 20 ) , 
         b ~ dlnorm( 0 , 1 ) , 
         sigma ~ dunif( 0 , 50 ) ) , data=dN )

# extract 20 samples from the posterior 
post <- extract.samples( mN , n=20 ) 
a_avg <- mean(extract.samples( mN)$a) 
b_avg <- mean(extract.samples( mN)$b) 

# display raw data and sample size 
plot( dN$weight , dN$height , xlim=range(d2$weight) , ylim=range(d2$height) , col=rangi2 , xlab="weight" , ylab="height" )+
  mtext(concat("N = ",N)) 
  curve( a_avg + b_avg*(x -  mean(dN$weight)) , add=TRUE ,  col=col.alpha("red",1))
# plot the lines, with transparency 
for ( i in 1:20 ) curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) , col=col.alpha("black",0.3) , add=TRUE )
```

For uncertainty around a mu_i given alpha something. Here x_i = 50 kg.

\mu\_i = \alpha + \beta \*(x_i - x_bar). And since the distributions of alpha and beta are Gaussian, so to is the distribution of mu (adding Gaussian distributions always produces a Gaussian distribution).

Focus for the moment on a single weight value, say 50 kilograms. You can quickly make a list of 10,000 values of mu for an individual who weighs 50 kilograms, by using your samples from the posterior:

```{r}
post <- extract.samples( m4.3 )
xbar <- mean(d2$weight) 
mu_at_50 <- post$a + post$b * ( 50 - xbar )


dens( mu_at_50 , col=rangi2 , lwd=2 , xlab="mu|weight=50" )
```

What these numbers mean is that the central 89% of the ways for the model to produce the data place the average height between about 159 cm and 160 cm (conditional on the model and data), assuming the weight is 50 kg.

```{r}
PI( mu_at_50 , prob=0.89 )  

```

That’s good so far, but we need to repeat the above calculation for every weight value on the horizontal axis, not just when it is 50 kg. We want to draw 89% intervals around the average slope.

This is made simple by strategic use of the link function, a part of the rethinking package. What link will do is take your quap approximation, sample from the posterior distribution, and then compute  for each case in the data and sample from the posterior distribution

```{r}
mu <- link( m4.3 ) 
str(mu)
```

You end up with a big matrix of values of mu. Each row is a sample from the posterior distribution. The default is 1000 samples, but you can use as many or as few as you like. Each column is a case (row) in the data. There are 352 rows in d2, corresponding to 352 individuals. So there are 352 columns in the matrix mu above.

Now what can we do with this big matrix? Lots of things. The function link provides a posterior distribution of  for each case we feed it. So above we have a distribution of  for each individual in the original data. We actually want something slightly different: a distribution of  for each unique weight value on the horizontal axis. It’s only slightly harder to compute that, by just passing link some new data:

```{r}
# define sequence of weights to compute predictions for 
# these values will be on the horizontal axis 
weight.seq <- seq( from=25 , to=70 , by=1 ) 

weight.seq
```

And now there are only 46 columns in mu, because we fed it 46 different values for weight. To visualize what you’ve got here, let’s plot the distribution of mu values at each height.

```{r}
# use link to compute mu # for each sample from posterior # and for each weight in weight.seq 
mu <- link( m4.3 , data=data.frame(weight=weight.seq) ) 
str(mu) 
```

```{r}
# use type="n" to hide raw data 
plot( height ~ weight , d2 , type="n" ) 
# loop over samples and plot each mu value 
for ( i in 1:100 ) points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )
```

The final step is to summarize the distribution for each weight value. We’ll use apply, which applies a function of your choice to a matrix.

Read apply(mu,2,mean) as compute the mean of each column (dimension “2”) of the matrix mu. Now mu.mean contains the average mu at each weight value, and mu.PI contains 89% lower and upper bounds for each weight value.

```{r}
# summarize the distribution of mu 
mu.mean <- apply( mu , 2 , mean ) 
mu.PI <- apply( mu , 2 , PI , prob=0.89 )

mu.mean
```

```{r}
# plot raw data 
# use type="n" to hide raw data 
plot( height ~ weight , d2 , type="n" )
# loop over samples and plot each mu value 
for ( i in 1:100 ) points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )


# fading out points to make line and interval more visible 
plot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) ) 

# plot the MAP line, aka the mean mu for each weight 
lines( weight.seq , mu.mean ) # lines(x_vec,y_vec)
# plot a shaded region for 89% PI 
shade( mu.PI , weight.seq )
```

Overconfident confidence intervals. The confidence interval for the regression line in Figure 4.9 clings tightly to the MAP line. Thus there is very little uncertainty about the average height as a function of average weight. But you have to keep in mind that these inferences are always conditional on the model. Even a very bad model can have very tight confidence intervals. It may help if you think of the regression line in Figure 4.9 as saying: Conditional on the assumption that height and weight are related by a straight line, then this is the most plausible line, and these are its plausible bounds.

To summarize, here’s the recipe for generating predictions and intervals from the posterior of a fit model.

\(1\) Use link to generate distributions of posterior values for mu. The default behavior of link is to use the original data, so you have to pass it a list of new horizontal axis values you want to plot posterior predictions across.

\(2\) Use summary functions like mean or PI to find averages and lower and upper bounds of mu for each value of the predictor variable.

\(3\) Finally, use plotting functions like lines and shade to draw the lines and intervals. Or you might plot the distributions of the predictions, or do further numerical calculations with them. It’s really up to you.

### prediction intervals for actual heights, not only average height

This means we’ll incorporate the standard deviation sigma and its uncertainty as well.

h_i \~ Normal(mu_i; sigma)

What you’ve done so far is just use samples from the posterior to visualize the uncertainty in mu_i, the linear model of the mean. But actual predictions of heights depend also upon the distribution in the first line. The Gaussian distribution on the first line tells us that the model expects observed heights to be distributed around mu, not right on top of it. And the spread around mu is governed by sigma. All of this suggests we need to incorporate sigma in the predictions somehow.

Here’s how you do it. Imagine simulating heights. For any unique weight value, you sample from a Gaussian distribution with the correct mean mu for that weight, using the correct value of sigma sampled from the same posterior distribution. If you do this for every sample from the posterior, for every weight value of interest, you end up with a collection of simulated heights that embody the uncertainty in the posterior as well as the uncertainty in the Gaussian distribution of heights. There is a tool called sim which does this:

```{r}
# Remember link? now we are using sim() instead.

# use link to compute mu # for each sample from posterior # and for each weight in weight.seq 
# mu <- link( m4.3 , data=data.frame(weight=weight.seq) ) 
# str(mu) 


sim.height <- sim( m4.3 , data=list(weight=weight.seq) ) 
str(sim.height)
```

We can summarize these simulated heights in the same way we summarized the distributions of mu, by using apply:

```{r}
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

mu.HPDI <- apply( mu , 2 , HPDI , prob=0.89 )
```

Let’s plot everything we’ve built up: (1) the average line, (2) the shaded region of 89% plausible mu, and (3) the boundaries of the simulated heights the model expects.

```{r}
# plot raw data 
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) ) 

# draw MAP line 
lines( weight.seq , mu.mean ) 

# draw HPDI region for line 
shade( mu.HPDI , weight.seq ) 
# draw PI region for simulated heights 
shade( height.PI , weight.seq )
```

89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions.

The narrow shaded interval around the line is the distribution of mu.

The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight.
