---
title: "PSY9550 Chapter 7"
author: "Alexandra & Winnie"
date: 03-18-2024     # Obs! American date format
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    page-layout: full
    html-math-method: katex
editor: source
---

```{css, echo = F}
body{
  font-family: Helvetica;
  font-size: 16pt;
  max-width: 1000px;
  margin: auto;
  margin-left:310px;
}
pre{
  font-size: 14px;
}
/* Headers */
h1{
    font-size: 24pt;
  }
h1,h2{
    font-size: 20pt;
  }
h3,h4,h5,h6{
  font-size: 18pt;
}
#TOC {
  position: fixed;
  left: 0;
  top: 0;
  width: 300px;
  height: 100%;
  overflow:auto;
}
```

```{r setup,}
#| include: false
#| message: false
#| warning: false
#| results: hide
knitr::opts_chunk$set(echo = TRUE, dpi = 300)

library(rethinking)
library(magrittr)
library(tidyverse)
library(dagitty)

Sys.setlocale("LC_ALL", 'en_US.UTF-8')
setwd("~/00_PhD_lokal/PSY9550_statistical_rethinking/PSY9550_wgma") ### !!!!!

# add other packages you use here
```

# Clarification and or discussion questions

Do not know what we do not know yet.

# Easy

## 7E1. Three criteria information entropy

**State the three motivating criteria that define information entropy. Try to express each in your own words.**

1.  The measure of uncertainty should be continuous. I guess we want it not be a discrete variable that may serve as interval of uncertainties, making us lose information.

2.  Increasing uncertainty with increasing number of possible outcomes that the prediction can take.

3.  Additive. The measure of the uncertainty should be additive, which means that the uncertainty for a combination of events should be the sum of each separate uncertainty.

## 7E2. Weird coin: 70% heads

**Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?**

Entropy of this coin is 0.61.

```{r}
# Entropy = H(p) = -(p1*log(p1)+p2*log(p2))
p_coin <- c(0.7, 0.3)
-sum(p_coin * log(p_coin))
```

## 7E3. Weird four-sided die

**Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, ”3” 25%, and ”4” 30% of the time. What is the entropy of this die?**

Entropy of the 4-sided die 1.38.

```{r}
p_die <- c(0.2, 0.25, 0.25, 0.3)
-sum(p_die * log(p_die))
```

## 7E4. Another weird four-sided die

**Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?**

Entropy of the 4-sided die with 0% for side 4 is 1.10.

```{r}

p_die <- c(1/3, 1/3, 1/3, 0)
-sum(p_die * log(p_die), na.rm = T)

p_die <- c(1/3, 1/3, 1/3)
-sum(p_die * log(p_die))
```

# Medium

## 7M1. Comparison AIC and WAIC

**Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?**

It seems like WAIC is the most general information criterion as AIC is similar to WAIC but assumed flat priors (unregularized).

## 7M2. Model selection vs model comparison

**Explain the difference between model selection and model comparison. What information is lost under model selection?**

In model selection you choose a model and model comparison the act of comparing the models, usually before selecting a model. The book mentions that the comparison stage is important as it makes us keep the models for a longer while so that the relative information in model differences can be kept. In practice this can take the form of using the function compare(), which takes models as input and returns a table with either PSIS or WAIC for each model + other comparison characteristics like standard error.

## 7M3. When comparing models, why need to fit them on the same data?

When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.

## 7M4. (not answered)

What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.

## 7M5. Prior that reduce overfitting

**Provide an informal explanation of why informative priors reduce overfitting.**

With a more informative prior the estimation of the posterior is less influenced by the sample data, and therefore less fitted to the sample. The prior reduces the influence of the data by emphasizing more plausible values.

## 7M6. Prior that result in underfitting

Provide an informal explanation of why overly informative priors result in underfitting.

With a too restrictive prior it may hinder the model in taking the data enough into account.

# Hard (found on internet)

https://sr2-solutions.wjakethompson.com/overfitting-mcmc#chapter-7

## 7H1 tax revenue

**7H1. In 2007, *The Wall Street Journal* published an editorial (“We’re Number One, Alas”) with a graph of corportate tax rates in 29 countries plotted against tax revenue. A badly fit curve was drawn in (reconstructed at right), seemingly by hand, to make the argument that the relationship between tax rate and tax revenue increases and then declines, such that higher tax rates can actually produce less tax revenue. I want you to actually fit a curve to these data, found in `data(Laffer)`. Consider models that use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax revenue.**

First, let’s standardize the data and fit a straight line, a quadratic line, and a spline model.

```{r}
data(Laffer)
library(brms)
# library(stan)

laf_dat <- Laffer %>%
  mutate(across(everything(), standardize),
         tax_rate2 = tax_rate ^ 2)

laf_line <- brm(tax_revenue ~ 1 + tax_rate, data = laf_dat, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234
                # file = here("fits", "chp7", "b7h1-line.rds")
                )

laf_quad <- brm(tax_revenue ~ 1 + tax_rate + tax_rate2, data = laf_dat,
                family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234)

laf_spln <- brm(tax_revenue ~ 1 + s(tax_rate, bs = "bs"), data = laf_dat,
                family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(normal(0, 0.5), class = sds),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                control = list(adapt_delta = 0.95))
```

...and visualizing it.

```{r}
tr_seq <- tibble(tax_rate = seq(0, 40, length.out = 100)) %>%
  mutate(tax_rate = (tax_rate - mean(Laffer$tax_rate)) / sd(Laffer$tax_rate),
         tax_rate2 = tax_rate ^ 2)

predictions <- bind_rows(
  predicted_draws(laf_line, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Linear"),
  predicted_draws(laf_quad, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Quadratic"),
  predicted_draws(laf_spln, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Spline")
)

fits <- bind_rows(
  epred_draws(laf_line, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Linear"),
  epred_draws(laf_quad, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Quadratic"),
  epred_draws(laf_spln, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Spline")
)

ggplot() +
  facet_wrap(~type, nrow = 1) +
  geom_ribbon(data = predictions,
              aes(x = tax_rate, ymin = .lower, ymax = .upper),
              alpha = 0.2) +
  geom_lineribbon(data = fits,
                  aes(x = tax_rate, y = .epred, ymin = .lower, ymax = .upper),
                  size = 0.6) +
  geom_point(data = laf_dat, aes(x = tax_rate, y = tax_revenue),
             alpha = 0.5) +
  scale_fill_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),
                    breaks = c(0.67, 0.89, 0.97)) +
  labs(x = "Standardized Tax Rate", y = "Standardized Tax Revenue",
       fill = "Interval")
```

They all look pretty similar, but the quadratic and spline models do show a slight curve. Next, we can look at the PSIS (called LOO in {brms} and {rstan}) and WAIC comparisons. Neither the PSIS or WAIC is really able to differentiate the models in a meaningful way. However, it should be noted that both the PSIS and WAIC have Pareto or penalty values that are exceptionally large, which could make the criteria unreliable.

```{r}
library(loo)

laf_line <- add_criterion(laf_line, criterion = c("loo", "waic"))
laf_quad <- add_criterion(laf_quad, criterion = c("loo", "waic"))
laf_spln <- add_criterion(laf_spln, criterion = c("loo", "waic"))

loo_compare(laf_line, laf_quad, laf_spln, criterion = "waic")
#>          elpd_diff se_diff
#> laf_quad  0.0       0.0   
#> laf_spln -0.1       0.6   
#> laf_line -0.9       0.9
loo_compare(laf_line, laf_quad, laf_spln, criterion = "loo")
#>          elpd_diff se_diff
#> laf_spln  0.0       0.0   
#> laf_quad  0.0       0.7   
#> laf_line -0.8       0.9
#>
```

## 7H2 tax revenue outliers

**7H2.** In the `Laffer` data, there is one country with a high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem. Then use robust regression with a Student’s t distribution to revist the curve fitting problem. How much does a curved relationship depend upon the outlier point.

Because I used [`brms::brm()`](https://rdrr.io/pkg/brms/man/brm.html) to estimate the models, we can’t use the convenience functions to get the pointwise values for the PSIS and WAIC that are available in the {rethinking} package. So I’ll write my own, called `criteria_influence()`. When we plot the Pareto *k* and pWAIC�WAIC values, we see that observation 12 is problematic in all three models.

```{r}
library(gghighlight)

criteria_influence <- function(mod) {
  tibble(pareto_k = mod$criteria$loo$diagnostics$pareto_k,
         p_waic = mod$criteria$waic$pointwise[, "p_waic"]) %>%
    rowid_to_column(var = "obs")
}

influ <- bind_rows(
  criteria_influence(laf_line) %>%
    mutate(type = "Linear"),
  criteria_influence(laf_quad) %>%
    mutate(type = "Quadratic"),
  criteria_influence(laf_spln) %>%
    mutate(type = "Spline")
)

ggplot(influ, aes(x = pareto_k, y = p_waic)) +
  facet_wrap(~type, nrow = 1) +
  geom_vline(xintercept = 0.7, linetype = "dashed") +
  geom_hline(yintercept = 0.4, linetype = "dashed") +
  geom_point() +
  gghighlight(pareto_k > 0.7 | p_waic > 0.4, n = 1, label_key = obs,
              label_params = list(size = 3)) +
  labs(x = "Pareto *k*", y = "p<sub>WAIC</sub>")
```

Let’s refit the model using a Student’s t distribution to put larger tails on our outcome distribution, and then visualize our new models.

```{r}
laf_line2 <- brm(bf(tax_revenue ~ 1 + tax_rate, nu = 1),
                 data = laf_dat, family = student,
                 prior = c(prior(normal(0, 0.2), class = Intercept),
                           prior(normal(0, 0.5), class = b),
                           prior(exponential(1), class = sigma)),
                 iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                 file = here("fits", "chp7", "b7h2-line.rds"))

laf_quad2 <- brm(bf(tax_revenue ~ 1 + tax_rate + tax_rate2, nu = 1),
                 data = laf_dat, family = student,
                 prior = c(prior(normal(0, 0.2), class = Intercept),
                           prior(normal(0, 0.5), class = b),
                           prior(exponential(1), class = sigma)),
                 iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                 file = here("fits", "chp7", "b7h2-quad.rds"))

laf_spln2 <- brm(bf(tax_revenue ~ 1 + s(tax_rate, bs = "bs"), nu = 1),
                 data = laf_dat, family = student,
                 prior = c(prior(normal(0, 0.2), class = Intercept),
                           prior(normal(0, 0.5), class = b),
                           prior(normal(0, 0.5), class = sds),
                           prior(exponential(1), class = sigma)),
                 iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                 control = list(adapt_delta = 0.99),
                 file = here("fits", "chp7", "bh72-spln.rds"))
```

...visualize it.

```{r}
predictions <- bind_rows(
  predicted_draws(laf_line2, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Linear"),
  predicted_draws(laf_quad2, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Quadratic"),
  predicted_draws(laf_spln2, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Spline")
)

fits <- bind_rows(
  epred_draws(laf_line2, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Linear"),
  epred_draws(laf_quad2, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Quadratic"),
  epred_draws(laf_spln2, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Spline")
)

ggplot() +
  facet_wrap(~type, nrow = 1) +
  geom_ribbon(data = predictions,
              aes(x = tax_rate, ymin = .lower, ymax = .upper),
              alpha = 0.2) +
  geom_lineribbon(data = fits,
                  aes(x = tax_rate, y = .epred, ymin = .lower, ymax = .upper),
                  size = 0.6) +
  geom_point(data = laf_dat, aes(x = tax_rate, y = tax_revenue),
             alpha = 0.5) +
  scale_fill_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),
                    breaks = c(0.67, 0.89, 0.97)) +
  labs(x = "Standardized Tax Rate", y = "Standardized Tax Revenue",
       fill = "Interval")
```

The prediction intervals are a little bit narrower, which makes sense as the predictions are no longer being as influenced by the outlier. When we look at the new PSIS and WAIC estimates, we are no longer getting warning messages about large Pareto *k* values; however, we do still see warnings about large p_WAIC values. The comparisons also tell the same story as before, with no distinguishable differences between the models.

```{r}
laf_line2 <- add_criterion(laf_line2, criterion = c("loo", "waic"))
laf_quad2 <- add_criterion(laf_quad2, criterion = c("loo", "waic"))
laf_spln2 <- add_criterion(laf_spln2, criterion = c("loo", "waic"))

loo_compare(laf_line2, laf_quad2, laf_spln2, criterion = "waic")
#>           elpd_diff se_diff
#> laf_quad2  0.0       0.0   
#> laf_spln2 -0.3       1.3   
#> laf_line2 -1.1       1.7
loo_compare(laf_line2, laf_quad2, laf_spln2, criterion = "loo")
#>           elpd_diff se_diff
#> laf_quad2  0.0       0.0   
#> laf_spln2 -0.2       1.2   
#> laf_line2 -1.1       1.7
```

> ## 7.3 polynesian islands + birds
>
> **7H3.** Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important bird species:

|          | Species A | Species B | Species C | Species D | Species E |
|:---------|:---------:|:---------:|:---------:|:---------:|:---------:|
| Island 1 |   0.200   |   0.200   |   0.200   |   0.200   |   0.200   |
| Island 2 |   0.800   |   0.100   |   0.050   |   0.025   |   0.025   |
| Island 3 |   0.050   |   0.150   |   0.700   |   0.050   |   0.050   |

> Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally complicated. But it is conceptually tricky. First, compute the entropy of each island’s bird distribution. Interpret these entropy values. Second, use each island’s bird distribution to predict the other two. This means to compute the KL divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different KL divergence values. Which island predicts the others best? Why?

First, lets compute the entropy for each each island.

```{r}

#earlier function code that is reused in this example.
p_logp <- function(p) {
  if (p == 0) return(0)
  p * log(p)
}

calc_entropy <- function(x) {
  avg_logprob <- sum(map_dbl(x, p_logp))
  -1 * avg_logprob
}


# setting up island data
islands <- tibble(island = paste("Island", 1:3),
       a = c(0.2, 0.8, 0.05),
       b = c(0.2, 0.1, 0.15),
       c = c(0.2, 0.05, 0.7),
       d = c(0.2, 0.025, 0.05),
       e = c(0.2, 0.025, 0.05)) %>%
  pivot_longer(-island, names_to = "species", values_to = "prop")

islands %>%
  group_by(island) %>%
  summarize(prop = list(prop), .groups = "drop") %>%
  mutate(entropy = map_dbl(prop, calc_entropy)) 
#> # A tibble: 3 × 3
#>   island   prop      entropy
#>   <chr>    <list>      <dbl>
#> 1 Island 1 <dbl [5]>   1.61 
#> 2 Island 2 <dbl [5]>   0.743
#> 3 Island 3 <dbl [5]>   0.984
```

The first island has the highest entropy. This is expected, because it has the most even distribution of bird species. All species are equally likely, so the observation of any one species is not surprising. In contrast, Island 2 has the lowest entropy. This is because the vast majority of birds on this island are Species A. Therefore, observing a bird that is not from Species A would be surprising.

For the second part of the question, we need to compute the KL divergence for each pair of islands. The KL divergence is defined as:

![](images/clipboard-1685272649.png)

We’ll write a function to do this calculation.

```{r}
d_kl <- function(p, q) {
  sum(p * (log(p) - log(q)))
}
```

Now, let’s calculate D_KL for each set of islands.

```{r}
crossing(model = paste("Island", 1:3),
         predicts = paste("Island", 1:3)) %>%
  filter(model != predicts) %>%
  
  left_join(islands, by = c("model" = "island")) %>% # get the data
  rename(model_prop = prop) %>%
  
  left_join(islands, by = c("predicts" = "island", "species")) %>%
  rename(predict_prop = prop) %>%
  
  group_by(model, predicts) %>%
  summarize(q = list(model_prop),
            p = list(predict_prop),
            .groups = "drop") %>%
  mutate(kl_distance = map2_dbl(p, q, d_kl))
#> # A tibble: 6 × 5
#>   model    predicts q         p         kl_distance
#>   <chr>    <chr>    <list>    <list>          <dbl>
#> 1 Island 1 Island 2 <dbl [5]> <dbl [5]>       0.866
#> 2 Island 1 Island 3 <dbl [5]> <dbl [5]>       0.626
#> 3 Island 2 Island 1 <dbl [5]> <dbl [5]>       0.970
#> 4 Island 2 Island 3 <dbl [5]> <dbl [5]>       1.84 
#> 5 Island 3 Island 1 <dbl [5]> <dbl [5]>       0.639
#> 6 Island 3 Island 2 <dbl [5]> <dbl [5]>       2.01
```

These results show us that when using Island 1 to predict Island 2, the KL divergence is about 0.87. When we use Island 1 to predict Island 3, the KL divergence is about 0.63, and so on. Overall, the distances are shorter when we used Island 1 as the model. This is because Island 1 has the highest entropy. Thus, we are less surprised by the other islands, so there’s a shorter distance. In contrast, Island 2 and Island 3 have very concentrated distributions, so predicting the other islands leads to more surprises, and therefore greater distances.
