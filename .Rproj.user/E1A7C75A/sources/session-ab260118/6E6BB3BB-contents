---
title: "PSY9550 Chapter 7"
author: "Alexandra & Winnie"
date: 03-18-2024     # Obs! American date format
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    page-layout: full
    html-math-method: katex
editor: source
---

```{css, echo = F}
body{
  font-family: Helvetica;
  font-size: 16pt;
  max-width: 1000px;
  margin: auto;
  margin-left:310px;
}
pre{
  font-size: 14px;
}
/* Headers */
h1{
    font-size: 24pt;
  }
h1,h2{
    font-size: 20pt;
  }
h3,h4,h5,h6{
  font-size: 18pt;
}
#TOC {
  position: fixed;
  left: 0;
  top: 0;
  width: 300px;
  height: 100%;
  overflow:auto;
}
```

```{r setup,}
#| include: false
#| message: false
#| warning: false
#| results: hide
knitr::opts_chunk$set(echo = TRUE, dpi = 300)

library(rethinking)
library(magrittr)
library(tidyverse)
library(dagitty)

Sys.setlocale("LC_ALL", 'en_US.UTF-8')

# add other packages you use here
```

# Clarification and or discussion questions

Do not know what we do not know yet.

# Easy

## 7E1. Three criteria information entropy

**State the three motivating criteria that define information entropy. Try to express each in your own words.**

1.  The measure of uncertainty should be continuous. I guess we want it not be a discrete variable that may serve as interval of uncertainties, making us lose information.

2.  Increasing uncertainty with increasing number of possible outcomes that the prediction can take.

3.  Additive. The measure of the uncertainty should be additive, which means that the uncertainty for a combination of events should be the sum of each separate uncertainty.

## 7E2. Weird coin: 70% heads

**Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?**

Entropy of this coin is 0.61.

```{r}
# Entropy = H(p) = -(p1*log(p1)+p2*log(p2))
p_coin <- c(0.7, 0.3)
-sum(p_coin * log(p_coin))
```

## 7E3. Weird four-sided die

**Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, ”3” 25%, and ”4” 30% of the time. What is the entropy of this die?**

Entropy of the 4-sided die 1.38.

```{r}
p_die <- c(0.2, 0.25, 0.25, 0.3)
-sum(p_die * log(p_die))
```

## 7E4. Another weird four-sided die

**Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?**

Entropy of the 4-sided die with 0% for side 4 is 1.10.

```{r}

p_die <- c(1/3, 1/3, 1/3, 0)
-sum(p_die * log(p_die), na.rm = T)

p_die <- c(1/3, 1/3, 1/3)
-sum(p_die * log(p_die))
```

# Medium

## 7M1. Comparison AIC and WAIC

**Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?**

It seems like WAIC is the most general information criterion as AIC is similar to WAIC but assumed flat priors (unregularized).

## 7M2. Model selection vs model comparison

**Explain the difference between model selection and model comparison. What information is lost under model selection?**

In model selection you choose a model and model comparison the act of comparing the models, usually before selecting a model. The book mentions that the comparison stage is important as it makes us keep the models for a longer while so that the relative information in model differences can be kept. In practice this can take the form of using the function compare(), which takes models as input and returns a table with either PSIS or WAIC for each model + other comparison characteristics like standard error.

## 7M3. When comparing models, why need to fit them on the same data?

When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.

## 7M4. (not answered)

What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.

## 7M5. Prior that reduce overfitting

**Provide an informal explanation of why informative priors reduce overfitting.**

With a more informative prior the estimation of the posterior is less influenced by the sample data, and therefore less fitted to the sample. The prior reduces the influence of the data by emphasizing more plausible values.

## 7M6. Prior that result in underfitting

Provide an informal explanation of why overly informative priors result in underfitting.

With a too restrictive prior it may hinder the model in taking the data enough into account.
