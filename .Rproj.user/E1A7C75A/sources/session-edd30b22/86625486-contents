---
title: "quarto_notes_hacks"
author: "Winnie Ma"
format: html
#format: 
 # html: default # ville generert HTML med mulighet for å laste ned PDF fra nettsiden.
  #pdf: default
editor: visual
bibliography: references.bib
html:
    toc: true
    toc-location: left
    embed-resources: true
    page-layout: full
    html-math-method: katex
    number-sections: true
    self-contained: true
    code-fold: true
---

https://github.com/gbiele/StatRethink

ebook from others: https://bookdown.org/content/4857/ solutions: https://sr2-solutions.wjakethompson.com/overfitting-mcmc#chapter-9

```{r, warning=F}
library(rethinking)
library(magrittr)
library(tidyverse)
library(dagitty)

Sys.setlocale("LC_ALL", 'en_US.UTF-8')
```

```{css, echo = F} body{   font-family: Helvetica;   font-size: 16pt;   max-width: 1000px;   margin: auto;   margin-left:310px; } pre{   font-size: 14px; } /* Headers */ h1{     font-size: 24pt;   } h1,h2{     font-size: 20pt;   } h3,h4,h5,h6{   font-size: 18pt; } #TOC {   position: fixed;   left: 0;   top: 0;   width: 300px;   height: 100%;   overflow:auto; }
```

```{r setup,} #| include: false #| message: false #| warning: false #| results: hide knitr::opts_chunk$set(echo = TRUE, dpi = 300)  library(rethinking) library(magrittr) library(rstan) # add other packages you use here}
```

# Exercises chapter 1&2

-   Chapter 1: 2E1-2E4, 2M1-2M3

```{r}

#define grid 
p_grid <- seq( from=0 , to=1 , length.out=20 ) 
# define prior, p for water
prior <- rep( 1 , 20 ) 

# compute likelihood at each value in grid 
likelihood <- dbinom( 6 , size=9 , prob=p_grid ) 

# compute product of likelihood and prior 

unstd.posterior <- likelihood * prior 
# standardize the posterior, so it sums to 1 

posterior <- unstd.posterior / sum(unstd.posterior)

```

## Plotting

If necessary, you can use a header 2 style (`# [your header text]`) to indicate sub-section of a solution. This will typically not be necessary.

Plotting is very easy. Just write the code for the plot into a code block:

`{r} hist(rnorm(1000, mean = 0, sd = 1),      main = "Histogram of 1000 random draws from the standard normal distribution")}`

`{r} curve(dnorm(x, mean = 0, sd = 1),      from = -4, to = 4, n = 500,      main = "Probability density for the standard normal distribution")}`

`{r} curve(pnorm(x, mean = 0, sd = 1),      from = -4, to = 4, n = 500,      main = "Cumulative density for the standard normal distribution")}`

`{r} curve(qnorm(x, mean = 0, sd = 1),      from = 0, to = 1, n = 500,      main = "Quantile function for the standard normal distribution")}`

`{r} curve(dbinom(6, size = 10, prob = 0.4),      from = 0, to = 1, n = 500,      main = "Probability density for the binomial distribution")}`

## Hide standard output

If you estimate models with `ulam` it is a good idea to do estimation and plotting of model results in separate code blocks.

Use a code block like a following to estimate the model. Here `#| results: hide` makes that the standard output will not be included in the rendered html document.

`{r results='hide'}`

`# #| results: hide`

`# n.cores = ifelse(Sys.info()["sysname"] == "Darwin",4,1)`

`#  # my.fit = ulam( #   alist( #     y ~ dnorm(mu, sigma), #     mu ~ dnorm(0,3), #     sigma ~ dexp(2) #   ), #   data = list(y = rnorm(25)), #   log_lik = TRUE, #   chains = 4) #`

# 

# And do additional steps with the resulting fit object in the following code blocks:

`{r} precis(my.fit)`

`{r} divergent(my.fit)`

## Equations

If you want to show equations, it is sufficient to write for example y = a + b\*x.

If you want show prettier equations, have to use LATEX. For instance `$y = a + b*x$`, where the `$`s indicate start and end of an inline equation, becomes $y = a + b*x$. [Here](https://tilburgsciencehub.com/building-blocks/collaborate-and-share-your-work/write-your-paper/amsmath-latex-cheatsheet/) is a cheat sheet that explains latex commands.

For instance, we can write

-   Greek letter like this `$y = \alpha + \beta*x$`: $y = \alpha + \beta*x$.

-   subscripts by using ``` _``  and superscripts by using ```\^`;`$y_i = \alpha + \beta*x_i + \gamma^2$\`: $y_i = \alpha + \beta*x_i + \gamma^2$.

-   fractions like with `\frac{ ... }{ ... }`: `$P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}$.` becomes $P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}$. this is Bayes rule. The probability of a hypothesis or parameter value $\theta$ given the data is equal to the probability of the data given $\theta$ times the prior probability of the $\theta$, divided by the probability of the data.

If we have multi line expressions, we start and end with two `$` and ude `\\` to indicate line breaks:

```         
$$  y \sim normal(\bar \mu, \sigma)   \\  \bar \mu \sim normal(0,3)  \\  \sigma \sim exponential(1) \\ $$
```

becomes

$$  y \sim normal(\bar \mu, \sigma)   \\  \bar \mu \sim normal(0,3)  \\  \sigma \sim exponential(1) \\ $$

# Discussion in class

## In class chapter 1&2

From class: how to get your prior? It has to be based in reality, and cannot be a crazy estimate/guess, because we know that our posterior is a result of the prior. Reviewers might ask you to do a prior sensitivity analysis of changing priors.

WOW, I suddenly understood the difference between p_grid and prior. Yes, prior is indeed the distribution/probability of the different p's that we have in p_grid. Lets say prior is the probability of people with curly hair, while p_grid gives us the whole range in possible p \[0,1\], and we wanted to know the posterior P(women\|curly). This makes sense as our focus on women with curly hair is very much dependent on the probability of people with curly hair!

The height and details of the posterior is defined by the grid size, the more grid points between 0 and 1, the higher the resolution.

## in class Chapter 3

THe reason why some dont like bayesian statistics, they do not bother to define or find a prior. Fisher tried finding his posterior without the prior, but he didnt find a solution.

would discourage use of linear regression for binary data, because then the model would allow predictions below 0 and values above 1.

# Chapter 1&2 and 3 guido

<https://htmlpreview.github.io/?https://raw.githubusercontent.com/gbiele/StatRethink/master/Chapter1/ExplainBayes.html#we-know-things-before-seeing-the-data>

```{r, warning=F}

library(tidyverse)
library(plotrix) #histstack

prob.success.prior = .2 # guessed expected proportion 
n.prior.obs = 3 # Here, sample size just refers to the number of fellow students you know well enough to guess how they would do.

a.prior = prob.success.prior*n.prior.obs # number of successes
b.prior = (1-prob.success.prior)*n.prior.obs # number of fails
```

```{r}
# using the alfa og beta from the super small friend sample. Simulate a beta distribution by drawing 5000 random points from the beta distribution.

N.sim = 5000
info_prior = rbeta(N.sim, a.prior, b.prior)
# a beta distribution. Prior. P(x), as in P(x|data) = posterior = P(data|x)*P(x)/P(data)

hist(info_prior, 
     breaks = seq(0,1,.05),
     col = "blue",
     main = "Information before seeing data",
     xlab = "Success probability")



```

```{r}

# our sample size of 13

n.obs = 13 # number of responses
successes = 4 # number of participants with a perfect score
prob.success = successes / n.obs
```

```{r}
# we make a random draw of #successes from the binomial distribution made from the 13 sample, and see if it matches 4 (our observed number of successes)

simulated.successes = 
        rbinom(1, n.obs, info_prior[1])
simulated.successes
# well, 10 is quite far off.

```

```{r}

# we try again, but make 250 draws with shifting priors (from prior beta distribution)
simulated.successes = 
        rbinom(250,n.obs,info_prior[1:250])

# we keep the prior probabilities that produced simulated-successes that match with our observed successes (4). Ex: from 250 simulations and diff info_prior, only 19 matched
good.thetas = 
        info_prior[which(simulated.successes == successes)] %>% 
        round(digits = 2)
round(good.thetas, digits = 3)
```

```{r}
filtered.samples = 
        data.frame(prior.value = info_prior) %>% # info_prior is 5000 long
        rowwise() %>% 
        mutate(simulated.successes = rbinom(1,n.obs,prior.value),
               keep = ifelse(simulated.successes == successes,"keep","reject"))


# rowwise and mutate are dplyr functions
filtered.samples$keep = 
  factor(filtered.samples$keep,
         levels = c("reject","keep"))
```

```{r}
filtered.samples %>% 
  histStack(prior.value~keep,.,
            breaks = seq(0,1,.05),
            col = c("blue","purple"),
            xlab = "Success probability",
            main = "Filter prior information",
            legend.pos = "topright")
```

```{r}
# i think this code makes a video

fn = 
  paste(a.prior,b.prior,n.obs,prob.success,"mp4", sep = ".")

my_histStack = function(dt, ylim = NULL) {
  dt %>%
    histStack(prior.value~keep,.,
              ylim = ylim, xlim = c(0,1),
              breaks = seq(0,1,.05),
              col = c("blue","purple"),
              xlab = "Success probability",
              main = "Filter prior information with data",
              legend.pos = "topright", border = NA,
              cex = 1.75, cex.axis = 1.75, cex.lab = 1.75, cex.main = 2)
}
ylim = c(0,
           hist(filtered.samples$prior.value,
                breaks = seq(0,1,.05), plot = FALSE)$count %>% max())

if(!file.exists(fn)) {
  library(plotrix)
  kk = round(seq(1,nrow(filtered.samples), length.out = 500))
  for (i in 1:length(kk)) {
    png(paste0("anim/",sprintf("%03.f", i),".png"),
        width = 700, height = 700)
    par(mar = c(5.1, 6.1, 4.1, 1.1))
    my_histStack(filtered.samples[1:kk[i],], ylim)
    dev.off()
  }
  imgs = list.files("anim", full.names = T)
  library(av)
  av::av_encode_video(imgs, framerate = 30,
                      output = fn)
}
tmp = file.remove("animation.mp4")
tmp = file.copy(fn,"animation.mp4")
```

Standardizing posterior: This distribution is only proportional to the posterior distribution, because the product of posterior and likelihood does not sum up to 1. We can calculate the posterior probability distribution by dividing by the sum.

```{r}
prior_x_likelihood = prior * likelihood
s = sum(prior_x_likelihood)
posterior = prior_x_likelihood/s
c(1/s, sum(posterior))
```

The following figure illustrates how we get from the the un-normalized posterior to the normalized posterior, which sums to 1, by multiplying with a constant, which is just `1/prior_x_likelihood`.

## Grid search

What is the posterior probability of land, given 10 W and 3 L tosses?

First we defined a grid and plot a the prior probability values: Is it true then that our prior tells us it is 2 times the chance for 100% water on Earth than it is actually 50% water.

```{r}
p_grid = seq(0,1,by = .05)
prior = dbeta(p_grid,2,1)
plot(p_grid, prior, type = "h", col = "blue",
     ylab = "density", main = "Prior")


```

### Why beta and not binomial prior?

So, why cant we have a binomial prior? Let's say we have a feeling that W=2, Land = 1. simulate 20 points

```{r}
p_grid = seq(0,1,0.05)
p = 2/3
plot(p_grid, dbinom(2,3, p_grid))

```

So, how does a beta looks like when W =2, Land = 1

```{r}
# why use beta distribution as prior???
# Beta (a,b), a = successes, b = fails
plot(p_grid, dbeta(p_grid, 2,1))

```

...And what about dbeta(p_grid, 20, 20)

```{r}
plot(p_grid, dbeta(p_grid, 20,20))
```

...and what about a=2, b=2...

```{r}

plot(p_grid, dbeta(p_grid, 2,2))
```

Vertical lines indicate the prior plausibility for parameter values in the grid.

Next we calculate the likelihood, i.e. the probability of the data given the model (a binomial distribution), the data (10 W, 3 L) and the parameter p (in p_grid):

```{r}
likelihood = dbinom(10,13,p_grid) 
plot(p_grid, likelihood, type = "h", col = "red",      
     ylab = "density", main = "Likelihood")
```

And now we can calculate the un-normalized posterior as a product of prior and likelihood:

```{r}

posterior = prior * likelihood
plot(p_grid, posterior, type = "h", col = "violet", 
     ylab = "density", main = "Posterior")
```

Vertical lines indicate the posterior plausibility of the parameter values in the grid, given the data and the prior.

Here is a plot with all three together:

```{r}
par(mar = c(5.1,4.1,1,2.1))
ylim = c(0,max(c(likelihood,posterior,prior)))
plot(p_grid, prior, type = "h",col = "blue", 
     ylab = "density", ylim = ylim)
lines(p_grid+.005, likelihood, type = "h", col = "red")
lines(p_grid-.005, posterior, type = "h", col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

We can make the plot easier to view by normalizing all values so that they sum up to 1 for prior, likelihood and posterior. In each distribution, only the relative values at the different points of the grid are relevant!

```{r}
n_prior = prior/sum(prior)
n_likelihood = likelihood/sum(likelihood)
n_posterior = posterior/sum(posterior)
n_ylim = c(0,max(c(n_likelihood,n_posterior,n_prior)))
par(mar = c(5.1,4.1,1,2.1))
plot(p_grid, n_prior, type = "h", col = "blue", 
     ylab = "normalized density", ylim = n_ylim)
lines(p_grid+.005, n_likelihood, type = "h", col = "red")
lines(p_grid-.005, n_posterior, type = "h", col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

This plot helps to understand that for a grid search we take each point of the grid and:

-   determine the prior probability (we used a beta distribution)

-   calculate the likelihood, i.e. the conditional probability of the data (we used the binomial distribution)

-   calculate the posterior by multiplying the prior and the likelihood.

However, usually one would not show distributions with vertical lines. Instead, one just shows their outlines:

```{r}
par(mar = c(5.1,4.1,1,2.1))
plot(p_grid, n_prior, col = "blue", type = "l", 
     ylab = "normalized density", ylim = n_ylim)
lines(p_grid, n_likelihood, col = "red")
lines(p_grid, n_posterior, col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

This distribution is not very smooth. We can simply make it smoother by increasing the gridsize:

```{r}
fp_grid = seq(0,1,by = .001)
f_prior = dbeta(fp_grid,2,1)
f_likelihood = dbinom(10,13,fp_grid)
f_posterior = f_prior * f_likelihood

f_prior = f_prior/sum(f_prior)
f_posterior = f_posterior/sum(f_posterior)
f_likelihood = f_likelihood/sum(f_likelihood)
f_ylim = c(0,max(c(f_likelihood,f_posterior,f_prior)))

par(mar = c(5.1,4.1,1,2.1))
plot(fp_grid, f_prior, type = "l", col = "blue",
     ylab = "normalized density", ylim = f_ylim)
lines(fp_grid, f_likelihood, col = "red")
lines(fp_grid, f_posterior, col = "violet")
legend("topleft",col = c("blue","red","violet"),
       lty = 1, legend = c("Prior","Likelihood","Posterior"), 
       bty = "n")
```

# Chapter 3. guido

<https://htmlpreview.github.io/?https://raw.githubusercontent.com/gbiele/StatRethink/master/Chapter2/Chapter2BG.html>

```{r, warning=T}
library(cursr)
library(cape)
library(coda)
library(mvtnorm)
library(devtools)
library(loo)
library(dagitty)
library(magrittr)
# install.packages(c("coda","mvtnorm","devtools","loo","dagitty"))

```

```{r}


set.seed(123)

# draw the background
draw_pie()
draw_ellipse()

N = 365
is.winter = vector(length = N) # vector to count winter days
is.snow = vector(length = N) # vector to count snow days

for (k in 1:N) {
  # generate random point with custom function
  xy = rpoint_in_circle()
  
  # check if it is a snow day, i.e. in ellipse, with custom function
  is.snow[k] = in_ellipse(xy,h.e,k.e,a.e,b.e,e.rot)
  # check if it is a winter day
  is.winter[k] = xy[1] > 0 & xy[2] < 0
  
  # plot points
  points(xy[1],xy[2],
         pch = ifelse(is.snow[k] == T,8,21), cex = .75,
         bg = ifelse(is.winter[k] == T,"blue","red"),
         col = ifelse(is.winter[k] == T,"blue","red"))
}
legend(.75,.8,
       pch = c(8,21,15,15), bty = "n",
       col = c("black","black","blue","red"),
       legend = c("snow","no snow", "winter", "no winter"))
```

## showing the results of cancer screening

He also show the different ways of calculating probability using two trees: conditional probability, and natural frequency.

```{r}
set.seed(123)
par(mar = c(0,0,0,0))
cols = c("violet","red","orange","green4")
pie(c(1,9,89,901), labels = "", border = NA, col = cols )

legend("topleft",
       bty = "n",
       fill = cols,
       legend = c(
         "False negatitve",
         "True positive",
         "False positive",
         "True Negative"
       ))
```

Beta distribution, how it works.Dbeta(x,1,1) gives a uniform distribution.

```{r}

plot(dbeta(seq(0,1,0.05), 1,1))
```

Beta distribution, how it works.Dbeta(x,1,2) gives a linear downwards.

```{r}

plot(seq(0,1,0.05), 
     dbeta(seq(0,1,0.05), 1,2))
```

## Learning from posterior

```{r}
n_water = 13
n_total = 20
p_grid = seq(0,1,.01)
prior = rep(1,length(p_grid))
likelihood = dbinom(n_water, n_total, prob = p_grid)
posterior = prior*likelihood
posterior = posterior / sum(posterior)
plot(p_grid,posterior,'l',
     main = "Posterior distribution for the proportion of water")
```

```{r}
# generate samples from the posterior distribution
posterior_samples = 
  sample(p_grid,5000, prob = posterior, replace = TRUE)
# simulate data given the median of the 
# posterior distribution for proportion of water
predictions.point = 
  rbinom(5000,n_total, prob = median(posterior_samples))  # 5000 people with globes in their hands with median p water ratio. THey toss it 20 times each.
simplehist(
  predictions.point, 
  xlab = "# predicted water tosses",
  main = "Posterior predictive distribution from median")
points(n_water,0,pch = 17, col = "green3", cex = 3)
points(round(mean(predictions.point)),0, col = "red", pch = 16, cex = 2)
```

However, **in a Bayesian approach, we do not use a point estimates of parameters to generate posterior predictions, but the full posterior distribution.**

```{r}
predictions.dist = rbinom(5000,n_total, prob = posterior_samples)
```

```{r}
simplehist(
  predictions.dist,
  ylim = c(0, max(table(predictions.point))),
  col = "blue",
  xlab = "# predicted water tosses",
  main = "Posterior predictive distribution")
lines(as.numeric(names(table(predictions.point)))+.2,
      table(predictions.point), type ="h", lwd = 3)
points(n_water,0,pch = 17, col = "green3", cex = 2.5)
points(round(mean(predictions.dist)),0, col = "red", pch = 16, cex = 2)
legend("topleft",
       bty = "n",
       col = c("blue","black"), lty = 1, lwd = 3,
       title = "Predictions from posterior",
       legend = c("5000 globes across distribution",
                  "5000 median globes"))

```

We try the wrong likelihood distribution for our case here: poisson. It gives us chances of water ABOVE 20, where 20 is our number of observations.

Now lets redo the analysis, but instead of using the binomial distribution we use another distribution for integer data, the poisson distribution. The sole parameter of the poisson distribution is the mean number of expected counts.

```{r}
m_grid = seq(0,40,.1)
prior = rep(1,length(m_grid))
likelihood = dpois(n_water, lambda = m_grid)
posterior = prior*likelihood
posterior = posterior / sum(posterior)
plot(m_grid,posterior,'l',
     main = "Posterior distribution for expected water tosses.")
```

```{r}
# generate samples from the posterior distribution
posterior_samples = sample(m_grid,5000, prob = posterior, replace = TRUE)
# simulate data given the median of the 
# posterior distribution for proportion of water
predictions.dist = rpois(5000,posterior_samples)
simplehist(
  predictions.dist, xlab = "# predicted water tosses",
  main = "Posterior predictive distribution from 'wrong' model.")
points(round(mean(predictions.dist)),0,pch = 17, col = "green3", cex = 2.5)
points(n_water,0, col = "red", pch = 16, cex = 2)
abline(v = 20.5, lty = 3, col = "magenta")

```

Here is a posterior predictive distribution from another bad model. What aspect of model is problematic?

```{r}
n_water = 13
n_total = 20
p_grid = seq(0,1,.01)
prior = dbeta(p_grid,1,10)
likelihood = dbinom(n_water, n_total, prob = p_grid)
posterior = prior*likelihood
```

Because now it has a MAP at 9 water, when our data observed 13 out of 20. This happens because of our prior that strongly skews it to the left.

```{r}
posterior = posterior / sum(posterior)
posterior_samples = 
  sample(p_grid,5000, prob = posterior, replace = TRUE)
predictions.dist = rbinom(5000,n_total, prob = posterior_samples)
simplehist(
  predictions.dist,
  xlab = "# predicted water tosses",
  main = "Bad posterior predictive distribution (I)")
points(n_water,0,pch = 17, col = "green3", cex = 2.5)
points(round(mean(predictions.dist)),0, col = "red", pch = 16, cex = 2)
```

# Chapter 4 (guido)

smooth scatter plot (looks like heatmap) \>\> scatter plot. Visualizes the density better.

when to accept normal and a little bit of the prior in negative domain, or when to use log-normal distribution. Use log-norm when the original distribution has a centre close to 0, like a poisson or negative-binomial distribution, as a normal prior would give a lot of negative values –\> not helpful.

Can also use lognormal for beta priors when we KNOW that the correlation between the outcome and the predictor is POSITIVE.

Before making the model and estimating the beta, we can make a guess of the linear beta through beta_guess = y2-y1/x2-x1. We afterwards set the prior for this beta with the beta_guess within the range. Guido: do no set beta \~ norm(beta_guess, sd) right away, because we do not want to bias our model.

centered variable x0 = where 0 on x-scale is the average x0.

prediction intervals is NOT the same as confidence intervals (of the model line). Prediction intervals cover the xx% of the predicted values, while the confidence intervals make room for the other possible model lines.

problem when not observed variables which are confounding! Factor analysis, yes, it is used, but the problem is that you have to make assumptions on the weights/loading between the factors and variables.

Table two fallacy in our example when we make the model: mu = alpha + books + D (daily reading time with parents). outcome = child's reading ability

# Chapter 4 (book)

```{r}
library(rethinking) 
data(Howell1) 
d <- Howell1

d2 <- d[ d$age >= 18 , ]

```

have a look at data d

```{r}
precis( d )
```

```{r}

#post1 gives dataframe with only 100 rows
n = 100
rep = 100


post <- tibble(
  # mu = seq(150,160,length.out=n ),
  # sigma = seq(7,9,length.out=n ),
  mu = rep(seq(150,160,length.out=n ), times = rep),
  sigma = rep(seq(7,9,length.out=n ), each = rep),
  LL1 = sapply( 1:nrow(.) , 
                function(i) sum( dnorm( d2$height , mu[i] , sigma[i] , log=TRUE ) ) ) , #10000 calc likelihood of observed height-vector given each 10000 combinations of mu and sigma.
  # LL2 = sum( dnorm( d2$height , mu, sigma , log=TRUE ) ) ,
  prod = LL1 + dnorm( mu , 178 , 20 , TRUE ) + dunif( sigma , 0 , 50 , TRUE ),
  prob = exp( prod - max(prod) )
)


# post2 gives dataframe with 10000 rows.

mu.list <- seq( from=150, to=160 , length.out=100 )
sigma.list <- seq( from=7 , to=9 , length.out=100 )
post2 <- expand.grid( mu=mu.list , sigma=sigma.list ) # expand.grid gir alle kombinasjoner av mu og sigma.
post2$LL <- sapply( 1:100 , function(i) sum( dnorm( d2$height , post2$mu[i] , post2$sigma[i] , log=TRUE ) ) ) 
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) + dunif( post2$sigma , 0 , 50 , TRUE ) 
# 
post2$prob <- exp( post2$prod - max(post2$prod) )
```

**`sapply`** is used to apply a function to each element of the specified vector.

-   The function being applied calculates the log-likelihood for each combination of **`mu`** and **`sigma`** using the **`dnorm`** function.

-   **`dnorm`** computes the density of the normal distribution with parameters **`post2$mu[i]`** (mean) and **`post2$sigma[i]`** (standard deviation) for the values in the **`d2$height`** vector.

-   **`sum`** is used to add up the log-likelihood values for each element in the **`d2$height`** vector.

-   The resulting log-likelihood values are stored in the **`LL`** column of the **`post2`** data frame.

In summary, this code generates a grid of combinations of **`mu`** and **`sigma`**, and for each combination, it calculates the log-likelihood of a set of observed heights (**`d2$height`**) assuming a normal distribution with the corresponding **`mu`** and **`sigma`** values. The results are stored in a data frame (**`post2`**) with columns for **`mu`**, **`sigma`**, and the log-likelihood (**`LL`**).

```{r}
contour_xyz( post2$mu , post2$sigma , post2$prob )
```

```{r}
#?image_xyz
image_xyz( post2$mu , post2$sigma , post2$prob )
```

Get samples from our normal posterior distribution

```{r}
library(rethinking)
sample.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob ) 
sample.mu <- post2$mu[ sample.rows ] 
sample.sigma <- post2$sigma[ sample.rows ]

plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )

# Adjust the plot to your tastes by playing around with cex (character expansion, the size of the points), pch (plot character), and the 0.1 transparency value.
```

```{r}
sample.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE , prob=post2$prob ) 
sample.mu <- post2$mu[ sample.rows ] 
sample.sigma <- post2$sigma[ sample.rows ]

plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
```

For example, to characterize the shapes of the marginal posterior densities of \sigma and \\mu\\ , all we need to do is:

```{r}

dens( sample.mu ) 

```

```{r}
dens( sample.sigma )
```

To summarize the widths of these densities with posterior compatibility intervals, just like in Chapter 3:

```{r}
PI( sample.mu ) 
PI( sample.sigma )

```

Overthinking: Sample size and the normality of \\sigma ’s posterior. Before moving on to using quadratic approximation (quap) as shortcut to all of this inference, it is worth repeating the analysis of the height data above, but now with only a fraction of the original data. The reason to do this is to demonstrate that, in principle, the posterior is not always so Gaussian in shape. There’s no trouble with the mean, mu. For a Gaussian likelihood and a Gaussian prior on mu, the posterior distribution is always Gaussian as well, regardless of sample size. It is the standard deviation sigma that causes problems. So if you care about sigma—often people do not—you do need to be careful of abusing the quadratic approximation.

The deep reasons for the posterior of signa tending to have a long right-hand tail are complex. But a useful way to conceive of the problem is that variances must be positive. As a result, there must be more uncertainty about how big the variance (or standard deviation) is than about how small it is. For example, if the variance is estimated to be near zero, then you know for sure that it can’t be much smaller. But it could be a lot bigger.

```{r}
d3 <- sample( d2$height , size=20 )
```

```{r}
n = 200

post3 <- tibble(
  mu = seq(150,170,length.out=n ),
  sigma = seq(4,20,length.out=n ),
  LL1 = sapply( 1:n , 
                function(i) sum( dnorm( d3 , mu[i] , sigma[i] , log=TRUE ) ) ) ,
  LL2 = sum( dnorm( d3 , mu, sigma , log=TRUE ) ) ,
  prod = LL1 + dnorm( mu , 178 , 20 , TRUE ) + dunif( sigma , 0 , 50 , TRUE ),
  prob = exp( prod - max(prod, na.rm = T) )
)



sample3.rows <- sample( 1:n , size=1e4 , replace=TRUE , prob=post3$prob ) 
sample3.mu <- post3$mu[ sample3.rows ] 
sample3.sigma <- post3$sigma[ sample3.rows ] 

plot( sample3.mu , sample3.sigma , cex=0.5 , col=col.alpha(rangi2,0.1) , xlab="mu" , ylab="sigma" , pch=16 )
```

## Quap()

```{r}
d <- Howell1 
d2 <- d[ d$age >= 18 , ]
```

Now place the R code equivalents into an alist. Here’s an alist of the formulas above:Now place the R code equivalents into an alist. Here’s an alist of the formulas above:

```{r}
flist <- alist( height ~ dnorm( mu , sigma ) , 
                mu ~ dnorm( 178 , 20 ) , 
                sigma ~ dunif( 0 , 50 ) )
```

Fit the model to the data in the data frame d2 with:

```{r}
m4.1 <- quap( flist , data=d2 )
precis( m4.1 )
```

These numbers provide Gaussian approximations for each parameter’s marginal distribution. This means the plausibility of each value of mu, after averaging over the plausibilities of each value of sigma, is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4. The 5.5% and 94.5% quantiles are percentile interval boundaries, corresponding to an 89% compatibility interval. Why 89%? It’s just the default. It displays a quite wide interval, so it shows a high-probability range of parameter values. If you want another interval, such as the conventional and mindless 95%, you can use precis(m4.1,prob=0.95). But I don’t recommend 95% intervals, because readers will have a hard time not viewing them as significance tests.

89 is also a prime number, so if someone asks you to justify it, you can stare at them meaningfully and incant, “Because it is prime.” That’s no worse justification than the conventional justification for 95%.

I encourage you to compare these 89% boundaries to the compatibility intervals from the grid approximation earlier. You’ll find that they are almost identical. When the posterior is approximately Gaussian, then this is what you should expect.

### start values quap()

Start values for quap. quap estimates the posterior by climbing it like a hill. To do this, it has to start climbing someplace, at some combination of parameter values. Unless you tell it otherwise, quap starts at random values sampled from the prior. But it’s also possible to specify a starting value for any parameter in the model.

These start values are good guesses of the rough location of the MAP values.

```{r}
start <- list( mu=mean(d2$height), 
               sigma=sd(d2$height) ) 

m4.1 <- quap( flist , data=d2 , start=start )
```

Alist() vs list(): Note that the list of start values is a regular list, not an alist like the formula list is. The two functions alist and list do the same basic thing: allow you to make a collection of arbitrary R objects. They differ in one important respect: list evaluates the code you embed inside it, while alist does not. So when you define a list of formulas, you should use alist, so the code isn’t executed. But when you define a list of start values for parameters, you should use list, so that code like mean(d2\$height) will be evaluated to a numeric value.

The priors we used before are very weak, both because they are nearly flat and because there is so much data. So I’ll splice in a more informative prior for mu, so you can see the effect. All I’m going to do is change the standard deviation of the prior to 0.1, so it’s a very narrow prior.

```{r}
m4.2 <- quap( alist( 
  height ~ dnorm( mu , sigma ) , 
  mu ~ dnorm( 178 , 0.1 ) , # 0.1 instead of 20
  sigma ~ dunif( 0 , 50 ) ) , data=d2 ) 

precis( m4.2 )
```

```{r}
precis(m4.1)
```

Notice that the estimate for mu has hardly moved off the prior dnorm(178, 0.1) . The prior was very concentrated around 178. So this is not surprising. But also notice that the estimate for sigma has changed quite a lot, even though we didn’t change its prior at all. Once the golem is certain that the mean is near 178—as the prior insists—then the golem has to estimate sigma conditional on that fact. This results in a different posterior for sigma, even though all we changed is prior information about the other parameter (mu).

### sampling from quap()

The above explains how to get a quadratic approximation of the posterior, using quap. But how do you then get samples from the quadratic approximate posterior distribution? The answer is rather simple, but non-obvious, and it requires recognizing that a quadratic approximation to a posterior distribution with more than one parameter dimension—mu and sigma each contribute one dimension—is just a multi-dimensional Gaussian distribution.

Just like a mean and standard deviation (or its square, a variance) are sufficient to describe a one-dimensional Gaussian distribution, a list of means and a matrix of variances and covariances are sufficient to describe a multi-dimensional Gaussian distribution. To see this matrix of variances and covariances, for model m4.1, use:

```{r}
vcov(m4.1)
```

(why is the self-cov not 1?

The above is a variance-covariance matrix. It is the multi-dimensional glue of a quadratic approximation, because it tells us how each parameter relates to every other parameter in the posterior distribution. A variance-covariance matrix can be factored into two elements: (1) a vector of variances for the parameters and (2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others.

Can also decompose it:

```{r}
diag( vcov( m4.1 ) ) 
cov2cor( vcov( m4.1 ) )
```

Each entry shows the correlation, bounded between 1and +1, for each pair of parameters. The 1’s indicate a parameter’s correlation with itself. If these values were anything except 1, we would be worried. The other entries are typically closer to zero, and they are very close to zero in this example. This indicates that learning mu tells us nothing about sigma and likewise that learning sigma tells us nothing about mu. This is typical of simple Gaussian models of this kind. But it is quite rare more generally, as you’ll see in later chapters.

```{r}
post <- extract.samples( m4.1 , n=1e4 ) 
head(post)
```

You end up with a data frame, post, with 10,000 (1e4) rows and two columns, one column for mu and one for sigma. Each value is a sample from the posterior, so the mean and standard deviation of each column will be very close to the MAP values from before. You can confirm this by summarizing the samples:

```{r}
precis(post)
```

About the function extract.samples(), it is based upon mvrnorm. The function rnorm simulates random Gaussian values, while mvrnorm simulates random vectors of multivariate Gaussian values. Here’s how to use it directly to do what extract.samples does:

```{r}
library(MASS) 
post <- mvrnorm( n=1e4 , 
                 mu=coef(m4.1) , 
                 Sigma=vcov(m4.1) )
```

## 4.4 Linear prediction

earlier in this chapter we have only sampled one variable: height. Usually we would like to study relations between variables, and maybe predict a variable based upon the other.

So now let’s look at how height in these Kalahari foragers (the outcome variable) covaries with weight (the predictor variable).

Go ahead and plot height and weight against one another to get an idea of how strongly they covary:

```{r}
plot( d2$height ~ d2$weight )
```

Regression, what does it mean? Regression = models. The term has come to mean using one or more predictor variables to model the distribution of one or more outcome variables. The original use of term, however, arose from anthropologist Francis Galton’s (1822–1911) observation that the sons of tall and short men tended to be more similar to the population mean, hence regression to the mean.73 The causal reasons for regression to the mean are diverse. In the case of height, the causal explanation is a key piece of the foundation of population genetics. But this phenomenon arises statistically whenever individual measurements are assigned a common distribution, leading to shrinkage as each measurement informs the others. In the context of Galton’s height data, attempting to predict each son’s height on the basis of only his father’s height is folly. Better to use the population of fathers. This leads to a prediction for each son which is similar to each father but “shrunk” towards the overall mean.

Log-normal: rlnorm (like rnorm) and dlnorm (dnorm).

```{r}
library(graphics)
set.seed(2971) 
N <- 100 # 100 lines 

# lin_norm <- tibble(
#   a = rnorm( N , 178 , 20 ),
#   b = rnorm( N , 0 , 10 ),
#   x_mean = mean(d2$weight),
#   height_i = a + b*(d2$weight-x_mean)
# )


# plotter alle linjene for d2$weight som x. 
a <- rnorm( N , 178 , 20 )
b <- rnorm( N , 0 , 10 )
xbar <- mean(d2$weight) 

plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , xlab="weight" , ylab="height" ) +
abline( h=0 , lty=2 ) +
abline( h=272 , lty=1 , lwd=0.5 ) +
mtext( "b ~ dnorm(0,10)" )
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) , 
                        from=min(d2$weight) , 
                        to=max(d2$weight) , add=TRUE , col=col.alpha("black",0.2) )

 

```

The pattern doesn’t look like any human population at all. It essentially says that the relationship between weight and height could be absurdly positive or negative. Before we’ve even seen the data, this is a bad model. Can we do better? We can do better immediately. We know that average height increases with average weight, at least up to a point. Let’s try restricting it to positive values. The easiest way to do this is to define the prior as Log-Normal instead.

If the logarithm of beta  is normal, then beta itself is strictly positive. The reason is that exp(x) is greater than zero for any real number x. This is the reason that Log-Normal priors are commonplace.

What does it mean that the density function of b can exceed 1? Winnie: x\*y = 1, usually when x range \< 1.

```{r}
b <- rlnorm( 1e4 , 0 , 1 ) 
dens( b , xlim=c(0,5) , adj=0.1 )
```

```{r}
# just curious...
dens(exp(b), xlim=c(0,100) , adj=0.1 )
```

```{r}
set.seed(2971) 
N <- 100 # 100 lines 
a <- rnorm( N , 178 , 20 ) 
b <- rlnorm( N , 0 , 1 )
xbar <- mean(d2$weight) 

plot( NULL , xlim=range(d2$weight) , ylim=c(-100,400) , xlab="weight" , ylab="height" ) +
abline( h=0 , lty=2 ) +
abline( h=272 , lty=1 , lwd=0.5 ) +
mtext( "b ~ dnorm(0,10)" )
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) , 
                        from=min(d2$weight) , 
                        to=max(d2$weight) , add=TRUE , col=col.alpha("black",0.2) )
```

```{r}
library(rethinking) 
data(Howell1) 
d <- Howell1 
d2 <- d[ d$age >= 18 , ] 
# define the average weight, x-bar 
xbar <- mean(d2$weight) # fit model 

m4.3 <- quap(
  alist(
height ~ dnorm( mu , sigma ) , 
mu <- a + b*( weight - xbar ) ,
a ~ dnorm( 178 , 20 ) , 
b ~ dlnorm( 0 , 1 ) ,  # remeber b here is log b
sigma ~ dunif( 0 , 50 ) ), 
data=d2 )

m4.3b <- quap(
  alist(
height ~ dnorm( mu , sigma ) , 
mu <- a + exp(log_b)*( weight - xbar ) ,
a ~ dnorm( 178 , 20 ) , 
log_b ~ dlnorm( 0 , 1 ) ,  # remeber b here is log b
sigma ~ dunif( 0 , 50 ) ), 
data=d2 )
```

The first row gives the quadratic approximation for alpha, the second the approximation for beta, and the third approximation for sigma.

Since  beta is a slope, the value 0.90 can be read as a person 1 kg heavier is expected to be 0.90 cm taller. 89% of the posterior probability lies between 0.84 and 0.97. 89% because just because, just as arbiterary as 95%.

That suggests that  beta values close to zero or greatly above one are highly incompatible with these data and this model. It is most certainly not evidence that the relationship between weight and height is linear, because the model only considered lines. It just says that, if you are committed to a line, then lines with a slope around 0.9 are plausible ones.

```{r}
precis( m4.3 )
```

Remember, the numbers in the default precis output aren’t sufficient to describe the quadratic posterior completely. For that, we also require the variance-covariance matrix. You can see the covariances among the parameters with vcov:

```{r}
round( vcov( m4.3 ) , 3 )
```

Very little covariation among the parameters in this case. Using pairs(m4.3) shows both the marginal posteriors and the covariance. In the problems at the end of the chapter, you’ll see that the lack of covariance among the parameters results from the something called centering.

```{r}
pairs(m4.3)
```

### plotting inference against the data

It’s almost always much more useful to plot the posterior inference against the data. Not only does plotting help in interpreting the posterior, but it also provides an informal check on model assumptions. When the model’s predictions don’t come close to key observations or patterns in the plotted data, then you might suspect the model either did not fit correctly or is rather badly specified. But even if you only treat plots as a way to help in interpreting the posterior, they are invaluable.

We’re going to start with a simple version of that task, superimposing just the posterior mean values over the height and weight data. Then we’ll slowly add more and more information to the prediction plots, until we’ve used the entire posterior distribution. We’ll start with just the raw data and a single line. The code below plots the raw data, computes the posterior mean values for a and b, then draws the implied line:

```{r}
plot( height ~ weight , data=d2 , col=rangi2 ) 
post <- extract.samples( m4.3 ) 
a_map <- mean(post$a) 
b_map <- mean(post$b) 
curve( a_map + b_map*(x - xbar) , add=TRUE )
```

Plots of the average line, like Figure 4.6, are useful for getting an impression of the magnitude of the estimated influence of a variable. But they do a poor job of communicating uncertainty. Remember, the posterior distribution considers every possible regression line connecting height to weight. It assigns a relative plausibility to each. This means that each combination of alpha and beta has a posterior probability. It could be that there are many lines with nearly the same posterior probability as the average line. Or it could be instead that the posterior distribution is rather narrow near the average line.

what are the parameters for each line? Lets look at 10 of them:

```{r}
post <- extract.samples( m4.3 ) 
post[1:10,]
```

Now we want to add uncertainty around the mean, by showing the scatter of lines around the average line (that we plotted).

```{r}
N <- 10 
dN <- d2[ 1:N , ] 

mN <- quap( 
  alist( height ~ dnorm( mu , sigma ) ,
         mu <- a + b*( weight - mean(weight) ) , 
         a ~ dnorm( 178 , 20 ) , 
         b ~ dlnorm( 0 , 1 ) , 
         sigma ~ dunif( 0 , 50 ) ) , data=dN )

# extract 20 samples from the posterior 
post <- extract.samples( mN , n=20 ) 
a_avg <- mean(extract.samples( mN)$a) 
b_avg <- mean(extract.samples( mN)$b) 

# display raw data and sample size 
plot( dN$weight , dN$height , xlim=range(d2$weight) , ylim=range(d2$height) , col=rangi2 , xlab="weight" , ylab="height" )+
  mtext(concat("N = ",N)) 
  curve( a_avg + b_avg*(x -  mean(dN$weight)) , add=TRUE ,  col=col.alpha("red",1))
# plot the lines, with transparency 
for ( i in 1:20 ) curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) , col=col.alpha("black",0.3) , add=TRUE )
```

Lets try again with 150 N. Yes, this looks better with respect to less uncertainty with more drawn samples from the posterior distribution. Look how th lines are more dispersed at the more extreme weights (low or high).

```{r}

N <- 150
dN <- d2[ 1:N , ] 

mN <- quap( 
  alist( height ~ dnorm( mu , sigma ) ,
         mu <- a + b*( weight - mean(weight) ) , 
         a ~ dnorm( 178 , 20 ) , 
         b ~ dlnorm( 0 , 1 ) , 
         sigma ~ dunif( 0 , 50 ) ) , data=dN )

# extract 20 samples from the posterior 
post <- extract.samples( mN , n=20 ) 
a_avg <- mean(extract.samples( mN)$a) 
b_avg <- mean(extract.samples( mN)$b) 

# display raw data and sample size 
plot( dN$weight , dN$height , xlim=range(d2$weight) , ylim=range(d2$height) , col=rangi2 , xlab="weight" , ylab="height" )+
  mtext(concat("N = ",N)) 
  curve( a_avg + b_avg*(x -  mean(dN$weight)) , add=TRUE ,  col=col.alpha("red",1))
# plot the lines, with transparency 
for ( i in 1:20 ) curve( post$a[i] + post$b[i]*(x-mean(dN$weight)) , col=col.alpha("black",0.3) , add=TRUE )
```

For uncertainty around a mu_i given alpha something. Here x_i = 50 kg.

\mu\_i = \alpha + \beta \*(x_i - x_bar). And since the distributions of alpha and beta are Gaussian, so to is the distribution of mu (adding Gaussian distributions always produces a Gaussian distribution).

Focus for the moment on a single weight value, say 50 kilograms. You can quickly make a list of 10,000 values of mu for an individual who weighs 50 kilograms, by using your samples from the posterior:

```{r}
post <- extract.samples( m4.3 )
xbar <- mean(d2$weight) 
mu_at_50 <- post$a + post$b * ( 50 - xbar )


dens( mu_at_50 , col=rangi2 , lwd=2 , xlab="mu|weight=50" )
```

What these numbers mean is that the central 89% of the ways for the model to produce the data place the average height between about 159 cm and 160 cm (conditional on the model and data), assuming the weight is 50 kg.

```{r}
PI( mu_at_50 , prob=0.89 )  

```

That’s good so far, but we need to repeat the above calculation for every weight value on the horizontal axis, not just when it is 50 kg. We want to draw 89% intervals around the average slope.

This is made simple by strategic use of the link function, a part of the rethinking package. What link will do is take your quap approximation, sample from the posterior distribution, and then compute  for each case in the data and sample from the posterior distribution

```{r}
mu <- link( m4.3 ) 
str(mu)
```

You end up with a big matrix of values of mu. Each row is a sample from the posterior distribution. The default is 1000 samples, but you can use as many or as few as you like. Each column is a case (row) in the data. There are 352 rows in d2, corresponding to 352 individuals. So there are 352 columns in the matrix mu above.

Now what can we do with this big matrix? Lots of things. The function link provides a posterior distribution of  for each case we feed it. So above we have a distribution of  for each individual in the original data. We actually want something slightly different: a distribution of  for each unique weight value on the horizontal axis. It’s only slightly harder to compute that, by just passing link some new data:

```{r}
# define sequence of weights to compute predictions for 
# these values will be on the horizontal axis 
weight.seq <- seq( from=25 , to=70 , by=1 ) 

weight.seq
```

And now there are only 46 columns in mu, because we fed it 46 different values for weight. To visualize what you’ve got here, let’s plot the distribution of mu values at each height.

```{r}
# use link to compute mu # for each sample from posterior # and for each weight in weight.seq 
mu <- link( m4.3 , data=data.frame(weight=weight.seq) ) 
str(mu) 
```

```{r}
# use type="n" to hide raw data 
plot( height ~ weight , d2 , type="n" ) 
# loop over samples and plot each mu value 
for ( i in 1:100 ) points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )
```

The final step is to summarize the distribution for each weight value. We’ll use apply, which applies a function of your choice to a matrix.

Read apply(mu,2,mean) as compute the mean of each column (dimension “2”) of the matrix mu. Now mu.mean contains the average mu at each weight value, and mu.PI contains 89% lower and upper bounds for each weight value.

```{r}
# summarize the distribution of mu 
mu.mean <- apply( mu , 2 , mean ) 
mu.PI <- apply( mu , 2 , PI , prob=0.89 )

mu.mean
```

```{r}
# plot raw data 
# use type="n" to hide raw data 
plot( height ~ weight , d2 , type="n" )
# loop over samples and plot each mu value 
for ( i in 1:100 ) points( weight.seq , mu[i,] , pch=16 , col=col.alpha(rangi2,0.1) )


# fading out points to make line and interval more visible 
plot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) ) 

# plot the MAP line, aka the mean mu for each weight 
lines( weight.seq , mu.mean ) # lines(x_vec,y_vec)
# plot a shaded region for 89% PI 
shade( mu.PI , weight.seq )
```

Overconfident confidence intervals. The confidence interval for the regression line in Figure 4.9 clings tightly to the MAP line. Thus there is very little uncertainty about the average height as a function of average weight. But you have to keep in mind that these inferences are always conditional on the model. Even a very bad model can have very tight confidence intervals. It may help if you think of the regression line in Figure 4.9 as saying: Conditional on the assumption that height and weight are related by a straight line, then this is the most plausible line, and these are its plausible bounds.

To summarize, here’s the recipe for generating predictions and intervals from the posterior of a fit model.

\(1\) Use link to generate distributions of posterior values for mu. The default behavior of link is to use the original data, so you have to pass it a list of new horizontal axis values you want to plot posterior predictions across.

\(2\) Use summary functions like mean or PI to find averages and lower and upper bounds of mu for each value of the predictor variable.

\(3\) Finally, use plotting functions like lines and shade to draw the lines and intervals. Or you might plot the distributions of the predictions, or do further numerical calculations with them. It’s really up to you.

### prediction intervals for actual heights, not only average height

This means we’ll incorporate the standard deviation sigma and its uncertainty as well.

h_i \~ Normal(mu_i; sigma)

What you’ve done so far is just use samples from the posterior to visualize the uncertainty in mu_i, the linear model of the mean. But actual predictions of heights depend also upon the distribution in the first line. The Gaussian distribution on the first line tells us that the model expects observed heights to be distributed around mu, not right on top of it. And the spread around mu is governed by sigma. All of this suggests we need to incorporate sigma in the predictions somehow.

Here’s how you do it. Imagine simulating heights. For any unique weight value, you sample from a Gaussian distribution with the correct mean mu for that weight, using the correct value of sigma sampled from the same posterior distribution. If you do this for every sample from the posterior, for every weight value of interest, you end up with a collection of simulated heights that embody the uncertainty in the posterior as well as the uncertainty in the Gaussian distribution of heights. There is a tool called sim which does this:

```{r}
# Remember link? now we are using sim() instead.

# use link to compute mu # for each sample from posterior # and for each weight in weight.seq 
# mu <- link( m4.3 , data=data.frame(weight=weight.seq) ) 
# str(mu) 


sim.height <- sim( m4.3 , data=list(weight=weight.seq) ) 
str(sim.height)
```

We can summarize these simulated heights in the same way we summarized the distributions of mu, by using apply:

```{r}
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

mu.HPDI <- apply( mu , 2 , HPDI , prob=0.89 )
```

Let’s plot everything we’ve built up: (1) the average line, (2) the shaded region of 89% plausible mu, and (3) the boundaries of the simulated heights the model expects.

```{r}
# plot raw data 
plot( height ~ weight , d2 , col=col.alpha(rangi2,0.5) ) 

# draw MAP line 
lines( weight.seq , mu.mean ) 

# draw HPDI region for line 
shade( mu.HPDI , weight.seq ) 
# draw PI region for simulated heights 
shade( height.PI , weight.seq )
```

89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions.

The narrow shaded interval around the line is the distribution of mu.

The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight.

## 4.5 Curves from lines

The models so far all assume that a straight line describes the relationship. But there’s nothing special about straight lines, aside from their simplicity. We’ll consider to commonplace methods that use the same essential linear regression approach to build curves. The first is polynomial regression. The second is b-splines.

```{r}
library(rethinking) 
data(Howell1) 
d <- Howell1

str(d)
```

Go ahead and plot height against weight. The relationship is visibly curved, now that we’ve included the non-adult individuals. The most common polynomial regression is a parabolic model of the mean: mu_i = a+ b_1xi + b2xi\^2

```{r}
plot(height ~ weight, data = d)
```

![](images/clipboard-4226336805.png)

The confusing issue here is assigning a prior for 2, the parameter on the squared value of x. Unlike 1, we don’t want a positive constraint. In the problems at the end of the chapter, you’ll use prior predictive simulation to understand why. These polynomial parameters are in general very difficult to set realistic priors for, which is another reason to avoid polynomial models.

```{r}
d <- Howell1
# d <- d %>% 
#   mutate(weight_s =  weight - mean(weight) /sd(weight),
#         weight_s2 = weight_s^2 )
# 
d$weight_s <- ( d$weight - mean(d$weight) )/sd(d$weight)
d$weight_s2 <- d$weight_s^2

m4.5 <- quap(
  alist( height ~ dnorm( mu , sigma ) , 
         mu <- a + b1*weight_s + b2*weight_s2 , 
         a ~ dnorm( 178 , 20 ) ,
         b1 ~ dlnorm( 0 , 1 ) , 
         b2 ~ dnorm( 0 , 1 ) , 
         sigma ~ dunif( 0 , 50 ) ), data=d )

precis( m4.5 )
```

Now, since the relationship between the outcome height and the predictor weight depends upon two slopes, b1 and b2, it isn’t so easy to read the relationship off a table of coefficients:

```{r}
weight.seq <- seq( from=-2.2 , to=2 , length.out=30 ) 
pred_dat <- list( weight_s=weight.seq , weight_s2=weight.seq^2 ) 
mu <- link( m4.5 , data=pred_dat ) 
mu.mean <- apply( mu , 2 , mean )

mu.PI <- apply( mu , 2 , PI , prob=0.89 )
sim.height <- sim( m4.5 , data=pred_dat ) 
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )

# then we plot

plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) ) 
lines( weight.seq , mu.mean ) 
shade( mu.PI , weight.seq ) 
shade( height.PI , weight.seq )
```

Now also a cubic regression, order 3. Which basically means it can bend at 3 points

```{r}
d$weight_s3 <- d$weight_s^3 
m4.6 <- quap( 
  alist( height ~ dnorm( mu , sigma ) , 
                     mu <- a + b1*weight_s + b2*weight_s2 + b3*weight_s3 ,
         a ~ dnorm( 178 , 20 ) ,
         b1 ~ dlnorm( 0 , 1 ) ,
         b2 ~ dnorm( 0 , 10 ) , 
         b3 ~ dnorm( 0 , 10 ) , 
         sigma ~ dunif( 0 , 50 )), data = d)

weight.seq <- seq( from=-2.2 , to=2 , length.out=30 ) 
pred_dat <- list( weight_s=weight.seq , weight_s2=weight.seq^2 , weight_s3= weight.seq^3) 

mu <- link( m4.6 , data=pred_dat ) 
mu.mean <- apply( mu , 2 , mean )

mu.PI <- apply( mu , 2 , PI , prob=0.89 )
sim.height <- sim( m4.6 , data=pred_dat ) 
height.PI <- apply( sim.height , 2 , PI , prob=0.89 )



plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5) ) 
mtext("cubic")
lines( weight.seq , mu.mean ) 
shade( mu.PI , weight.seq ) 
shade( height.PI , weight.seq )
```

### b-splines = zig-zag curves (b for basic)

```{r}
library(rethinking) 
data(cherry_blossoms) 
d <- cherry_blossoms 
precis(d)
```

The short explanation of B-splines is that they divide the full range of some predictor variable, like year, into parts. Then they assign a parameter to each part. These parameters are gradually turned on and off in a way that makes their sum into a fancy, wiggly curve.

It runs from the year 839 to 1980. It is very wiggly. You should go ahead and plot temp against year to see. No parabolic curve is going to do the job here.

```{r}
plot(temp~year, data = d)
```

Our goal is to approximate the temperature trend with a wiggly function. With B-splines, just like with polynomial regression, we do this by generating new predictor variables and using those in the linear model, i. Unlike polynomial regression, B-splines do not directly transform the predictor by squaring or cubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of these variables serves to gradually turn a specific parameter on and off within a specific range of the predictor variables. Each of these variables is called a basis function.

![](images/clipboard-2158318590.png)

How do we construct these basis variables B? First, I divide the full range of the horizontal axis into four parts, using pivot points called knots. The knots are shown by the + symbols in the top plot. Then five different basis functions, our B variables, are used to gently transition from one part to the next. Essentially, these variables tell you which knot you are close to. Beginning on the left of the top plot, basis function 1 has value 1 and all of the others are set to zero. As we move rightwards towards the second knot, basis 1 declines and basis 2 increases. At knot 2, basis 2 has value 1, and all of the others are set to zero.

![](images/clipboard-517916497.png)

The nice feature of these basis functions is that they make the the influence of each parameter quite local. At any point on the horizontal axis in Figure 4.12, only two basis functions have non-zero values. For example, the dashed blue line in the top plot shows the year 1306. Basis functions 2 and 3 are non-zero for that year. So the parameters for basis functions 2 and 3 are the only parameters influencing prediction for the year 1306. This is quite unlike polynomial regression, where parameters influence the entire shape of the curve.

Weights: I got these weights by fitting the model to the data. I’ll show you how to do that in a moment. But focus on the figure for now. Weight parameters can be positive or negative. So for example basis functions 2 and 4 end up below the zero line. They have negative weights. To construct a prediction for any given year, say for example 1306 again, we just add up these weighted basis functions at that year. In the year 1306, only basis functions 2 and 3 influence prediction.

Sum weighted basis functions: Finally, in the bottom plot of Figure 4.12, I display the spline, as a 97% posterior interval for , over the raw temperature data. This isn’t yet a very good approximation. There are two things we can do to improve it. First, we can use more knots. The more knots, the more flexible the spline. Second, instead of linear approximations, we can use higher degree polynomials.

# 

# 5 The Many Variables & The Spurious Waffles

<https://rdrr.io/github/rmcelreath/rethinking/src/tests/rethinking_tests/sim%20counterfactual%20test.R>

Waffle House is nearly always open, even just after a hurricane. Most diners invest in disaster preparedness, including having their own electrical generators. As a consequence, the United States’ disaster relief agency (FEMA) informally uses Waffle House as an index of disaster severity.77 If the Waffle House is closed, that’s a serious event.

States with many Waffle Houses per person, like Georgia and Alabama, also have some of the highest divorce rates in the United States. The lowest divorce rates are found where there are zero Waffle Houses. Could always-available waffles and hash brown potatoes put marriage at risk?

Probably not. This is an example of a misleading correlation. No one thinks there is any plausible mechanism by which Waffle House diners make divorce more likely. Instead, when we see a correlation of this kind, we immediately start asking about other variables that are really driving the relationship between waffles and divorce. In this case, Waffle House began in Georgia in the year 1955. Over time, the diners spread across the Southern United States, remaining largely within it. So Waffle House is associated with the South. Divorce is not a uniquely Southern institution, but the Southern United States has some of the highest divorce rates in the nation. So it’s probably just an accident of history that Waffle House and high divorce rates both occur in the South.

```{r}
library(rethinking)
data("WaffleDivorce")
d <- WaffleDivorce
precis(d)


```

```{r}
d %>% 
  mutate(Loc_nonFac = as.character(Loc)) %>% 
  ggplot(aes(x = WaffleHouses, y = Divorce, label = Loc_nonFac)) +
  geom_point(aes(color = South), size = 3) +
  geom_smooth(method = "lm")
```

## why multiple regression

But since most correlations do not indicate causal relationships, we need tools for distinguishing mere association from evidence of causation. This is why so much effort is devoted to multiple regression, using more than one predictor variable to simultaneously model an outcome. Reasons given for multiple regression models include:

-   \(1\) Statistical “control” for confounds.

A confound is something that misleads us about a causal influence—there will be a more precise definition in the next chapter. The spurious waffles and divorce correlation is one possible type of confound, where the confound (southernness) makes a variable with no real importance (Waffle House density) appear to be important. But confounds are diverse. They can hide real important variables just as easily as they can produce false ones.

-   \(2\) Multiple causation.

A phenomenon may arise from multiple causes. Measurement of each cause is useful, so when we can use the same data to estimate more than one type of influence, we should. Furthermore, when causation is multiple, one cause can hide another.

-   \(3\) Interactions.

The importance of one variable may depend upon another. For example, plants benefit from both light and water. But in the absence of either, the other is no benefit at all. Such interactions occur very often. Effective inference about one variable will often depend upon consideration of others.

We will focus on the firste two in this chapter. However, multiple regression can be worse than useless, if we don’t know how to use it. Just adding variables to a model can do a lot of damage. So in this chapter, we’ll begin to think formally about causal inference and introduce graphical causal models as a way to design and interpret regression models.

## 5.1 spurious association

Let’s leave waffles behind, at least for the moment. An example that is easier to understand is the correlation between divorce rate and marriage rate. Age at marriage is also a good predictor of divorce ratehigher age at marriage predicts less divorce.

```{r}
plot(data = d, Divorce  ~ Marriage, col=col.alpha(rangi2,0.7) )
```

```{r}
# standardize variables. A = median age marriage in that state, scaled. D = divorce rate, scaled.
d$A <- scale( d$MedianAgeMarriage ) 
d$D <- scale( d$Divorce )
```

![](images/clipboard-59181640.png)

What about those priors? Since the outcome and the predictor are both standardized, the intercept alpha should end up very close to zero. What does the prior slope beta-A imply? If beta-A = 1, that would imply tha**t a change of one standard deviation in age at marriage is associated likewise with a change of one standard deviation in divorce**. To know whether or not that is a strong relationship, you need to know how big a standard deviation of age at marriage is:

```{r}

# one standard deviation = 1 beta = 1,24 years
sd( d$MedianAgeMarriage )
```

So when beta-A = 1, a change of 1.2 years in median age at marriage is associated with a full standard deviation change in the outcome variable. That seems like an insanely strong relationship. The prior above thinks that only 5% of plausible slopes more extreme than 1.

```{r}
m5.1 <- quap( 
  alist( 
    D ~ dnorm( mu , sigma ) , 
    mu <- a + bA * A , 
    a ~ dnorm( 0 , 0.2 ) , 
    bA ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 ) ) , data = d )

set.seed(10) 
prior <- extract.prior( m5.1 ) 
mu <- link( m5.1 , post=prior , data=list( A=c(-2,2) ) ) 
plot( NULL , xlim=c(-2,2) , ylim=c(-2,2) ) 
for ( i in 1:50 ) lines( c(-2,2) , mu[i,] , col=col.alpha("black",0.4) )
```

We plot the linear model for standardized divorce \~ median marriage age

```{r}
# compute percentile interval of mean 
A_seq <- seq( from=-3 , to=3.2 , length.out=30 ) 
mu <- link( m5.1 , data=list(A=A_seq) ) 
mu.mean <- apply( mu , 2, mean ) 
mu.PI <- apply( mu , 2 , PI )

# plot it all 
plot( D ~ A , data=d , col=rangi2 ) 
lines( A_seq , mu.mean ,lwd=2 ) 
shade( mu.PI , A_seq )
```

...and now for divorce \~ marriage.

```{r}
d$M <- scale( d$Marriage )
m5.2 <- quap( 
  alist( D ~ dnorm( mu , sigma ) , 
         mu <- a + bM * M , 
         a ~ dnorm( 0 , 0.2 ) , 
         bM ~ dnorm( 0 , 0.5 ) , 
         sigma ~ dexp( 1 ) ) , data = d )

# compute percentile interval of mean 
M_seq <- seq( from=-3 , to=3.2 , length.out=30 ) 
mu <- link( m5.2 , data=list(M=M_seq) ) 
mu.mean <- apply( mu , 2, mean ) 
mu.PI <- apply( mu , 2 , PI )

# plot it all
plot( D ~ M , data=d , col=rangi2 )
lines( M_seq , mu.mean , lwd=2 ) 
shade( mu.PI , M_seq )
```

As you can see in the figure, this relationship isn’t as strong as the previous one. But merely comparing parameter means between different bivariate regressions is no way to decide which predictor is better**. Both of these predictors could provide independent value, or they could be redundant, or one could eliminate the value of the other.**

There are three observed variables in play: divorce rate (D), marriage rate (M), and the median age at marriage (A) in each State. To understand this better, it is helpful to introduce a particular type of casual graph known as a DAG, short for directed acyclic graph.

![](images/clipboard-35928475.png)

uld be the indirect effect of age at marriage. To infer the strength of these different arrows, we need more than one statistical model. Model m5.1, the regression of D on A, tells us only that the total influence of age at marriage is strongly negative with divorce rate. The “total” here means we have to account for every path from A to D. There are two such paths in this graph: A -\>D, a direct path, and A -\> M -\> D, an indirect path. In general, it is possible that a variable like A has no direct effect at all on an outcome like D. It could still be associated with D entirely through the indirect path. That type of relationship is known as mediation, and we’ll have another example later.

In this example however, the indirect path does almost no work. How can we show that? We know from m5.2 that marriage rate is positively associated with divorce rate. But that isn’t enough to tell us that the path M -\> D is positive. It could be that the association between M and D arises entirely from A’s influence on both M and D. Like this:

![](images/clipboard-2767827199.png)

Draw DAG

```{r}
library(dagitty) 
dag5.1 <- dagitty( "dag { A -> D A -> M M -> D }") 
coordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) ) 
drawdag( dag5.1 )
```

In these data, all three pairs are in fact strongly associated. Check for yourself. You can use cor() to measure simple correlations. Correlations are very simple and sometimes terrible measures of association—mamy different patterns of association with different implications can produce the same correlation.

![](images/clipboard-3940258894.png)

If you are like most people, this is still pretty mysterious. So it might help to read the + symbols as “or” and then say: A State’s divorce rate can be a function of its marriage rate or its median age at marriage. The “or” indicates independent associations, which may be purely statistical or rather causal.

```{r}
m5.3 <- quap( 
  alist( D ~ dnorm( mu , sigma ) , 
         mu <- a + bM*M + bA*A , 
         a ~ dnorm( 0 , 0.2 ) , 
         bM ~ dnorm( 0 , 0.5 ) ,
         bA ~ dnorm( 0 , 0.5 ) , 
         sigma ~ dexp( 1 ) ) , data = d ) 
precis( m5.3 )
```

The posterior mean for marriage rate, bM, is now close to zero, with plenty of probability of both sides of zero. The posterior mean for age at marriage, bA, is essentially unchanged. It will help to visualize the posterior distributions for all three models, focusing just on the slope parameters beta-A and beta-M

```{r}
plot( coeftab(m5.1,m5.2,m5.3), par=c("bA","bM") )  
```

remember model m5.1 is model divorce \~ age, while, m5.2 was divorce \~ marriage. Notice how bA doesn’t move, only grows a bit more uncertain, while bM is only associated with divorce when age at marriage is missing from the model. You can interpret these distributions as saying: **Once we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State.**

This is so interesting as it really seemed to be a strong relationship between #marriages and divorces.

### 5.1.5 plotting multivariate posteriors

With multivariate regression (and not bivariate), you’ll need more plots.

So the approach I take here is to instead help you compute whatever you need from the model. I offer three examples of interpretive plots:

1.  \(1\) Predictor residual plots. These plots show the outcome against residual predictor values. They are useful for understanding the statistical model, but not much else.

2.  \(2\) Posterior prediction plots. These show model-based predictions against raw data, or otherwise display the error in prediction. They are tools for checking fit and assessing predictions. They are not causal tools.

3.  \(3\) Counterfactual plots. These show the implied predictions for imaginary experiments. These plots allow you to explore the causal implications of manipulating one or more variables.

#### 5.1.5.1 predictor residual plots

this procedure also brings home the message that regression models measure the remaining association of each predictor with the outcome, after already knowing the other predictors. In computing the predictor residual plots, you had to perform those calculations yourself. In the unified multivariate model, it all happens automatically.

In our multivariate model of divorce rate, we have two predictors: (1) marriage rate (M) and (2) median age at marriage (A). To compute predictor residuals for either, we just use the other predictor to model it. so, for marriage rate, we use age in the model:

![](images/clipboard-1687960711.png)

```{r}
m5.4 <- quap( 
  alist( M ~ dnorm( mu , sigma ) , 
         mu <- a + bAM * A , 
         a ~ dnorm( 0 , 0.2 ) , 
         bAM ~ dnorm( 0 , 0.5 ) , 
         sigma ~ dexp( 1 ) ) , 
  data = d )
```

![](images/clipboard-2018562312.png)

Figure 5.4. Understanding multiple regression through residuals. The top row shows each predictor regressed on the other predictor. The lengths of the line segments connecting the model’s expected value of the outcome, the regression line, and the actual value are the residuals. In the bottom row, divorce rate is regressed on the residuals from the top row.

Bottom left: The residual variation in marriage **rate shows no association with divorce rate**. You can think of this plot as displaying the linear relationship between divorce and marriage rates, having statistically “controlled” for median age of marriage. The vertical dashed line indicates marriage rate that exactly matches the expectation from median age at marriage. So States to the right of the line have higher marriage rates than expected. States to the left of the line have lower rates. Average divorce rate on both sides of the line is about the same, and so the regression line demonstrates little relationship between divorce and marriage rates.

Bottom right: Divorce rate on age at marriage residuals, showing remaining variation among the residuals, and this variation is associated with divorce rate. States to the right of the vertical dashed line have older-than-expected median age at marriage, while those to the left have younger-than-expected median age at marriage. Now we find that the average divorce rate on the right is lower than the rate on the left, as indicated by the regression line. States in which people marry older than expected for a given rate of marriage tend to have less divorce.

What does the residuals mean?

When a residual is positive, that means that the observed rate was in excess of what the model expects, given the median age at marriage in that State.In simpler terms, States with positive residuals have high marriage rates for their median age of marriage, while States with negative residuals have low rates for their median age of marriage.

```{r}
mu <- link(m5.4) 
mu_mean <- apply( mu , 2 , mean ) 
mu_resid <- d$M - mu_mean
```

#### 5.1.5.2 posterior prediction plots

```{r}
# call link without specifying new data
# so it uses original data 
mu <- link( m5.3 ) 
# summarize samples across cases 
mu_mean <- apply( mu , 2 , mean ) 
mu_PI <- apply( mu , 2 , PI ) 
# simulate observations # again no new data, so uses original data 
D_sim <- sim( m5.3 , n=1e4 ) 
D_PI <- apply( D_sim , 2 , PI )

plot( mu_mean ~ d$D , col=rangi2 , ylim=range(mu_PI) , 
      xlab="Observed divorce" , ylab="Predicted divorce" ) 
abline( a=0 , b=1 , lty=2 ) 
for ( i in 1:nrow(d) ) lines( rep(d$D[i],2) , mu_PI[,i] , col=rangi2 )

identify( x=d$D , y=mu_mean , labels=d$Loc )
```

It’s easy to see from this arrangement of the simulations that the model under-predicts for States with very high divorce rates while it overpredicts for States with very low divorce rates. That’s normal. This is what regression does—it is skeptical of extreme values, so it expects regression towards the mean.

#### 5.1.5.3 Counterfactual plotting

Once we have multiple predictor variables, we can conduct counterfactual experiments that vary one predictor while holding the others constant. In the real world, such experiments are typically impossible. If we change an animal’s body size, natural selection will then change the other features to match it. But these counterfactual plots do help us see how the model views the association between each predictor and the outcome.

So let’s see how to generate plots of model predictions that take the causal structure into account. The basic recipe is:

\(1\) Pick a variable to manipulate, the intervention variable.

\(2\) Define the range of values to set the intervention variable to.

(3) For each value of the intervention variable, and for each sample in posterior, use the causal model to simulate the values of other variables, including the outcome.

In the end, you end up with a posterior distribution of counterfactual outcomes that you can plot and summarize in various ways, depending upon your goal. Let’s see how to do this for the divorce model. Again we take this DAG as given:

![](images/clipboard-561903630.png)

To simulate from this, we need more than the DAG. We also need a set of functions that tell us how each variable is generated. For simplicity, we’ll use Gaussian distributions for each variable, just like in model m5.3. But model m5.3 ignored the assumption that A influences M. We didn’t need that to estimate A -\> D. But we do need it to predict the consequences of manipulating A, because some of the effect of A acts through M.

To estimate the influence of A on M, all we need is to regress A on M, there are no other variables in the DAG creating an association between A and M. We can just add this regression to the quap() model, running two regressions at the same time:

```{r}
data(WaffleDivorce) 
d <- list( A <- standardize( WaffleDivorce$MedianAgeMarriage ),
           D <- standardize( WaffleDivorce$Divorce ),
           M <- standardize( WaffleDivorce$Marriage )
)
           


          
m5.3_AD <- quap(
  alist(
    # A -> D <- M
    D ~ dnorm( mu , sigma ) ,
    mu <- a + bM*M + bA*A ,  # with the effect of M and A
    a ~ dnorm( 0 , 0.2 ) ,
    bM ~ dnorm( 0 , 0.5 ) ,
    bA ~ dnorm( 0 , 0.5 ) ,
    sigma ~ dexp( 1 )
  ) , data = d )


    ## A -> M    
m5.3_AM <- quap(
  alist(
    M ~ dnorm( mu_M , sigma_M ),
    mu_M <- aM + bAM*A,
    aM ~ dnorm( 0 , 0.2 ),
    bAM ~ dnorm( 0 , 0.5 ),
    sigma_M ~ dexp( 1 )
    ) , data = d )
```

```{r}
precis(m5.3_AD)
```

Look at the precis(5.3_AD) summary. You’ll see that M and A are strongly negatively associated. If we interpret this causally, it indicates that manipulating A reduces M. The goal is to simulate what would happen, if we manipulate A. So next we define a range of values for A. This defines a list of 30 imaginary interventions, ranging from 2 standard deviations below and 2 above the mean.

Now we can use sim(), which you met in the previous chapter, to simulate observations from model m5.3_A. But this time we’ll tell it to simulate both M and D, in that order. Why in that order? Because we have to simulate the influence of A on M before we simulate the joint influence of A and M on D. The vars argument to sim() tells it both which observables to simulate and in which order.

```{r}
# m5.3_AD. Where A -> D <- M

A_seq <- seq( from=-2 , to=2 , length.out=30 )
# prep data
sim_dat <- data.frame( A=A_seq )

# simulate M and then D, using A_seq
s <- sim( m5.3_AD , data=sim_dat , vars=c("M","D") )

# display counterfactual predictions
plot( sim_dat$A , colMeans(s$D) , ylim=c(-2,2) , type="l" , 
    xlab="manipulated A" , ylab="counterfactual D"  )
shade( apply(s$D,2,PI) , sim_dat$A )
mtext( "Total counterfactual effect of A on D" )

```

```{r}
# x = manipulated A, y = counterfactual M

A_seq <- seq( from=-2 , to=2 , length.out=30 )
# prep data
sim_dat <- data.frame( A=A_seq )

# simulate M and then D, using A_seq
s <- sim( m5.3_AM , data=sim_dat , vars=c("M","D") )

# display counterfactual predictions
plot( sim_dat$A , colMeans(s$M) , ylim=c(-2,2) , type="l" , 
    xlab="manipulated A" , ylab="counterfactual M"  )

shade( apply(s$M,2,PI) , sim_dat$A )
mtext( "Total counterfactual effect of A on M" )

```

![](images/clipboard-1190098720.png)

```{r}
    ## m -> D. A = 0.    
m5.3_MD <- quap(
  alist(
    D ~ dnorm( mu , sigma ),
    mu <- a + b*A,
    a ~ dnorm( 0 , 0.2 ),
    b ~ dnorm( 0 , 0.5 ),
    sigma ~ dexp( 1 )
    ) , data = d )

          
# x = manipulated M, y = counterfactual D

M_seq <- seq( from=-2 , to=2 , length.out=30 )
# prep data
sim_dat <- data.frame( M=M_seq , A = 0)  # Mind that A = 0.

# simulate A and then D, using M_seq
s <- sim( m5.3_MD , data=sim_dat , vars=c("D") )  # since A = 0, do not need vars=c("A","D")

# display counterfactual predictions
plot( sim_dat$M , colMeans(s) , ylim=c(-2,2) , type="l" ,   #  colMeans(s$D)
    xlab="manipulated M" , ylab="counterfactual D"  )

shade( apply(s,2,PI) , sim_dat$M )
# shade( apply(s$D,2,PI) , sim_dat$M )
mtext( "Total counterfactual effect of M on D" )
```

doesnt look like the figure in the book...

![](images/clipboard-3272488323.png)

## 5.2 Masked relationship

```{r}
library(rethinking) 
data(milk) 
d <- milk 
str(d)
```

```{r}
d$K <- scale( d$kcal.per.g ) 
d$N <- scale( d$neocortex.perc )
d$M <- scale( log(d$mass) )
```

where K is standardized kilocalories and N is standardized neocortex percent. We still need to consider the priors. But first let’s just try to run this as a quap model with some vague priors,

Each NA in the output is a missing value. If you pass a vector like this to a likelihood function like dnorm, it doesn’t know what to do. After all, what’s the probability of a missing value? Whatever the answer, it isn’t a number, and so dnorm returns a NaN. Unable to even get started, quap (or rather optim, which does the real work) gives up and barks about some weird thing called vmmin not being finite. This kind of opaque error message is unfortunately the norm in R. The additional part of the message suggesting NA values might be responsible is just quap taking a guess.

To just get the complete cases based on these columns

```{r}
 dcc <- d[ complete.cases(d$K,d$N,d$M) , ]
```

```{r}
# vague priors. K = Kilocalories of energy per gram of milk., N = neocortex.perc : The percent of total brain mass that is neocortex mass.
m5.5_draft <- quap( 
  alist( K ~ dnorm( mu , sigma ) , 
         mu <- a + bN*N , 
         a ~ dnorm( 0 , 1 ) , 
         bN ~ dnorm( 0 , 1 ) , 
         sigma ~ dexp( 1 ) ) , data=dcc )

# better priors
m5.5 <- quap( 
  alist( 
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bN*N , 
    a ~ dnorm( 0 , 0.2 ) , 
    bN ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 ) ) , 
  data=dcc )
```

plotting the lines make it easier to judge the effect priors have on the linear models.

```{r}
model <- m5.5
# model <- m5.5_draft

prior <- extract.prior( model ) 
xseq <- c(-2,2) 
mu <- link( model , post=prior , data=list(N=xseq) ) 
plot( NULL , xlim=xseq , ylim=xseq ) 
for ( i in 1:50 ) lines( xseq , mu[i,] , col=col.alpha("black",0.3) )
```

```{r}
precis(m5.5, digits = 4
       )
```

From this summary, you can possibly see that this is neither a strong nor very precise association. The standard deviation is almost twice the posterior mean. plots are easier to read than tables:

The posterior mean line is weakly positive, but it is highly imprecise. A lot of mildly positive and negative slopes are plausible, given this model and these data.

```{r}
xseq <- seq( from=min(dcc$N)-0.15 , to=max(dcc$N)+0.15 , length.out=30 ) 
mu <- link( m5.5 , data=list(N=xseq) ) 
mu_mean <- apply(mu,2,mean) 
mu_PI <- apply(mu,2,PI) 

plot( K ~ N , data=dcc ) 
lines( xseq , mu_mean , lwd=2 ) 
shade( mu_PI , xseq )
```

Now consider another predictor variable, adult female body mass, mass in the data frame. Let’s use the logarithm of mass, log(mass), as a predictor as well. Why the logarithm of mass instead of the raw mass in kilograms? **It is often true that scaling measurements like body mass are related by magnitudes to other variables.** Taking the log of a measure translates the measure into magnitudes. So by using the logarithm of body mass here, we’re saying that we suspect that the magnitude of a mother’s body mass is related to milk energy, in a linear fashion.

We construct a similar model of m5.5 that had K \~ scaled(Neocortex). This time we set K \~ log(mass)

```{r}
m5.6 <- quap( 
  alist(
    K ~ dnorm( mu , sigma ) , 
    mu <- a + bM*M , 
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 ) ) , data=dcc ) 

precis(m5.6)
```

Med økt kroppsmasse, minker kilokalorier per gram melk

```{r}
xseq <- seq( from=min(dcc$M)-0.15 , to=max(dcc$M)+0.15 , length.out=30 ) 
mu <- link( m5.6 , data=list(M=xseq) ) 
mu_mean <- apply(mu,2,mean) 
mu_PI <- apply(mu,2,PI) 

plot( K ~ M , data=dcc ) 
lines( xseq , mu_mean , lwd=2 ) 
shade( mu_PI , xseq )
```

now we make a model with both neocortex and body mass

```{r}
m5.7 <- quap( alist( K ~ dnorm( mu , sigma ) , mu <- a + bN*N + bM*M , a ~ dnorm( 0 , 0.2 ) , bN ~ dnorm( 0 , 0.5 ) , bM ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) , data=dcc ) 

precis(m5.7)
```

s

```{r}
plot( coeftab( m5.5 , m5.6 , m5.7 ) , pars=c("bM","bN") )
```

What happened here? Why did adding neocortex and body mass to the same model lead to larger estimated effects of both? This is a context in which there are two variables correlated with the outcome, but one is positively correlated with it and the other is negatively correlated with it. In addition, both of the explanatory variables are positively correlated with one another. Try a simple pairs plot to appreciate this correlation: pairs( \~K + M + N , dcc ). As a result, they tend to cancel one another out.

```{r}
pairs( ~K + M + N , dcc )
```

What the regression model does is ask if species that have high neocortex percent for their body mass have higher milk energy. Likewise, the model asks if species with high body mass for their neocortex percent have higher milk energy. Bigger species, like apes, have milk with less energy. But species with more neocortex tend to have richer milk. The fact that these two variables, body size and neocortex, are correlated across species makes it hard to see these relationships, unless we account for both.

![](images/clipboard-1637075509.png)

```{r}
xseq <- seq( from=min(dcc$M)-0.15 , to=max(dcc$M)+0.15 , length.out=30 )

mu <- link( m5.7 , data=data.frame( M=xseq , N=0 ) ) # setting N = 0
mu_mean <- apply(mu,2,mean) 
mu_PI <- apply(mu,2,PI) 

plot( NULL , xlim=range(dcc$M) , ylim=range(dcc$K), xlab = "log body mass (std)" , ylab = "kilocal per g (std)") 
lines( xseq , mu_mean , lwd=2 ) 
shade( mu_PI , xseq )
mtext("Counterfactual holding N = 0")
```

## 5.3 categorical variables

-   Binary categories = just two outcomes of a variable

### 5.3.1 binary variables

![](images/clipboard-2876496439.png)

Using this approach means that beta-m represents the expected difference between males and females in height. The parameter alpha is used to predict both female and male heights. But male height gets an extra m. This also means that alpha is no longer the average height in the sample, but rather just the average female height.

```{r}
data(Howell1)
d <- Howell1


mu_female <- rnorm(1e4,178,20) 
mu_male <- rnorm(1e4,178,20) + rnorm(1e4,0,10) 

precis( data.frame( mu_female , mu_male ), depth = 2)
```

```{r}
hist(mu_female, col=rgb(1,0,0,0.2), xlim=c(50,250))
hist(mu_male, col=rgb(0,0,1,0.2), xlim=c(50,250), add =T )
# title("heights men and women, rnorm", )
```

The prior for males is wider, because it uses both parameters, showing that the lower 5,5% quantile height for men is 141 and 146 for women. It does not make sense that menæs height range would be lower than women. While in a regression this simple, these priors will wash out very quickly, in general we should be careful. We aren’t actually more unsure about male height than female height, a priori. Is there another way?

Instead of using a swith-on/off dummy variable, we start indexing. And give the same sigma to both. Now the same prior can be assigned to each, corresponding to the notion that all the categories are the same, prior to the data. Neither category has more prior uncertainty than the other. And as you’ll see a bit later, this approach extends really nicely to contexts with more than two categories.

```{r}
d$sex <- ifelse( d$male==1 , 2 , 1 ) # male = 2, female =1

m5.8 <- 
  quap( 
    alist( height ~ dnorm( mu , sigma ) , 
           mu <- a[sex] ,
           a[sex] ~ dnorm( 178 , 20 ) , 
           sigma ~ dunif( 0 , 50 ) ) ,
    data=d ) 

precis( m5.8 , depth=2 )
```

```{r}
post <- extract.samples(m5.8, 1e4) 
post$diff_fm <- post$a[,1] - post$a[,2] 
precis( post , depth=2)
```

### 5.3.2 many categories

```{r, warning=F}
library(tidyverse)
data(milk) 
d <- milk 

# we have variable clade that have 4 categories
d %>% 
  count(clade)
```

```{r}
# we make an integer of it, index it
d <- d %>% 
  mutate(clade_id = as.integer(clade))
str(d$clade_id)
```

We want to find AVERAGE milk energy in each group clade

```{r}
d$K1 <- scale( d$kcal.per.g )
d <- d %>% 
  mutate(K2 = scale(kcal.per.g))

```

```{r}
m5.9 <- quap( 
  alist( 
    K ~ dnorm( mu , sigma ),
    mu <- a[clade_id], 
    a[clade_id] ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 ) ) , data=d ) 

labels <- paste( "a[" , 1:4 , "]: " , levels(d$clade) , sep="" ) 
plot( precis( m5.9 , depth=2 , pars="a" ) , labels=labels , xlab="expected kcal (std)" )
```

```{r}
d %>% 
  group_by(clade) %>% 
  # summarise(avg_kcal = mean(kcal.per.g, na.rm = T),
  #           sd_kcal = sd(kcal.per.g)) %>% 
  ggplot(aes(y=clade, x = kcal.per.g)) +
  geom_boxplot()
```

```{r}
# but now scaled
d %>% 
  group_by(clade) %>% 
  # summarise(avg_kcal = mean(kcal.per.g, na.rm = T),
  #           sd_kcal = sd(kcal.per.g)) %>% 
  ggplot(aes(y=clade, x = K)) +
  geom_boxplot()
```

Why are the boxplots so different from the linear model ??

### 

# chapter 6 - DAG and causality

If we plot some random generated dots for a papers trustworthiness and newsworthiness, it will look likea ball of dots. Then if our selection criteria for funding is to be top 10% when ranked for both criteria, the selection will look like a sliced corner. This slice-line has a decline of -0.77. Or model will think that there is negative correlation of -0.77 between trustworthiness and newsworthiness.

This line is made because there is a higher chance for regression to the mean, that not both parameters x and y is in the tail-end of the normal distribution.

This general phenomenon has been recognized for a long time. It is sometimes called Berkson’s paradox. But it is easier to remember if we call it the selection-distortion effect. Once you appreciate this effect, you’ll see it everywhere. Why can’t restaurants have good food and be in good locations? Restaurants serving bad food in bad locations go out of business. The only way a restaurant with less-than-good food can survive is if it is in a nice location. And restaurants with excellent food can survive even in bad locations. Selection distortion ruins your city.

![](images/clipboard-3955399936.png)

The selection-distortion effect can happen inside of a multiple regression, because the act of adding a predictor induces statistical selection within the model, a phenomenon that goes by the unhelpful name collider bias. This can mislead us into believing, for example, that there is a negative association between newsworthiness and trustworthiness in general, when in fact it is just a consequence of conditioning on some variable. This is both a deeply confusing fact and one that is important to understand in order to regress responsibly. This chapter and the next are both about terrible things that can happen when we simply add variables to a regression, without a clear idea of a causal model. In this chapter, we’ll explore three different hazards: **multicollinearity, post-treatment bias, and collider bias.**

## 6.1 Multicollinearity

They simulated some height and leg lengths data, and tried to incorporate each leg as a seperate variables as predictors for height. Lengths of both legs are usually perfectly correlated unless there are special cases or measuring uncertainty (which they did incorporate as well). The problem is that each leg got different effects on height, but both had the same standard deviation. The total impact of both variables did equate in the same impact as if the model had only included one leg, though for this model the sd is much smaller. The point is that there are infinite ways of estimated two parameters that sum two one effect.

The problem is the question. Recall that a multiple linear regression answers the question: What is the value of knowing each predictor, after already knowing all of the other predictors? So in this case, the question becomes: What is the value of knowing each leg’s length, after already knowing the other leg’s length?

Now to the milk dataset.

```{r}
data("milk")
d <- milk
pairs( ~ kcal.per.g + perc.fat + perc.lactose , data=d , col=rangi2 )
```

This is the same statistical phenomenon as in the leg length example. What has happened is that the variables perc.fat and perc.lactose contain much of the same information. They are almost substitutes for one another. As a result, when you include both in a regression, the posterior distribution ends up describing a long ridge of combinations of bF and bL that are equally plausible.

So what to do when there is multicollinearity? In the scientific literature, you might encounter a variety of dodgy ways of coping with multicollinearity. Few of them take a causal perspective. Some fields actually teach students to inspect pairwise correlations before fitting a model, to identify and drop highly correlated predictors. This is a mistake. Pairwise correlations are not the problem. It is the conditional associations—not correlations—that matter.

Extra stuff: simulating collinearity So for each correlation value in r.seq, the code generates 100 regressions and returns the average standard deviation from them. This code uses implicit flat priors, which are bad priors. So it does exaggerate the effect of collinear variables. When you use informative priors, the inflation in standard deviation can be much slower.

```{r}
library(rethinking) 
data(milk) 
d <- milk 
sim.coll <- function( r=0.9 ) { 
  d$x <- rnorm( nrow(d) , 
                mean=r*d$perc.fat , 
                sd=sqrt( (1-r^2)*var(d$perc.fat) ) ) 
  m <- lm( kcal.per.g ~ perc.fat + x , data=d ) 
  sqrt( diag( vcov(m) ) )[2] # stddev of parameter 
  } 

rep.sim.coll <- function( r=0.9 , n=100 ) { 
  stddev <- replicate( n , sim.coll(r) ) 
  mean(stddev) 
  }

r.seq <- seq(from=0,to=0.99,by=0.01) 
stddev <- sapply( r.seq , function(z) rep.sim.coll(r=z,n=100) ) 
plot( stddev ~ r.seq , type="l" , col=rangi2, lwd=2 , xlab="correlation" )
```

## 6.2 post-treatment bias

It is routine to worry about mistaken inferences that arise from omitting predictor variables. Such mistakes are often called omitted variable bias, and the examples from the previous chapter illustrate it. It is much less routine to worry about mistaken inferences arising from including variables that are consequences of other variables. We’ll call this posttreatment bias.

Example with fungus and plants. We simulate a fake dataset, but what we know when making it we are supposed to not know when making a model later.

```{r, warning=F}
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
set.seed(71) 
# number of plants 
N <- 100
# simulate initial heights 
h0 <- rnorm(N,10,2) 

# assign treatments and simulate fungus and growth 
treatment <- rep( 0:1 , each=N/2 ) 
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 ) 
h1 <- h0 + rnorm(N, 5 - 3*fungus) 
# compose a clean data frame 
d <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus ) 
precis(d)
```

The language “post-treatment” comes in fact from thinking about experimental designs. Suppose for example that you are growing some plants in a greenhouse. You want to know the difference in growth under different anti-fungal soil treatments, because fungus on the plants tends to reduce their growth. Plants are initially seeded and sprout. Their heights are measured. Then different soil treatments are applied. Final measures are the height of the plant and the presence of fungus. There are four variables of interest here: initial height, final height, treatment, and presence of fungus. Final height is the outcome of interest. But which of the other variables should be in the model? If your goal is to make a causal inference about the treatment, you shouldn’t include the presence of fungus, because it is a post-treatment effect.

We make a model. THe prior is set a h1 proportion of ho (initial plant height). the p CAN be less than 1, meaning we killed the plant, but p cannot be negative (negative proportation doesnt make sense). To keep it positive, we use a log-normal distribution for the prior.

So this prior expects anything from 40% shrinkage up to 50% growth.

```{r}
# simulate prior
sim_p <- rlnorm( 1e4 , 0 , 0.25 ) 
precis( data.frame(sim_p) )
```

```{r}
hist(sim_p)
```

Our model gives a 40% growth on average...we have not yet included the variables for treatment and fungus...

```{r}

m6.6 <- quap( 
  alist( 
    h1 ~ dnorm( mu , sigma ), 
    mu <- h0*p,
    p ~ dlnorm( 0 , 0.25 ),
    sigma ~ dexp( 1 ) ), data=d ) 
precis(m6.6)
```

The parameters for these variables will also be on the proportion scale. They will be changes in proportion growth.

```{r}
m6.7 <- quap( 
  alist( 
    h1 ~ dnorm( mu , sigma ), 
    mu <- h0 * p, 
    p <- a + bt*treatment + bf*fungus, 
    a ~ dlnorm( 0 , 0.2 ) ,
    bt ~ dnorm( 0 , 0.5 ), 
    bf ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 ) ), data=d ) 

precis(m6.7) %>% 
  View
```

The marginal posterior for bt, the effect of treatment, is solidly zero, with a tight interval. The treatment is not associated with growth. The fungus seems to have hurt growth, however. Given that we know the treatment matters, because we built the simulation that way, what happened here?

The problem is that fungus is mostly a consequence of treatment. This is to say that fungus is a post-treatment variable. So when we control for fungus, the model is implicitly answering the question: **Once we already know whether or not a plant developed fungus, does soil treatment matter?** The answer is “no,” because soil treatment has its effects on growth through reducing fungus. But we actually want to know, based on the design of the experiment, is the impact of treatment on growth. To measure this properly, we should omit the post-treatment variable fungus. Here’s what the inference looks like in that case:

```{r}
m6.8 <- quap( 
  alist( 
    h1 ~ dnorm( mu , sigma ), 
    mu <- h0 * p, 
    p <- a + bt*treatment, 
    a ~ dlnorm( 0 , 0.2 ), 
    bt ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 ) ), data=d ) 
precis(m6.8) %>% 
  View
```

Now the impact of treatment is clearly positive, as it should be. It makes sense to control for pre-treatment differences, like the initial height h0, that might mask the causal influence of treatment. But including post-treatment variables can actually mask the treatment itself.

This doesn’t mean you don’t want the model that includes both treatment and fungus. The fact that including fungus zeros the coefficient for treatment suggests that the treatment works for exactly the anticipated reasons. It tells us about mechanism. But a correct inference about the treatment still depends upon omitting the post-treatment variable.

```{r, warning=F}
library(dagitty) 
plant_dag <- dagitty( "dag { H_0 -> H_1 F -> H_1 T -> F }") 
coordinates( plant_dag ) <- list( x=c(H_0=0,T=2,F=1.5,H_1=1) , y=c(H_0=0,T=0,F=0,H_1=0) ) 
drawdag( plant_dag )
```

So the treatment T influences the presence of fungus F which influences plant height at time 1, H1. Plant height at time 1 is also influenced by plant height at time 0, H0. That’s our DAG. When we include F, the post-treatment effect, in the model, we end up blocking the path from the treatment to the outcome. This is the DAG way of saying that learning the treatment tells us nothing about the outcome, once we know the fungus status.

## d-seperation

An even more DAG way to say this is that conditioning on F induces d-separation. The “d” stands for directional.88 d-separation means that some variables on a directed graph are independent of others. There is no path connecting them. In this case, H1 is d-separated from T, but only when we condition on F. Conditioning on F effectively blocks the directed path T-F-\> h1. Why does this happen? There is no information in T about H1 that is not also in F. So once we know F, learning T provides no additional information about H1.

You can query the implied conditional independencies for this DAG:

```{r}
impliedConditionalIndependencies(plant_dag)
```

There are three. The third one is the focus of our discussion. But the other two implications provide ways to test the DAG. What F \|\| H0 and H0 \|\| T say is that the original plant height, H0, should not be associated with the treatment T or fungus F, provided we do not condition on anything.

For example, conditioning on a post-treatment variable can not only fool you into thinking the treatment doesn’t work. It can also fool you into thinking it does work. Consider the DAG below.

![](images/clipboard-976053816.png)

Here T do not influence H1, neighter do F, but M do. M also influence fungus.

Our knew dataset depending on the DAG real world situation.

```{r}
set.seed(71) 
N <- 1000 
h0 <- rnorm(N,10,2) 
treatment <- rep( 0:1 , each=N/2 ) 
M <- rbern(N) 
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 + 0.4*M ) 
h1 <- h0 + rnorm( N , 5 + 3*M ) 
d2 <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )
```

And how our estimated parameters look like with the wrong models (that do not represetn the DAG).

```{r}
m6.7 <- quap( 
  alist( 
    h1 ~ dnorm( mu , sigma ), 
    mu <- h0 * p, 
    p <- a + bt*treatment + bf*fungus, 
    a ~ dlnorm( 0 , 0.2 ) ,
    bt ~ dnorm( 0 , 0.5 ), 
    bf ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 ) ), data=d2 ) 

precis(m6.7)
```

Our previous model 6.7 with h1 \~ fungus + treatment, show that with the data based on the new DAG (moisture being the influencing variable), that both treatment and fungus now have a POSITIVE effect on growth.

```{r}
m6.8 <- quap( 
  alist( 
    h1 ~ dnorm( mu , sigma ), 
    mu <- h0 * p, 
    p <- a + bt*treatment, 
    a ~ dlnorm( 0 , 0.2 ), 
    bt ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 ) ), data=d2 ) 
precis(m6.8)
```

While the 6.8 model with just T and no F, now show no effect of treatment (critical interval with 0)

## 6.3 Collider bias

The fact that two arrows enter C means that C is a collider. A -\> C \<- B. Thee is no correlation between A and B, but collider bias make it seem like it.

When you condition on a collider, it creates statistical—but not necessarily causalassociations among its causes. In this case, once you learn that a proposal has been selected (S), then learning its trustworthiness (T) also provides information about its newsworthiness (N). Why? Because if, for example, a selected proposal has low trustworthiness, then it must have high newsworthiness. Otherwise it wouldn’t have been funded. **This is the informational phenomenon that generates the negative association between T and N in the population of selected proposals.**

Winnie: in my mind, it seems to be the case of sampling bias that skews the sample and therefor makes it seem like there is a relationship between T and N, when this is clearly the sampling rule and not a rule in the population of papers.

An example of collider bias is when the collider is used as condition (controlled for/included) in a model when predicted another variable (which is not how the causation is supposed to be according to the DAG). The book mentions Age -\> Marriage \<- Happiness to be the "real" world, for example. Marriage is the collider. Lets say we are happiness-scientists, and makes a model that tests, takes into, all variables just to check for influencing effects. A model with happiness \~ marriage + age, will suddenly show that age has a negative relation with happiness = spurious association...(though, in this example the sd is 0.11 \< 0.05, i would call this non-significant, but the author is so against that...). The other model: happiness \~ age, show no effect of age on happiness, which is the right result and confounds with the DAG. SO, something happened when including the collider, and as scientists we wouldnt know which variable is the collider. It works well for predictio maybe, but it is not causally REAL if that is importatnt....SO in this case, less is more. But later we will see that more is more (education - grandparent case)

Verdict: If you don’t have a causal model, you can’t make inferences from a multiple regression. And the regression itself does not provide the evidence you need to justify a causal model. Instead, you need some science.

This time, adding all the variables saved us from allocating effects to variables wrongfully ![](images/clipboard-3621735263.png)

How the model without U (neighbourhood) estimate a negative correlation between children's education and grandparents' education. This only happens because the model doesnt take into account U and allocate the U (neighbourhood) effect to the grandparents.

![](images/clipboard-1461027995.png)

So consider two different parents with the same education level, say for example at the median 50th centile. One of these parents has a highly educated grandparent. The other has a poorly educated grandparent. The only probable way, in this example, for these parents to have the same education is if they live in different types of neighborhoods. We can’t see these neighborhood effects—we haven’t measured them, recall—but the influence of neighborhood is still transmitted to the children C.

![](images/clipboard-3193699745.png)

The unmeasured U makes P a collider, and conditioning on P produces collider bias. So what can we do about this? You have to measure U.

```{r}
# producing dataset
N <- 200 # number of grandparent-parent-child triads 
b_GP <- 1 # direct effect of G on P 
b_GC <- 0 # direct effect of G on C 
b_PC <- 1 # direct effect of P on C 
b_U <- 2 # direct effect of U on P and C


set.seed(1) 
U <- 2*rbern( N , 0.5 ) - 1 
G <- rnorm( N ) 
P <- rnorm( N , b_GP*G + b_U*U ) 
C <- rnorm( N , b_PC*P + b_GC*G + b_U*U ) 
d <- data.frame( C=C , P=P , G=G , U=U )


# the model, now with U
m6.12 <- quap( 
  alist( C ~ dnorm( mu , sigma ), 
         mu <- a + b_PC*P + b_GC*G + b_U*U, 
         a ~ dnorm( 0 , 1 ), 
         c(b_PC,b_GC,b_U) ~ dnorm( 0 , 1 ), 
         sigma ~ dexp( 1 ) ), data=d ) 
precis(m6.12)
```

## 6.4 Confronting confounding

Let’s define confounding as any context in which the association between an outcome Y and a predictor of interest X is not the same as it would be, if we had experimentally determined the values of X.

E = education, W = wages, U = unobserved variables

![](images/clipboard-1201847498.png)

Closing backdoors = controlling for other influencing variables = including the in the model

We can use the dagitty to help us find the variables we need to control for

![\^first path (1)](images/clipboard-3134465237.png)

```{r}
library(dagitty) 
dag_6.1 <- dagitty( "dag { 
                    U [unobserved]  # set U as unobserved so it will not recommmend U as well
                    X -> Y 
                    X <- U <- A -> C -> Y 
                    U -> B <- C                                       # collider
                    }") 
adjustmentSets( dag_6.1 , exposure="X" , outcome="Y" )
```

Conditioning on either C or A would suffice. Conditioning on C is the better idea, from the perspective of efficiency, since it could also help with the precision of the estimate of X -\> Y.

Now consider the second path (2), passing through B. This path does contain a collider, U -\>B \<-C. It is therefore already closed. It is not a backdoor, and that is why adjustmentSets above did not mention B. But if we do condition on B, it is not harmless. It will open the path, creating a confound. Then our inference about X -\> Y will change, but without the DAG, we won’t know whether that change is helping us or rather misleading us.

ABOUT DAG: because sometimes you really can test whether your DAG is consistent with the evidence. The data alone can never tell us when a DAG is right. But the data can tell us when a DAG is wrong.

New example. Divorces \~ waffles+++. We’re interested in the total causal effect of the number of Waffle Houses on divorce rate in each State. There are three open backdoor paths between W and D. Just trace backwards, starting at W and ending up at D. But notice that all of them pass first through S. So we can close them all by conditioning on S. That’s all there is to it.

```{r}
library(dagitty) 
dag_6.2 <- dagitty( "dag { 
                    A -> D            # open
                    A -> M -> D     # open
                    A <- S -> M     # open? fork
                    S -> W -> D }") # open

adjustmentSets( dag_6.2 , exposure="W" , outcome="D" )
```

In this case, there are three implied conditional independencies:

```{r}
impliedConditionalIndependencies( dag_6.2 )
```

Read the first as “median age of marriage should be independent of (*\|\|*) Waffle Houses, conditioning on (\|) a State being in the south.” In the second, divorce and being in the south should be independent when we simultaneously condition on all of median age of marriage, marriage rate, and Waffle Houses. Finally, marriage rate and Waffle Houses should be independent, conditioning on being in the south.

# 7 ulysses compass

## Guido 04.04.2024

The idea of shrinkage is to "calibrate" the extreme effects (betas) that we find, hmm also so known as regularization??. It is a chance that there is alot of noise the model is trying to accomodate, we do not want that.

The four faceted GIF show that the wider priors make room for very flexible models (when testing 8 different models) with the same prior. Four different priors are used, what is shown that a stricter prior makes the model bend LESS to the dataset.

This overfitting can be catched by tested the model on test data. Good model fit with training data and poor fit to test data =\> bad model.

Divergence = using the divergence between models to pick the best one among them. And therefore do not need pick the RIGHT model, the "TRUE" model.

log pointwise preditice density = lppd = difficult to understand!! lppd = all people so we sum. the log( p(y\|theta)\*1/S) = is the average probability of observing (or predicting??) this one individual point given the parameters in our model (likelihood), the average is averaged on the number of samples, like 500 for example.

The dream of testing models: CROSS VALIDATION. cross validation testing (train \| test). When you split the dataset and test the model you get, but lets say we can make many models by splitting the data several times (just repeat the split and calibrate the model repeatedly).

something about taking the log-likelihood on the dependent level, example students in schools. the students within one school is usually dependent on each other, while between schools there might be none. So, guido said take the log-L on student-level.

....womp womp.

But, often we cannot split data data, we dont have enough data. So, the next best is using information criteria: WAIC, PSIS, AIC (great for linear only, but should not be used for hierarchical model, because a better model with more parameters is penalized just based on its number of parameters.) +++.... on hieracrhical = multilevel: <https://rpubs.com/rslbliss/r_mlm_ws>.

Example of multilevel model from the link:

```         
model <- lmer(math ~ homework + (1 | schid), data=mlmdata)
```

PSIS-LOO. the weights of the data points are higher for test data points + surprising data points. surprising data points have a lower likelihood, p(y\|theta). weight = w = 1/p(y\|theta).

comments on AIC, BIC. AIC penalize based on number of parameters. BIC penalize complex models when the dataset is small. therefore one would think BIC \> AIC. AIC is simpler. AIC \< BIC \< DIC \< WAIC.

compare(m1, m2, m3, ....) give a table with WAIC. A guy said that the best model is only clearly better by having a dWAIC difference of 5 standard deviations. Guido thinks 2 is okey. pWAIC = approximate number of effective parameters in the model.

robust regression = means instead of assuming gaussian error distribution, we use dstudent(), the tails are just thicker.

## 7.4 predicting predictive accuracy

two methods: cross-validation (pareto smoothed importance sampling, PSIS), and information criteria (AIC)

### 7.4.2 information criteria: WAIC, AIC, DIC

![](images/clipboard-3399357724.png)

AIC is of mainly historical interest now. Newer and more general approximations exist that dominate AIC is every context. But Akaike deserves tremendous credit for the initial inspiration. See the box further down for more details. AIC is an approximation that is reliable only when:

(1) The priors are flat or overwhelmed by the likelihood.
(2) The posterior distribution is approximately multivariate Gaussian.
(3) The sample size N is much greater than the number of parameters k.

Since flat priors are hardly ever the best priors, we’ll want something more general. And when you get to multilevel models, the priors are never flat by definition. There is a slightly more general criterion called the Deviance Information Criterion (DIC). DIC accommodates informative priors, but still assumes that the posterior is multivariate Gaussian and that N ≫ k.

WAIC = WAIC is just the log-posterior-predictivedensity (lppd, page 214) that we calculated earlier plus a penalty proportional to the variance

We’ll focus on a criterion that is more general than both AIC and DIC, WAIC. The Widely Applicable Information Criterion (WAIC) makes no assumption about the shape of the posterior.

It provides an approximation of the out-of-sample deviance that converges to the leave-one-out cross-validation approximation in a large sample. But in a finite sample, it can disagree. It can disagree because it has a different target—it isn’t trying to approximate the cross-validation score, but rather guess the out-of-sample K-L Divergence.

![](images/clipboard-3383304046.png)

I’ll try to call the penalty term “the overfitting penalty.” Because of the analogy to Akaike’s original criterion, the penalty term in WAIC is sometimes called the effective number of parameters, labeled pwaic.

## 7.5 Model comparison

### 7.5.1 Model mis-selection

We must keep in mind the lessons of the previous chapters: Inferring cause and making predictions are different tasks. CV, PSIS, and WAIC aim to find models that make good predictions. They don’t solve any causal inference problem.

But recall that our working definition of knowing a cause is that we can **predict the consequences of an intervention.** So a good PSIS or WAIC score does not in general indicate a good causal model.

For example, recall the plant growth example from the previous chapter (6.2 post-treatment bias). The model that conditions on fungus will make better predictions than the model that omits it. If you return to that section and run models m6.6, m6.7, and m6.8 again, we can compare their WAIC values. To remind you, m6.6 is the model with just an intercept, m6.7 is the model that include both treatment and fungus (the post-treatment variable), and m6.8 is the model that includes treatment but omits fungus. It’s m6.8 that allows us to correctly infer the causal influence of treatment.

```{r}
set.seed(11) 
WAIC( m6.7 )
```

```{r}
set.seed(77) 
compare( m6.6 , m6.7 , m6.8 )
```

Model m6.7 is a lot better.

The filled points are the in-sample deviance values. The open points are the WAIC values.

Notice that naturally each model does better in-sample than it is expected to do out-ofsample. The line segments show the standard error of each WAIC. These are the values in the column labeled SE in the table above. So you can probably see how much better m6.7 is than m6.8. What we really want however is the standard error of the difference in WAIC between the two models. That is shown by the lighter line segment with the triangle on it, between m6.7 and m6.8.

```{r}
plot( compare( m6.6 , m6.7 , m6.8 ) )
```

(PSIS will give you the same values. You can add func=PSIS to the compare call to check.)

What do all of these numbers mean?

The first column contains the WAIC values. **Smaller values are better, and the models are ordered by WAIC, from best to worst.** The model that includes the fungus variable has the smallest WAIC, as promised.

The second column, pWAIC, is the penalty term of WAIC. These values are close to, but slightly below, the number of dimensions in the posterior of each model, which is to be expected in linear regressions with regularizing priors.

The next column, dWAIC, is the difference between each model’s WAIC and the best WAIC in the set. So it’s zero for the best model and then the differences with the other models tell you how far apart each is from the top model. So m6.7 is about 40 units of deviance smaller than both other models. The intercept model, m6.6, is 3 units worse than m6.8. Are these big differences or small differences? \<- need to check the standard error.

SE: Since we don’t have the target sample, these are just guesses, and we know from the simulations that there is a lot of variation in WAIC’s error. In a very approximate sense, we expect out-of-sample accuracy to be normally distributed with mean equal to the reported WAIC value and a standard deviation equal to the standard error. This approximation will be quite bad, when the sample is small. **But it is still better than older criteria like AIC, which provide no way to gauge their err**or.

dSE. Now to judge whether two models are easy to distinguish, we don’t use their standard errors but rather the standard error of their difference. What does that mean? Just like each WAIC value, each difference in WAIC values also has a standard error.

```{r}
# not sure what this code wants to show...
set.seed(11) 
waic_m6.6 <- WAIC( m6.6 , pointwise=TRUE )   #intercept only model
waic_m6.7 <- WAIC( m6.7 , pointwise=TRUE )   #fungus+treatment
waic_m6.8 <- WAIC( m6.8 , pointwise=TRUE )   # treatment only
n <- length(waic_m6.6) 

# waic_m6.7 - waic_m6.8
```

Similarly, we can ask about the difference between models m6.8, the model with treatment only, and model m6.6, the intercept model. Model m6.8 provides pretty good evidence that the treatment works. You can inspect the posterior again, if you have forgotten. But WAIC thinks these two models are quite similar. There difference is only 3 units of deviance. Let’s calculate the standard error of the difference, to highlight the issue:

```{r}
set.seed(92) 
waic_m6.6 <- WAIC( m6.6 , pointwise=TRUE ) 
diff_m6.6_m6.8 <- waic_m6.6 - waic_m6.8 
sqrt( n*var( diff_m6.6_m6.8 ) )
```

**hææææ, doesnt get the same results as in the book...**

![](images/clipboard-116172224.png)

**Does this mean that the treatment doesn’t work? Of course not. We know that it works. We simulated the data. And the posterior distribution of the treatment effect, bt in m6.8, is reliably positive. But it isn’t especially large. So it doesn’t do much alone to improve prediction of plant height**. There are just too many other sources of variation.

We can use WAIC/CV/PSIS to measure how big a difference some variable makes in prediction. But we cannot use these criteria to decide whether or not some effect exists. We need the posterior distributions of multiple models, maybe examining the implied conditional independencies of a relevant causal graph, to do that.

### 7.5.2 Outliers' effect on the model prediction ability

It is mentioned that PSIS also give warnings for high Pareto k values (above 1) as a sign that some observations have high influence on the models dis-fit for testing data (outliers obervations). WAIC do take this into account throught the overfitting penalty, but the function does not give a warning.

```{r}
# using the example of the waffledivorce data.
# m5.1 = age, m5.2 = M, m5.3 = age + m
compare(m5.1, m5.2, m5.3, func = PSIS)
```

# 8 Conditional manatees

The evidence from surviving manatees and bombers is misleading, because it is conditional on survival. Manatees and bombers that perished look different. A manatee struck by a keel is less likely to live than another grazed by a propeller. So among the survivors, propeller scars are common. Similarly, bombers that returned home conspicuously lacked damage to the cockpit and engines. They got lucky.

## 8.1 Building an interaction

(Africa is also geographically special, in a puzzling way: Bad geography tends to be related to bad economies outside of Africa, but African economies may actually benefit from bad geography.)

To appreciate the puzzle, look at regressions of terrain ruggedness—a particular kind of bad geography—against economic performance (log GDP133 per capita in the year 2000), both inside and outside of Africa (Figure 8.2). **The variable rugged is a Terrain Ruggedness Index that quantifies the topographic heterogeneity of a landscape.** The outcome variable here is the logarithm of real gross domestic product per capita, from the year 2000, rgdppc_2000. We use the logarithm of it, because the logarithm of GDP is the magnitude of GDP. **Since wealth generates wealth, it tends to be exponentially related to anything that increases it.** This is like saying that the absolute distances in wealth grow increasingly large, as nations become wealthier. So when we work with logarithms instead, we can work on a more evenly spaced scale of magnitudes. **Regardless, keep in mind that a log transform loses no information.** It just changes what we the model assumes about the shape of the association between variables. In this case, raw GDP is not linearly associated with anything, because of its exponential pattern. But log GDP is linearly associated with lots of things.

### 8.1.1 Doing it wrong. splitting data by category, making two models

![](images/clipboard-937574487.png)

What is going on in this figure? **It makes sense that ruggedness is associated with poorer countries, in most of the world. Rugged terrain means transport is difficult.** Which means market access is hampered. Which means reduced gross domestic product. So the reversed relationship within Africa is puzzling. Why should difficult terrain be associated with higher GDP per capita?

Possible explanation: If this relationship is at all causal, it may be because rugged regions of Africa were protected against the Atlantic and Indian Ocean slave trades. Slavers preferred to raid easily accessed settlements, with easy routes to the sea. Those regions that suffered under the slave trade understandably continue to suffer economically, long after the decline of slave-trading markets. However, an outcome like GDP has many influences, and is furthermore a strange measure of economic activity. So it is hard to be sure what’s going on here.

The plots divided/split the dataset in two, one for africa and one for non-africa. That is cheating in regression modelling, due to 4 reasons:

1.  The assumption that sigma is different for africa and non-african countries. By splitting the data table, you are hurting the accuracy of the estimates for these parameters, because you are essentially making two less-accurate estimates instead of pooling all of the evidence into one estimate.

2.  ???

3.  the possibility to compare two different models based on the same dataset. For example one including the cont_africa (1/0 africa) and the other not accounting for it at all.

4.  ??? something about the different sample sized in the categories of a variable that may lead to overfitting...and a total dataset will account for this\`??? idk

But first, about this example:

Rethinking: Practicing for when it matters. The exercise in Figure 8.3 is really not necessary in this example, because there is enough data, and the model is simple enough, that even awful priors get washed out. You could even use completely flat priors (don’t!), and it would all be fine. But we practice doing things right not because it always matters. Rather, we practice doing things right so that we are ready when it matters.

Chances are that raw magnitudes of GDP and terrain ruggedness aren’t meaningful to you. So I’ve scaled the variables to make the units more useful. The usual standardization is to subtract the mean and divide by the standard deviation. This makes a variable into z-scores. We don’t want to do that here, because zero is meaningful. So instead terrain ruggedness is divided by the maximum value observed.

```{r}
# library(rethinking) 

# cont_africa==1 || Africa

data(rugged) 
d <- rugged %>% 
  filter(!is.na(rgdppc_2000)) %>% 
  mutate(log_gdp = log(rgdppc_2000 ),
         
         # percentage lower or higher than mean
         log_gdp_std = log_gdp / mean(log_gdp), # mean. low p < mean 1 < high p
         rugged_std = rugged / max(rugged),  # max. 0 to 1
         
         # -1 to 1 with mean at 0.
          log_gdp_std1 = standardize(log_gdp), 
         rugged_std1 = standardize(rugged),
         cid = ifelse( cont_africa==1 , 1 , 2 )
         )

d_africa <- d %>% 
  filter(cont_africa == 1)

d_non_africa  <- d %>% 
  filter(cont_africa == 0)
```

The model:

![](images/clipboard-3183881765.png)

Specifying priors... The hard thinking here comes when we specify priors. If you are like me, you don’t have much scientific information about plausible associations between log GDP and terrain ruggedness. But even when we don’t know much about the context, the measurements themselves constrain the priors in useful ways. The scaled outcome and predictor will make this easier.

Alpha. Consider first the intercept, alpha, defined as the log GDP when ruggedness is at the sample mean. So it must be close to 1, because we scaled the outcome so that the mean is Let’s start with a guess at: alpha \~ Normal(1, 1)

```{r}
mean(d$rugged)
mean(d$rugged_std)
```

Beta. Now for beta, the slope. If we center it on zero, that indicates no bias for positive or negative, which makes sense. But what about the standard deviation? Let’s start with a guess at 1: beta \~ Normal(0, 1)

Sigma. Let’s assign something very broad, sigma \~ Exponential(1). In the problems at the end of the chapter, I’ll ask you to confront this prior as well. But we’ll ignore it for the rest of this example.

In the example they make model 8.1 based on africa-only splitted data:

```{r}
m8.1 <- quap( 
  alist( 
    log_gdp_std ~ dnorm( mu , sigma ) , 
    mu <- a + b*( rugged_std - 0.215 ) , 
    a ~ dnorm( 1 , 1 ) , 
    b ~ dnorm( 0 , 1 ) , 
    sigma ~ dexp( 1 ) ) , 
  data=d_africa )
```

So, we know our data sample and can "draw" priors from the distribution.

![](images/clipboard-1482992377.png)

```{r}
set.seed(7) 
prior <- extract.prior( m8.1 ) 
# set up the plot dimensions 
plot( NULL , xlim=c(0,1) , ylim=c(0.5,1.5) , 
      xlab="ruggedness", 
      ylab="log GDP",
      main = "a ~ dnorm(1,1), b ~ dnorm(0,1)")

abline( h=min(d$log_gdp_std) , lty=2 ) 
abline( h=max(d$log_gdp_std) , lty=2 ) 
# draw 50 lines from the prior 

rugged_seq <- seq( from=-0.1 , to=1.1 , length.out=30 ) 
mu <- link( m8.1 , post=prior , data=data.frame(rugged_std=rugged_seq) ) 

for ( i in 1:50 ) lines( rugged_seq , mu[i,] , col=col.alpha("black",0.3) )
```

The regression lines trend both positive and negative, as they should, but many of these lines are in impossible territory. Considering only the measurement scales, the lines have to pass closer to the point where ruggedness is average (0.215 on the horizontal axis) and proportional log GDP is 1. Instead there are lots of lines that expect average GDP outside observed ranges. So we need a tighter standard deviation on the alpha prior.

Something like alpha \~ Normal(0, 0.1) will put most of the plausibility within the observed GDP values. Remember: 95% of the Gaussian mass is within 2 standard deviations. So a Normal(0; 0:1) prior assigns 95% of the plausibility between 0.8 and 1.2 (1-2\*0.1, 1+2\*0.1). That is still very vague, but at least it isn’t ridiculous.

Beta. At the same time, the slopes are too variable. It is not plausible that terrain ruggedness explains most of the observed variation in log GDP. **An implausibly strong association would be, for example, a line that goes from minimum ruggedness and extreme GDP on one end to maximum ruggedness and the opposite extreme of GDP on the other end.** I’ve highlighted such a line in blue. Our min-max log(GDP) show a slope room that should not be steeper than (1.3-0.7)/1 = 0.6. So, under the  beta\~ Normal(0, 1) prior, more than half of all slopes will have absolute value greater than 0.6.

We therefore set beta \~ N(0, 0.3) such that it makes the slope 0.6 at 2\*sd (95% confidence/gaussian mass)

```{r}
# hmm well, this was supposed to give 0.545...
sum( abs(prior$b) > 0.6 ) / length(prior$bR)
```

Our new model with more restricted priors:

```{r}
m8.1 <- quap( 
  alist( 
    log_gdp_std ~ dnorm( mu , sigma ) , 
    mu <- a + b*( rugged_std - 0.215 ) , 
    a ~ dnorm( 1 , 0.1) , 
    b ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 ) ) , 
  data=d_africa )


set.seed(7) 
prior <- extract.prior( m8.1 ) 
# set up the plot dimensions 
plot( NULL , xlim=c(0,1) , ylim=c(0.5,1.5) , 
      xlab="ruggedness", 
      ylab="log GDP",
      main = "New: a ~ dnorm(1,0.1), b ~ dnorm(0,0.3)")

abline( h=min(d$log_gdp_std) , lty=2 ) 
abline( h=max(d$log_gdp_std) , lty=2 ) 
# draw 50 lines from the prior 

rugged_seq <- seq( from=-0.1 , to=1.1 , length.out=30 ) 
mu <- link( m8.1 , post=prior , data=data.frame(rugged_std=rugged_seq) ) 

for ( i in 1:50 ) lines( rugged_seq , mu[i,] , col=col.alpha("black",0.3) )
```

```{r}
# now for non-africa model with more restricted priors
m8.2 <- quap( 
  alist( 
    log_gdp_std ~ dnorm( mu , sigma ) , 
    mu <- a + b*( rugged_std - 0.215 ) , 
    a ~ dnorm( 1 , 0.1) , 
    b ~ dnorm( 0 , 0.25 ) , 
    sigma ~ dexp( 1 ) ) , 
  data=d_non_africa )
```

```{r}
# rugged_seq from earlier.
# plotting our model posteriors...
mu <- link( m8.1 , data=list(rugged_std=rugged_seq) ) 
mu_mean <- apply(mu,2,mean) 
mu_PI <- apply(mu,2,PI) 

plot( log_gdp_std ~ rugged_std , data=d_africa ) 
lines( rugged_seq , mu_mean , lwd=2 ) 
shade( mu_PI , rugged_seq )
```

### 8.1.2 No splitting. Adding an INDEPENDENT indicator variable does not work

The first thing to realize is that just including an indicator variable for African nations, cont_africa here, won’t reveal the reversed slope.

We are going to compare two models: Note that model comparison here is not about selecting a model. Scientific considerations already select the relevant model. Instead it is about measuring the impact of model differences while accounting for overfitting risk.

\^not sure i fully get it, but these sentences seemed important.

Model 1 (8.3): log_gdp \~ rugged

Model 2 (8.4): log_gdp \~ rugged + cont_africa (different alpha, but not different beta)

```{r}
m8.3 <- quap( 
  alist( 
    log_gdp_std ~ dnorm( mu , sigma ) , 
    mu <- a + b*( rugged_std - 0.215 ) , 
    a ~ dnorm( 1 , 0.1) , 
    b ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 ) ) , 
  data=d )
```

![](images/clipboard-3148096026.png)

Hmm, i do not fully understand how the added term represent more uncertainty, unless it changes sigma. Because right now this is linear model for mu.

Well, their solution is as showed earlier with categorical variables: estimating different alphas (intercepts) for each category.

![](images/clipboard-166376676.png)

```{r}
d$cid <- ifelse( d$cont_africa==1 , 1 , 2 )

m8.4 <- quap( 
  alist( 
    log_gdp_std ~ dnorm( mu , sigma ) , 
    mu <- a[cid] + b*( rugged_std - 0.215 ) , 
    a[cid] ~ dnorm( 1 , 0.1 ) , 
    b ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 ) ) , 
  data=d )
```

So, now the two categories have different alphas but the same betas. This mean that they will plot as two parallel lines.

```{r}
rugged.seq <- seq( from=-0.1 , to=1.1 , length.out=30 ) 
# compute mu over samples, fixing cid=2 
mu.NotAfrica <- link( m8.4 , data=data.frame( cid=2 , rugged_std=rugged.seq ) )

# compute mu over samples, fixing cid=1 
mu.Africa <- link( m8.4 , data=data.frame( cid=1 , rugged_std=rugged.seq ) ) 

# summarize to means and intervals 
mu.NotAfrica_mu <- apply( mu.NotAfrica , 2 , mean ) 
mu.NotAfrica_ci <- apply( mu.NotAfrica , 2 , PI , prob=0.97 ) 

mu.Africa_mu <- apply( mu.Africa , 2 , mean ) 
mu.Africa_ci <- apply( mu.Africa , 2 , PI , prob=0.97 )

plot( log_gdp_std ~ rugged_std , data=d_non_africa ) 
lines( rugged_seq , mu.NotAfrica_mu , lwd=2 ) 
shade( mu.NotAfrica_ci , rugged_seq )
```

...and now we plot. We started this subchapter knowing that the plots of the splitted dataset show that teach category have different slopes. Africa having positive slope and non-africa having negative. We want to show this in our model using the whole dataset.

```{r}
# scatter plot by  color for dummy variable 0/1 går ikke, men id 1/2 går.
plot( log_gdp_std ~ rugged_std , data=d, col = cid) 
lines( rugged_seq , mu.NotAfrica_mu , lwd=2 ) 
shade( mu.NotAfrica_ci , rugged_seq )

lines( rugged_seq , mu.Africa_mu , lwd=2 , col = "blue") 
shade( mu.Africa_ci , rugged_seq, col =  alpha("lightblue", 0.4))
title(main = "model 8.4, different alpha, same beta")
```

```{r}
rugged.seq <- seq( from=-0.1 , to=1.1 , length.out=30 ) 
m8.5 <- quap( 
  alist( 
    log_gdp_std ~ dnorm( mu , sigma ) , 
    mu <- a[cid] + b[cid]*( rugged_std - 0.215 ) , 
    a[cid] ~ dnorm( 1 , 0.1 ) , 
    b[cid] ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 ) ), 
  data=d )

mu.NotAfrica <- link( m8.5 , data=data.frame( cid=2 , rugged_std=rugged.seq ) )

# compute mu over samples, fixing cid=1 
mu.Africa <- link( m8.5 , data=data.frame( cid=1 , rugged_std=rugged.seq ) ) 

# summarize to means and intervals 
mu.NotAfrica_mu <- apply( mu.NotAfrica , 2 , mean ) 
mu.NotAfrica_ci <- apply( mu.NotAfrica , 2 , PI , prob=0.97 ) 

mu.Africa_mu <- apply( mu.Africa , 2 , mean ) 
mu.Africa_ci <- apply( mu.Africa , 2 , PI , prob=0.97 )

plot( log_gdp_std ~ rugged_std , data=d , col = cid) 
lines( rugged_seq , mu.NotAfrica_mu , lwd=2 ) 
shade( mu.NotAfrica_ci , rugged_seq )

lines( rugged_seq , mu.Africa_mu , lwd=2 , col = "blue") 
shade( mu.Africa_ci , rugged_seq, col =  alpha("lightblue", 0.4))
title(main = "model 8.4, different alpha, same beta")
```

```{r}
compare( m8.3 , m8.4 , m8.5 )
```

The first column contains the WAIC values. Smaller values are better, and the models are ordered by WAIC, from best to worst. so; 8.5 \> 8.4 \> 8.3.

M**odel family m8.5 has about 98% of the estimated model weight**. That’s very strong support for including the interaction effect, if prediction is our goal. That probably isn’t surprising, given the obvious difference in slope we began this story with. **But the modicum of weight given to m8.4 suggests that the posterior means for the slopes in m8.5 are a little overfit**. And the standard error of the difference in WAIC between the top two models is almost the same as the difference itself. There are only so many African countries, after all.

It’s useful in these cases to look at WAIC (or LOO) point by point. This helps us see which nations (points) are sensitive and accounting for posterior variance. Recall from last chapter: The simplest approximation of how sensitive a model is to the sample and therefore the overfitting risk is just the variance in log-probability. The “effective number of parameters” is then the variance in log-probability, summed over all cases in the sample. But we can look at that number for each case, because each case gets its own “effective number of parameters.” To get WAIC (or LOO) in pointwise fashion, you can just add pointwise=TRUE:

```{r}
waic_list <- WAIC( m8.5 , pointwise=TRUE )
str(waic_list)
```

The result is 170 individual WAIC values for 170 nations.

So do information criteria make sense at all here? I think they do. No matter what you want to do with the estimates produced by a model, the estimates will be overfit. They will be overfit, because not every feature of the sample is caused by the process of interest. So even if our interest is explanation, rather than prediction, and there will be no new African nations to make a prediction for, overfitting still matters. Whether we use regularizing priors or information criteria or something else, it’s always worth worrying about overfitting and measuring it, if possible. This is perhaps the main use of criteria like WAIC and cross-validation scores alike.

```{r}
lm1 <- lm(log_gdp_std ~ rugged_std + cont_africa, data = d)
summary(lm1)
# lm1$coefficients
```

```{r}
lm2 <- lm(log_gdp_std ~ rugged_std * cont_africa, data = d)
summary(lm2)

# lm2$coefficients
```

```{r}

rugged_seq <- seq( from=-0.1 , to=1.1 , length.out=30 )

y_nonafrica_lm1 = lm1$coefficients[1] + (lm1$coefficients[2])*rugged_seq
y_africa_lm1 = lm1$coefficients[1] + + lm1$coefficients[3]+ lm1$coefficients[2] *rugged_seq

y_nonafrica_lm2 = lm2$coefficients[1] + (lm2$coefficients[2])*rugged_seq
y_africa_lm2 = lm2$coefficients[1] +  lm2$coefficients[3]+ (lm2$coefficients[2]  + lm2$coefficients[4])*rugged_seq

# get shade for lm2
# mu.nonAfrica <- link( lm2 , data=data.frame( cid=1 , rugged_std=rugged.seq ) ) 
# mu.Africa <- link( lm2 , data=data.frame( cid=2 , rugged_std=rugged.seq ) ) 

# mu.nonAfrica_mu <- apply( mu.nonAfrica , 2 , mean ) 
# mu.nonAfrica_ci <- apply( mu.nonAfrica , 2 , PI , prob=0.97 ) 
# 
# mu.Africa_mu <- apply( mu.Africa , 2 , mean ) 
# mu.Africa_ci <- apply( mu.Africa , 2 , PI , prob=0.97 ) 

plot( log_gdp_std ~ rugged_std , data=d , col = cid) 
lines( rugged_seq , y_nonafrica_lm1 , lwd=2, col = "black") 
lines( rugged_seq , y_africa_lm1 , lwd=2, col = "gray") 


lines( rugged_seq , y_nonafrica_lm2 , lwd=2 , col = "blue", type = "b") 
lines( rugged_seq , y_africa_lm2 , lwd=2 , col = "lightblue", type = "b") 

shade( mu_ci_lm2_nonafrica , rugged_seq , col =  alpha("blue", 0.3))
shade( mu_ci_lm2_africa , rugged_seq, col =  alpha("lightblue", 0.3))


legend(0.4, 1.2, legend=c("gdp~ rugged+cont_africa (diff alpha, same beta", "gdp~rugged*cont_africa (diff alpha and beta)"),
       col=c("black", "blue"), lty=1:2, cex=0.8)

title(main = "two lm. , different alpha, same beta. Dark color = non africa")
```

Modell lm2 er bedre enn lm1 ofc, fordi kategoriene har ikke lik beta. Linjene ligner linjene fra bayes-metoden, men mangler sd rundt prediksjonene.

```{r}
# finne shade for lm2 africa

#lm1
#                    coef         sd
# (Intercept)  1.06017358 0.01486673
# rugged_std  -0.04869203 0.04665699
# cont_africa -0.17245196 0.01934990

#lm2
#                              coef         sd
# (Intercept)             1.0829047 0.01639669
# rugged_std             -0.1477166 0.05635684
# cont_africa            -0.2287215 0.02668247
# rugged_std:cont_africa  0.2864617 0.09585361




#names = names(lm2$coefficients)
all_models <- cbind(names = c("int", "rugged", "cont_africa", "rugged_cont"), 
      coef = lm2$coefficients, 
      sd = sqrt(diag(vcov(lm2)))) %>%
  as.tibble() %>% 
  mutate(coef = as.numeric(coef),
         sd=as.numeric(sd),
    min = coef-sd,
         max = coef+sd) %>% 
  select(-c(coef,sd)) %>%
   pivot_longer(!names, names_to = "min_max", values_to = "values") %>%
  # select(-c(min_max)) %>%
  pivot_wider(names_from = names, values_from = values) %>%
    select(-c(min_max)) %>%
   expand(int, rugged, cont_africa, rugged_cont) # instead om expland.grid. dplyr uses crossing() and expand()

mu_ci_lm2_nonafrica <- crossing(all_models, rugged_seq, .name_repair = "check_unique") %>%
  mutate(y = int + (rugged )*rugged_seq) %>%
  group_by(rugged_seq) %>%
  summarize(min_y = min(y), max_y = max(y)) %>%
  select(-rugged_seq) %>% 
  (t) 

mu_ci_lm2_africa <- crossing(all_models, rugged_seq, .name_repair = "check_unique") %>%
  mutate(y = int + cont_africa + (rugged+ rugged_cont)*rugged_seq) %>%
  group_by(rugged_seq) %>%
  summarize(min_y = min(y), max_y = max(y)) %>%
  select(-rugged_seq) %>% 
  (t) 

```

### 8.1.4 Plotting the interaction

Plotting this model doesn’t really require any new tricks. The goal is to make two plots. In the first, we’ll display nations in Africa and overlay the posterior mean (MAP) regression line and the 97% interval of that line. In the second, we’ll display nations outside of Africa instead.

```{r}
# plot Africa - cid=1 
plot( d_africa$rugged_std , d_africa$log_gdp_std , pch=16 , col=rangi2 , xlab="ruggedness (standardized)" , ylab="log GDP (as proportion of mean)" , xlim=c(0,1) )

mu <- link( m8.5 , data=data.frame( cid=1 , rugged_std=rugged_seq ) ) 
mu_mean <- apply( mu , 2 , mean ) 
mu_ci <- apply( mu , 2 , PI , prob=0.97 ) 
lines( rugged_seq , mu_mean , lwd=2 ) 
shade( mu_ci , rugged_seq , col=col.alpha(rangi2,0.3) ) 

mtext("African nations") 


# plot non-Africa - cid=2 
plot( d_non_africa$rugged_std , d_non_africa$log_gdp_std , pch=1 , col="black" , xlab="ruggedness (standardized)" , ylab="log GDP (as proportion of mean)" , xlim=c(0,1) ) 

mu <- link( m8.5 , data=data.frame( cid=2 , rugged_std=rugged_seq ) ) 
mu_mean <- apply( mu , 2 , mean ) 
mu_ci <- apply( mu , 2 , PI , prob=0.97 ) 
lines( rugged_seq , mu_mean , lwd=2 ) 
shade( mu_ci , rugged_seq ) 
mtext("Non-African nations")
```

## 8.2 Symmetry of interactions

(1) How much does the association between ruggedness and log GDP depend upon whether the nation is in Africa?
(2) \(2\) How much does the association of Africa with log GDP depend upon ruggedness?

While these two possibilities sound different to most humans, your golem thinks they are identical. In this section, we’ll examine this fact, first mathematically. Then we’ll plot the ruggedness and GDP example again, but with the reverse phrasing—the association between Africa and GDP depends upon ruggedness.

![](images/clipboard-2860072008.png)

Below the horizontal dashed line, African nations have lower expected GDP. This is the case for most terrain ruggedness values. But at the highest ruggedness values, a nation is possibly better off inside Africa than outside it. Really it is hard to find any reliable difference inside and outside Africa, at high ruggedness values. It is only in smooth nations that being in Africa is a liability for the economy.

It’s simultaneously true in these data (and with this model) that

\(1\) the influence of ruggedness depends upon continent (gdp \~ ruggedness \| continent)

...and

\(2\) the influence of continent depends upon ruggedness (gdp \~ continent \| ruggedness)

## 8.3 continuous interactions (good practical comments!)

I want to convince the reader that interaction effects are difficult to interpret. They are nearly impossible to interpret, using only posterior means and standard deviations. Once interactions exist, multiple parameters are always in play at the same time. Pictures can help. It is dangerous to go alone.

It’s one thing to make a slope conditional upon a category. In such a context, the model reduces to estimating a different slope for each category. But it’s quite a lot harder to understand that a slope varies in a continuous fashion with a continuous variable. Interpretation is much harder in this case, even though the mathematics of the model are essentially the same as in the categorical case.

tulips

The blooms column will be our outcome—what we wish to predict. The water and shade columns will be our predictor variables.

```{r}

# water indicates one of three ordered levels of soil moisture, from low (1) to high (3). shade indicates one of three ordered levels of light exposure, from high (1) to low (3). The last column, bed, indicates a cluster of plants from the same section of the greenhouse.
data(tulips) 
d <- tulips 
str(d)
```

![So, with the variables being used in their scaled versions w_std = (wi - mean(w)), we can scale them beforehand](images/clipboard-1024377954.png)

```{r}
# Now blooms_std ranges from 0 to 1, and both water_cent and shade_cent range from -1 to 1.
d$blooms_std <- d$blooms / max(d$blooms) # scale
d$water_cent <- d$water - mean(d$water) # standardize
d$shade_cent <- d$shade - mean(d$shade) # standardize
```

Scaling the variable blooms: I’ve scaled blooms by its maximum observed value, for three reasons.

First, the large values on the raw scale will make optimization difficult.

Second, it will be easier to assign a reasonable prior this way.

Third, we don’t want to standardize blooms, because zero is a meaningful boundary we want to preserve.

Some thoughts on the relationship between rescaling and setting/resetting priors: As always in rescaling variables, the goals are to create focal points that you might have prior information about, prior to seeing the actual data values. That way we can assign priors that are not obviously crazy, and in thinking about those priors, we might realize that the model makes no sense. But this is only possible if we think about the relationship between measurements and parameters, and the exercise of rescaling and assigning sensible priors helps us along that path. Even when there are enough data that priors are pointless, this thought exercise is useful.

Thoughts around setting the priors:

![](images/clipboard-566558460.png)

The prior bounds on the parameters come from the prior standard deviations, all set to 1 here. These are surely too broad. The intercept alpha must be greater than zero and less than one, for example. But this prior assigns most of the probability outside that range:

```{r}
a <- rnorm( 1e4 , 0.5 , 1 ) 
sum( a < 0 | a > 1 ) / length( a )
```

A better prior for alpha (when water and shade is at their mean):

```{r}
# If it’s 0.5 units from the mean to zero, then a standard deviation of 0.25 should put only 5% of the mass outside the valid internal

a <- rnorm( 1e4 , 0.5 , 0.25 ) 
sum( a < 0 | a > 1 ) / length( a )
```

Much better. We now went from having 62% probability mass outside our reasonably boundaries to just 5%.

What about those slopes? What would a very strong effect of water and shade look like? How big could those slopes be in theory?

Max blooms range (y): 0 to 1. And water and shade goes from -1 to 1 (x lenght = 2). Then a straightforward calculation would give us a max slope of 1/2 = 0.5, this is the positive version of the slope, and lets say we do not make an assumption on the direction of the slope, so it could be max negative -0.5. The range of the beta -0.5 to 0.5, each side being 2 standard deviations, so one sd = 0.5/2 = 0.25. voila:

```{r}
m8.6 <- quap( 
  alist( 
    blooms_std ~ dnorm( mu , sigma ) , 
    mu <- a + bw*water_cent + bs*shade_cent , 
    a ~ dnorm( 0.5 , 0.25 ) , 
    bw ~ dnorm( 0 , 0.25 ) , 
    bs ~ dnorm( 0 , 0.25 ) , 
    sigma ~ dexp( 1 ) ), 
  data=d )

```

It’s a good idea at this point to simulate lines from the prior. But before doing that, let’s define the interaction model as well. Then we can talk about how to plot predictions from interactions and see both prior and posterior predictions together.

What would an interaction between shade and water mean? For example, if water is low, then decreasing the shade (increase light) can’t help as much as when water is high. So we want the slope of water, w, to be conditional on shade.

```{r}
mu <- link( m8.6 , data=data.frame( shade_cent=s , water_cent=-1:1 ) ) 
```

Posterior plotting:

```{r}
par(mfrow=c(1,3)) # 3 plots in 1 row 

for ( s in -1:1 ) { 
  idx <- which( d$shade_cent==s ) 

plot( d$water_cent[idx] , d$blooms_std[idx] , 
      xlim=c(-1,1) , ylim=c(0,1) , 
      xlab="water" , ylab="blooms" , 
      pch=16 , col=rangi2 ) 

# mu har 1000 rader, 3 kolonner. en kolonne hver for -1 to 1 water.
mu8.6 <- link( m8.6 , data=data.frame( shade_cent=s , water_cent=-1:1 ) )

for ( i in 1:20 ) lines( -1:1 , mu8.6[i,] , col=col.alpha("black",0.3) ) 

}
```

```{r}
prior <- extract.prior(m8.6)
mu <- link( m8.6 , 
            post = prior,
            data=data.frame( shade_cent=s , water_cent=-1:1 ) ) 
```

Prior plotting:

```{r}
set.seed(7) 
prior <- extract.prior(m8.6)

par(mfrow=c(1,3)) # 3 plots in 1 row 

for ( s in -1:1 ) { idx <- which( d$shade_cent==s ) 

plot( d$water_cent[idx] , d$blooms_std[idx] , 
      xlim=c(-1,1) , ylim=c(0,1) , 
      xlab="water" , ylab="blooms" , 
      pch=16 , col=rangi2 ) 

mu <- link( m8.6 , 
            post = prior,
            data=data.frame( shade_cent=s , water_cent=-1:1 ) ) 

for ( i in 1:20 ) lines( -1:1 , mu[i,] , col=col.alpha("black",0.3) ) 

}
```

# 9 Markov chain Monte Carlo (MCMC)

1\) grid approximation. 2) quadratic approx. 3) MCMC

known as Markov chain Monte Carlo (MCMC) estimation. Unlike in every earlier chapter in this book, here we’ll produce samples from the joint posterior of a model without maximizing anything. Instead of having to lean on quadratic and other approximations of the shape of the posterior, now we’ll be able to sample directly from the posterior without assuming a Gaussian, or any other, shape for it.

The cost of this power is that it may take much longer for our estimation to complete, and usually more work is required to specify the model as well. But the benefit is escaping the awkwardness of assuming multivariate normality. Equally important is the ability to directly estimate models, such as the generalized linear and multilevel models of later chapters. Such models routinely produce non-Gaussian posterior distributions, and sometimes they cannot be estimated at all with the techniques of earlier chapters.

The good news is that tools for building and inspecting MCMC estimates are getting better all the time. In this chapter you’ll meet a convenient way to convert the map formulas you’ve used so far into Markov chains. The engine that makes this possible is Stan (free and online at: mc-stan.org). Stan’s creators describe it as “a probabilistic programming language implementing statistical inference.” You won’t be working directly in Stan to begin withthe rethinking package provides tools that hide it from you for now. But as you move on to more advanced techniques, you’ll be able to generate Stan versions of the models you already understand. Then you can tinker with them and witness the power of a fully armed and operational Stan.

![](images/clipboard-3429914840.png)

-   <div>

    ```{r}
    num_weeks <- 1e5 
    positions <- rep(0,num_weeks) 
    current <- 10 

    for ( i in 1:num_weeks ) { 
      # record current position 
      positions[i] <- current 
      # flip coin to generate proposal 
      proposal <- current + sample( c(-1,1) , size=1 ) 
      # now make sure he loops around the archipelago 
      if ( proposal < 1 ) proposal <- 10 
      if ( proposal > 10 ) proposal <- 1 
      # move? 
      prob_move <- proposal/current 
      current <- ifelse( runif(1) < prob_move , proposal , current ) 
    }



    ```

    </div>

![](images/clipboard-2838508832.png)

## 9.2 Metropolis, Gibbs, and Sadness

MCMC is better than metropolis and gibbs!!

The metroplos algorithm was the one used in the island problem.

-   The “islands” in our objective are parameter values, and they need not be discrete, but can instead take on a continuous range of values as usual.

-   The “population sizes” in our objective are the posterior probabilities at each parameter value.

-   The “weeks” in our objective are samples taken from the joint posterior of the parameters in the model.

### 9.2.1 Gibbs sampling

The Metropolis algorithm works whenever the probability of proposing a jump to B from A is equal to the probability of proposing A from B, when the proposal distribution is symmetric. There is a more general method, known as MetropolisHastings,142 that allows asymmetric proposals. This would mean, in the context of King Markov’s fable, that the King’s coin were biased to lead him clockwise on average. Why would we want an algorithm that allows asymmetric proposals?

One reason is that it makes it easier to handle parameters, like standard deviation**s, that have boundaries at zero.** A better reason, however, is that it allows us to generate savvy proposals that explore the posterior distribution more efficiently. By “more efficiently,” I mean that we can acquire an equally good image of the posterior distribution in fewer steps. The most common way to generate savvy proposals is a technique known as Gibbs sampling. Gibbs sampling is a variant of the Metropolis-Hastings algorithm that uses clever proposals and is therefore more efficient. By “efficient,” I mean that you can get a good estimate of the posterior from Gibbs sampling with many fewer samples than a comparable Metropolis approach.

Adaptive proposals (positive)

The improvement arises from adaptive proposals in which the distribution of proposed parameter values adjusts itself intelligently, depending upon the parameter values at the moment. H

ow Gibbs sampling computes these adaptive proposals depends upon using particular combinations of prior distributions and likelihoods known as conjugate pairs. Conjugate pairs have analytical solutions for the posterior distribution of an individual parameter. And these solutions are what allow Gibbs sampling to make smart jumps around the joint posterior distribution of all parameters.

In practice, Gibbs sampling can be very efficient, and it’s the basis of popular Bayesian model fitting software like BUGS (Bayesian inference Using Gibbs Sampling) and JAGS (Just Another Gibbs Sampler). In these programs, you compose your statistical model using definitions very similar to what you’ve been doing so far in this book. The software automates the rest, to the best of its ability.

(negative of gibbs)

9.2.2. High-dimensional sadness. But there are some severe limitations to Gibbs sampling.

-   First, maybe you don’t want to use conjugate priors. Some conjugate priors are actually pathological in shape, once you start building multilevel models and need priors for entire covariance matrixes. This will be something to discuss once we reach such models in Chapter 14.

-   Second, as models become more complex and contain hundreds or thousands or tens of thousands of parameters, both Metropolis and Gibbs sampling become shockingly inefficient.

The reason is that they tend to get stuck in small regions of the posterior for potentially a long time. The high number of parameters isn’t the problem so much as the fact that models with many parameters nearly always have regions of high correlation in the posterior. This means that two or more parameters are highly correlated with one another in the posterior samples. You’ve seen this before with, for example, the two legs example in Chapter 6. Why is this a problem? Because high correlation means a narrow ridge of high probability combinations, and both Metropolis and Gibbs make too many dumb proposals of where to go next. So they get stuck.

Ridge:

![](images/clipboard-4106697993.png)

That the max mass distrubtions is far from the mode.

Any Markov chain approach that samples individual parameters in individual steps is going to get stuck, once the number of parameters grows sufficiently large. The reason goes by the name concentration of measure. This is an awkward name for the amazing fact that most of the probability mass of a high-dimension distribution is always very far from the mode of the distribution.

In two dimensions, a Gaussian distribution is a hill. The highest point is in the middle, at the mode. But if we imagine this hill is filled with dirt—what else are hills filled with?—then we can ask: Where is most of the dirt? As we move away from the peak in any direction, the altitude declines, so there is less dirt directly under our feet. But in the ring around the hill at the same distance, there is more dirt than there is at the peak. The area increases as we move away from the peak, even though the height goes down. So the total dirt, um probability, increases as we move away from the peak. Eventually the total dirt (probability) declines again, as the hill slopes down to zero. So at some radial distance from the peak, dirt (probability mass) is maximized.

In three dimensions, it isn’t a hill, but now a fuzzy sphere. The sphere is densest at the core, its “peak.” But again the volume increases as we move away from the core. And so there is more total sphere-stuff in a shell around the core.

```{r}
# this is one density of data with D dimensions. The figure from the book show all these poste in the same diagram (for different dimensional data)
D <- 10 
T <- 1e3 
Y <- rmvnorm(T,rep(0,D),diag(D)) 
rad_dist <- function( Y ) 
  sqrt( sum(Y^2) ) 
Rd <- sapply( 1:T , function(i) rad_dist( Y[i,] ) ) 
dens( Rd )
```

![](images/clipboard-413906092.png)

You can see that an ordinary Gaussian distribution with only 1 dimension, on the left, samples most of its points right next to this peak, as you’d expect. But with 10 dimensions, already there are no samples next to the peak at zero. With 100 dimensions, we’ve moved very far from the peak. And with 1000 dimensions, even further. The sampled points are in a thin, high-dimensional shell very far from the mode. Inside this shell, pairs of parameters curve dramatically against one another, creating very hard paths for a sampler to follow.

Why MCMC is better than metropolis and gibbs:

There are two consequences of this to focus on for now. First, this is why we need MCMC algorithms other than Metropolis and Gibbs that focus on the entire posterior at once, instead of one or a few dimensions at a time. Otherwise we get stuck in a narrow, highly curving region of parameter space. Second, this is also why optimization methods, like quap, that look for mode are no good in general. They are looking for an irrelevant part of the parameter space. In even a dozen dimensions, there just isn’t much probability mass near the mode. This isn’t such a worry in simple models, where distributions are often quite symmetric. But when we arrive at generalized linear and multilevel models, you’ll see that things are no longer so simple. Non-Bayesian methods of fitting such models play dangerous games by summarizing some of those parameters with the mode, even though the posterior mean can be very far away.

## 9.3. Hamiltonian Monte Carlo

It appears to be a quite general principle that, whenever there is a randomized way of doing something, then there is a nonrandomized way that delivers better performance but requires more thought. —E. T. Jaynes

The Metropolis algorithm and Gibbs sampling are highly random procedures. They try out new parameter values—proposals—and see how good they are, compared to the current values. Gibbs sampling gains efficiency by reducing the randomness of proposals by exploiting knowledge of the target distribution. This seems to fit Jaynes’ suggestion.

Hamiltonian Monte Carlo (or Hybrid Monte Carlo, HMC) pushes Jaynes’ principle much further. HMC is more computationally costly than Metropolis or Gibbs sampling. But its proposals are also much more efficient. As a result, HMC doesn’t need as many samples to describe the posterior distribution. You need less computer time in total, even though each sample needs more. And as models become more complex—thousands or tens of thousands of parameters—HMC can really outshine other algorithms, because the other algorithms just won’t work. The Earth would be swallowed by the Sun before your chain produces a reliable approximation of the posterior. We’re going to be using HMC on and off for the remainder of this book. You won’t have to implement it yourself.

Suppose King Markov’s cousin Monty is King on the mainland. Monty’s kingdom is not a discrete set of islands. Instead, it is a continuous territory stretched out along a narrow valley, **running north-south (NOT west and east).** Here’s how it works. The king’s vehicle picks random direction, either north or south, and drives off at a random momentum. As the vehicle goes uphill, it slows down and turns around when its declining momentum forces it to. Then it picks up speed again on the way down. After a fixed period of time, they stop the vehicle, get out, and start shaking hands and kissing babies. Then they get back in the vehicle and begin again. Amazingly, Hamilton can prove mathematically that this procedure guarantees that, in the long run, the locations visited will be inversely proportional to their relative elevations, which are also inversely proportional to the population densities. Not only does this keep the king moving, but it also spaces the locations apart better—unlike the other king, Monty does not only visit neighboring locations.

![](images/clipboard-2460845425.png)

This is not another metaphor. HMC really does run a physics simulation, pretending the vector of parameters gives the position of a little frictionless particle. The log-posterior provides a surface for this particle to glide across. When the log-posterior is very flat, because there isn’t much information in the likelihood and the priors are rather flat, then the particle can glide for a long time before the slope (gradient) makes it turn around. When instead the log-posterior is very steep, because either the likelihood or the priors are very concentrated, then the particle doesn’t get far before turning around. In principle, HMC will always accept every proposal, because it only makes intelligent proposals.

In practice, HMC uses a rejection criterion, because it is only approximating the smooth path of a particle. **It isn’t unusual to see acceptance rates over 95% with HMC**. Making smart proposals pays. What is the rejection criterion? Because HMC runs a physics simulation, certain things have to be conserved, like total energy of the system. When the total energy changes during the simulation, that means the numerical approximation is bad. When the approximation isn’t good, it might reject the proposal.

An example:

To take some of magic out of this, let’s do a two-dimensional simulation, for a simple posterior distribution with two parameters, the mean and standard deviation of a Gaussian. Suppose the data are 100 x values and 100 y values sampled from Normal(0; 1) and that the model is:

![](images/clipboard-1653437461.png)

What HMC needs to drive are two functions and two settings.

Functions:

1.  The first function computes the log-probability of the data and parameters. This is just the top part of Bayes formula, and every MCMC strategy requires this. (ugly formula)

2.  The second thing HMC needs is the gradient, which just means the slope in all directions at the current position. In this case, that means just two derivatives. If you take the expression above and differentiate it with respect to x and then y, you have what you need.

Settings: The two settings that HMC needs are a choice of number of leapfrog steps (#steps) and a choice of step size for each (size). This part is strange. And usually your machine will pick these values for you. Each path in the simulation—each curve for example between visits in Figure 9.5—is divided up into a **number of leapfrog steps.** If you choose many steps, the paths will be long. If you choose few, they will be short. **The size of each step is determined by, you guessed it, the step size**. The step size determines a how fine grained the simulation is. If the step size is small, then the particle can turn sharply. If the step size is large, then each leap will be large and could even overshoot the point where the simulation would want to turn around.

U-turn problem: However, that low autocorrelation is not automatic. The righthand plot in Figure 9.6 shows the same code but with L = 28leapfrog steps. Now because of the combination of leapfrog steps and step size, the paths tend to land close to where they started. Instead of independent samples from the posterior, we get correlated samples, like in a Metropolis chain. This problem is called the U-turn problem—the simulations turn around and return to the same neighborhood. The U-turn problem looks especially bad in this example, because the posterior is a perfect 2-dimensional Gaussian bowl. So the parabolic paths always loop back onto themselves. In most models, this won’t be the case. But you’ll still get paths returning close to where they started. This just shows that the efficiency of HMC comes with the expense of having to tune the leapfrog steps and step size in each application.

## 9.5 Care and feeding Marcov chain

### 9.5.1. How many samples do you need? 

You can control the number of samples from the chain by using the iter and warmup parameters. The defaults are 1000 for iter and warmup is set to iter/2, which gives you 500 warmup samples and 500 real samples to use for inference. you can decide on other values for iter and warmup.

1) First, what really matters is the effective number of samples, not the raw number. The effective number of samples is an estimate of the number of independent samples from the posterior distribution. Markov chains are typically autocorrelated, so that sequential samples are not entirely independent. Stan chains tend to be less autocorrelated than those produced by other engines, but there is always some autocorrelation. As you saw earlier in the chapter, Stan provides an estimate of effective number of samples as n_eff.

2) If all you want are posterior means, it doesn’t take many samples at all to get very good estimates. Even a couple hundred samples will do. But if you care about the exact shape in the extreme tails of the posterior, the 99th percentile or so, then you’ll need many many more. So there is no universally useful number of samples to aim for. In most typical regression applications, you can get a very good estimate of the posterior mean with as few as 200 effective samples. And if the posterior is approximately Gaussian, then all you need in addition is a good estimate of the variance, which can be had with one order of magnitude more, in most cases. For highly skewed posteriors, you’ll have to think more about which region of the distribution interests you.

3) How much warm up? The warmup setting is more subtle. On the one hand, you want to have the shortest warmup period necessary, so you can get on with real sampling. But on the other hand, more warmup can mean more efficient sampling. With Stan models, typically you can devote as much as half of your total samples, the iter value, to warmup and come out very well. But for simple models like those you’ve fit so far, much less warmup is really needed. Models can vary a lot in the shape of their posterior distributions, so again there is no universally best answer. But if you are having trouble, you might try increasing the warmup. If not, you might try reducing it.

### 9.5.2. How many chains do you need?


First, when debugging a model, use a single chain. Then when deciding whether the chains are valid, you need more than one chain. Third, when you begin the final run that you’ll make inferences from, you only really need one chain.

The first time you try to sample from a chain, you might not be sure whether the chain is working right. So of course you will check the trace plot. Having more than one chain during these checks helps to make sure that the Markov chains are all converging to the same distribution. Using 3 or 4 chains is conventional, and quite often more than enough to reassure us that the sampling is working properly.

One long chain, or several shorter chains? 
But once you’ve verified that the sampling is working well, and you have a good idea of how many warmup samples you need, it’s perfectly safe to just run one long chain. For example, suppose we learn that we need 1000 warmup samples and about 9000 real samples in total. Should we run one chain, with warmup=1000 and iter=10000, or rather 3 chains, with warmup=1000 and iter=4000? It doesn’t really matter, in terms of inference. But it might matter in efficiency, because the 3 chains cost you an extra 2000 samples of warmup that just get thrown away. And since warmup is typically the slowest part of the chain, these extra 2000 samples cost a disproportionate amount of your computer’s time. On the other hand, if you run the chains on different computers or processor cores within a single computer, then you might prefer 3 chains, because you can spread the load and finish the whole job faster.

**There are exotic situations in which all of the advice above must be modified. But for typical regression models, you can live by the motto four short chains to check, one long chain for inference.**

Rhat: When Rhat is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldn’t trust the samples. Rhat can reach 1.00 even for an invalid chain. So view it perhaps as a signal of danger, but never of safety.




# 11 God Spiked the Integers

## 11.1 Binomial distribution

When our outcome variable is of binomial distribution, two types of distributions that are binomial: (1) Logistic regression is the common name when the data are organized into single-trial cases, such that the outcome variable can only take values 0 and 1. (2) When individual trials with the same covariate values are instead aggregated together, it is common to speak of an aggregated binomial regression. In this case, the outcome can take the value zero or any positive integer up to n, the number of trials.

Example: chimpanzee

We’ll want to infer what happens in each combination of prosoc_left and condition. There are four combinations: (1) prosoc_left= 0and condition= 0: Two food items on right and no partner. (2) prosoc_left= 1and condition= 0: Two food items on left and no partner. (3) prosoc_left= 0and condition= 1: Two food items on right and partner present. (4) prosoc_left= 1and condition= 1: Two food items on left and partner present.

Using dummy variables makes it hard to construct sensible priors. So instead let’s build an index variable containing the values 1 through 4, to index the combinations above.

```{r}

library(rethinking) 

data(chimpanzees) 
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition
```

```{r}
#check the combinations

xtabs( ~ treatment + prosoc_left + condition , d )
```
L = count data for how many pulled left

![](images/clipboard-3332960037.png)
